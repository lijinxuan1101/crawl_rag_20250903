[
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/CRISPR",
      "title": "CRISPR",
      "main_content": "CRISPR(/ˈkrɪspər/; acronym ofclustered regularly interspaced short palindromic repeats) is a family ofDNAsequences found in thegenomesofprokaryoticorganisms such asbacteriaandarchaea.Each sequence within an individual prokaryotic CRISPR is derived from a DNA fragment of abacteriophagethat had previously infected the prokaryote or one of its ancestors.These sequences are used to detect and destroy DNA from similar bacteriophages during subsequent infections. Hence these sequences play a key role in the antiviral (i.e. anti-phage) defense system of prokaryotes and provide a form of heritable,acquired immunity.CRISPR is found in approximately 50% of sequencedbacterial genomesand nearly 90% of sequenced archaea.\n\nCas9(or \"CRISPR-associated protein 9\") is anenzymethat uses CRISPR sequences as a guide to recognize and open up specific strands of DNA that are complementary to the CRISPR sequence. Cas9 enzymes together with CRISPR sequences form the basis of a technology known asCRISPR-Cas9that can be used to edit genes within living organisms.This editing process has a wide variety of applications including basicbiologicalresearch, development ofbiotechnologicalproducts, and treatment of diseases.The development of the CRISPR-Cas9 genome editing technique was recognized by theNobel Prize in Chemistryin 2020 awarded toEmmanuelle CharpentierandJennifer Doudna.\n\nThe discovery of clustered DNA repeats took place independently in three parts of the world. The first description of what would later be called CRISPR is fromOsaka UniversityresearcherYoshizumi Ishinoand his colleagues in 1987. They accidentally cloned part of a CRISPR sequence together with the \"iap\" gene(isozyme conversion of alkaline phosphatase)from their target genome, that ofEscherichia coli.The organization of the repeats was unusual. Repeated sequences are typically arranged consecutively, without interspersing different sequences.They did not know the function of the interrupted clustered repeats.\n\nIn 1993, researchers ofMycobacterium tuberculosisin the Netherlands published two articles about a cluster of interrupteddirect repeats(DR) in that bacterium. They recognized the diversity of the sequences that intervened in the direct repeats among different strains ofM. tuberculosisand used this property to design a typing method calledspoligotyping, still in use today.\n\nFrancisco Mojicaat theUniversity of Alicantein Spain studied the function of repeats in the archaeal speciesHaloferaxandHaloarcula. Mojica's supervisor surmised that the clustered repeats had a role in correctly segregating replicated DNA into daughter cells during cell division, because plasmids and chromosomes with identical repeat arrays could not coexist inHaloferax volcanii. Transcription of the interrupted repeats was also noted for the first time; this was the first full characterization of CRISPR.By 2000, Mojica and his students, after an automated search of published genomes, identified interrupted repeats in 20 species of microbes as belonging to the same family.Because those sequences were interspaced, Mojica initially called these sequences \"short regularly spaced repeats\" (SRSR).In 2001, Mojica andRuud Jansen, who were searching for additional interrupted repeats, proposed the acronym CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) to unify the numerous acronyms used to describe these sequences.In 2002, Tang, et al. showed evidence that CRISPR repeat regions from the genome ofArchaeoglobus fulgiduswere transcribed into long RNA molecules subsequently processed into unit-length small RNAs, plus some longer forms of 2, 3, or more spacer-repeat units.\n\nIn 2005,yogurtresearcherRodolphe Barrangoudiscovered thatStreptococcus thermophilus, after iterative phage infection challenges, develops increased phage resistance due to the incorporation of additional CRISPR spacer sequences.Barrangou's employer, the Danish food company Danisco, then developed phage-resistantS. thermophilusstrains for yogurt production. Danisco was later bought byDuPont, which owns about 50 percent of the global dairy culture market, and the technology spread widely.\n\nA major advance in understanding CRISPR came with Jansen's observation that the prokaryote repeat cluster was accompanied by four homologous genes that make up CRISPR-associated systems,cas1–4. The Cas proteins showedhelicaseandnucleasemotifs, suggesting a role in the dynamic structure of the CRISPR loci.In this publication, the acronym CRISPR was used as the universal name of this pattern, but its function remained enigmatic.\n\nIn 2005, three independent research groups showed that some CRISPR spacers are derived fromphageDNA andextrachromosomal DNAsuch asplasmids.In effect, the spacers are fragments of DNA gathered from viruses that previously attacked the cell. The source of the spacers was a sign that the CRISPR-cassystem could have a role in adaptive immunity inbacteria.All three studies proposing this idea were initially rejected by high-profile journals, but eventually appeared in other journals.\n\nThe first publicationproposing a role of CRISPR-Cas in microbial immunity, by Mojica and collaborators at theUniversity of Alicante, predicted a role for the RNA transcript of spacers on target recognition in a mechanism that could be analogous to theRNA interferencesystem used by eukaryotic cells. Koonin and colleagues extended this RNA interference hypothesis by proposing mechanisms of action for the different CRISPR-Cas subtypes according to the predicted function of their proteins.\n\nExperimental work by several groups revealed the basic mechanisms of CRISPR-Cas immunity. In 2007, the first experimental evidence that CRISPR was an adaptive immune system was published.A CRISPR region inStreptococcus thermophilusacquired spacers from the DNA of an infectingbacteriophage. The researchers manipulated the resistance ofS. thermophilusto different types of phages by adding and deleting spacers whose sequence matched those found in the tested phages.In 2008, Brouns and Van der Oost identified a complex of Cas proteins called Cascade, that inE. colicut the CRISPR RNA precursor within the repeats into mature spacer-containing RNA molecules calledCRISPR RNA(crRNA), which remained bound to the protein complex.Moreover, it was found that Cascade, crRNA and a helicase/nuclease (Cas3) were required to provide a bacterial host with immunity against infection by aDNA virus. By designing an anti-virus CRISPR, they demonstrated that two orientations of the crRNA (sense/antisense) provided immunity, indicating that the crRNA guides were targetingdsDNA. That year Marraffini and Sontheimer confirmed that a CRISPR sequence ofS. epidermidistargeted DNA and not RNA to preventconjugation. This finding was at odds with the proposed RNA-interference-like mechanism of CRISPR-Cas immunity, although a CRISPR-Cas system that targets foreign RNA was later found inPyrococcus furiosus.A 2010 study showed that CRISPR-Cas cuts strands of both phage and plasmid DNA inS. thermophilus.\n\nA simpler CRISPR system fromStreptococcus pyogenesuses the proteinCas9, an endonuclease functioning with two small RNAs—crRNA and tracrRNA—to form a four-component complex.In 2012,Jennifer DoudnaandEmmanuelle Charpentiersimplified this into a two-component system by fusing the RNAs into a \"single-guide RNA\", enabling Cas9 to target and cut specific DNA sequences—a breakthrough that earned them theNobel Prize in Chemistryin 2020.Parallel work showed theS. thermophilusCas9 could be similarly reprogrammed by altering the crRNA sequence.These developments spurred genome editing efforts, including demonstrations by groups led byFeng ZhangandGeorge Churchshowing genome editing in human cells using CRISPR-Cas9.\n\nCas12a, a Class II Type V CRISPR-associated nuclease, was characterized in 2015 and was formerly known as Cpf1.This nuclease is found in the CRISPR-Cpf1 system of bacteria such asFrancisella novicida.The initial designation, derived from a TIGRFAMs protein family definition established in 2012, reflected the prevalence of this CRISPR-Cas subtype in thePrevotellaandFrancisellalineages. Cas12a exhibits several key distinctions from Cas9: it generates staggered cuts in double-stranded DNA, in contrast to the blunt ends produced by Cas9;it relies on a 'T-rich' protospacer adjacent motif (PAM) (typically 5'-TTTV-3', where V is A, C, or G), offering alternative targeting sites compared to the 'G-rich' PAMs (typically 5'-NGG-3') favored by Cas9;and it requires only a CRISPR RNA (crRNA) for effective targeting, whereas Cas9 necessitates both a crRNA and atrans-activating crRNA (tracrRNA).\n\nIn 2016, the nuclease (formerly known as C2c2) from the bacteriumLeptotrichia shahiiwas characterized by researchers inFeng Zhang's group atMITand theBroad Institute. Cas13 is an RNA-guided RNA endonuclease, which means that it does not cleave DNA, but only single-stranded RNA. Cas13 is guided by its crRNA to a ssRNA target and binds and cleaves the target. Similar to Cas12a, the Cas13 remains bound to the target and then cleaves other ssRNA molecules non-discriminately.This collateral cleavage property has been exploited for the development of various diagnostic technologies.\n\nThe CRISPR array is made up of an AT-rich leader sequence followed by short repeats that are separated by unique spacers.CRISPR repeats typically range in size from 28 to 37base pairs(bps), though there can be as few as 23 bp and as many as 55 bp.Some showdyad symmetry, implying the formation of asecondary structuresuch as astem-loop('hairpin') in the RNA, while others are designed to be unstructured. The size of spacers in different CRISPR arrays is typically 32 to 38 bp (range 21 to 72 bp).New spacers can appear rapidly as part of the immune response to phage infection.There are usually fewer than 50 units of the repeat-spacer sequence in a CRISPR array.\n\nSmall clusters ofcasgenes are often located next to CRISPR repeat-spacer arrays. Collectively the 93casgenes are grouped into 35 families based on sequence similarity of the encoded proteins. 11 of the 35 families form thecascore, which includes the protein families Cas1 through Cas9. A complete CRISPR-Cas locus has at least one gene belonging to thecascore.\n\nCRISPR-Cas systems fall into two classes. Class 1 systems use a complex of multiple Cas proteins to degrade foreign nucleic acids. Class 2 systems use a single large Cas protein for the same purpose. Class 1 is divided into types I, III, and IV; class 2 is divided into types II, V, and VI.The 6 system types are divided into 33 subtypes.Each type and most subtypes are characterized by a \"signature gene\" found almost exclusively in the category. Classification is also based on the complement ofcasgenes that are present. Most CRISPR-Cas systems have a Cas1 protein. Thephylogenyof Cas1 proteins generally agrees with the classification system,but exceptions exist due to module shuffling.Many organisms contain multiple CRISPR-Cas systems suggesting that they are compatible and may share components.The sporadic distribution of the CRISPR-Cas subtypes suggests that the CRISPR-Cas system is subject tohorizontal gene transferduring microbialevolution.\n\nCRISPR-Cas immunity is a natural process of bacteria and archaea.CRISPR-Cas prevents bacteriophage infection,conjugationandnatural transformationby degrading foreign nucleic acids that enter the cell.\n\nWhen amicrobeis invaded by abacteriophage, the first stage of the immune response is to capture phage DNA and insert it into a CRISPR locus in the form of a spacer.Cas1andCas2are found in both types of CRISPR-Cas immune systems, which indicates that they are involved in spacer acquisition. Mutation studies confirmed this hypothesis, showing that removal of Cas1 or Cas2 stopped spacer acquisition, without affecting CRISPR immune response.\n\nMultiple Cas1 proteins have been characterised and their structures resolved.Cas1 proteins have diverseamino acidsequences. However, their crystal structures are similar and all purified Cas1 proteins are metal-dependent nucleases/integrasesthat bind to DNA in a sequence-independent manner.Representative Cas2 proteins have been characterised and possess either (single strand) ssRNA-or (double strand) dsDNA-specificendoribonucleaseactivity.\n\nIn the I-E system ofE. coli, Cas1 and Cas2 form a complex where a Cas2 dimer bridges two Cas1 dimers.In this complex, Cas2 performs a non-enzymatic scaffolding role,binding double-stranded fragments of invading DNA, while Cas1 binds the single-stranded flanks of the DNA and catalyses their integration into CRISPR arrays.New spacers are usually added at the beginning of the CRISPR next to the leader sequence creating a chronological record of viral infections.InE. coliahistone like proteincalled integration host factor (IHF), which binds to the leader sequence, is responsible for the accuracy of this integration.IHF also enhances integration efficiency in the type I-F system ofPectobacterium atrosepticum,but in other systems, different host factors may be required\n\nBioinformatic analysis of regions of phage genomes that were excised as spacers (termed protospacers) revealed that they were not randomly selected but instead were found adjacent to short (3–5 bp) DNA sequences termedprotospacer adjacent motifs(PAM). Analysis of CRISPR-Cas systems showed PAMs to be important for type I and type II, but not type III systems during acquisition.In type I and type II systems, protospacers are excised at positions adjacent to a PAM sequence, with the other end of the spacer cut using a ruler mechanism, thus maintaining the regularity of the spacer size in the CRISPR array.The conservation of the PAM sequence differs between CRISPR-Cas systems and appears to be evolutionarily linked to Cas1 and theleader sequence.\n\nNew spacers are added to a CRISPR array in a directional manner,occurring preferentially,but not exclusively, adjacentto the leader sequence. Analysis of the type I-E system fromE. colidemonstrated that the first direct repeat adjacent to the leader sequence is copied, with the newly acquired spacer inserted between the first and second direct repeats.\n\nThe PAM sequence appears to be important during spacer insertion in type I-E systems. That sequence contains a strongly conserved final nucleotide (nt) adjacent to the first nt of the protospacer. This nt becomes the final base in the first direct repeat.This suggests that the spacer acquisition machinery generates single-stranded overhangs in the second-to-last position of the direct repeat and in the PAM during spacer insertion. However, not all CRISPR-Cas systems appear to share this mechanism as PAMs in other organisms do not show the same level of conservation in the final position.It is likely that in those systems, a blunt end is generated at the very end of the direct repeat and the protospacer during acquisition.\n\nAnalysis ofSulfolobus solfataricusCRISPRs revealed further complexities to the canonical model of spacer insertion, as one of its six CRISPR loci inserted new spacers randomly throughout its CRISPR array, as opposed to inserting closest to the leader sequence.\n\nMultiple CRISPRs contain many spacers to the same phage. The mechanism that causes this phenomenon was discovered in the type I-E system ofE. coli. A significant enhancement in spacer acquisition was detected where spacers already target the phage, even mismatches to the protospacer. This 'priming' requires the Cas proteins involved in both acquisition and interference to interact with each other. Newly acquired spacers that result from the priming mechanism are always found on the same strand as the priming spacer.This observation led to the hypothesis that the acquisition machinery slides along the foreign DNA after priming to find a new protospacer.\n\nCRISPR-RNA (crRNA), which later guides the Cas nuclease to the target during the interference step, must be generated from the CRISPR sequence. The crRNA is initially transcribed as part of a single long transcript encompassing much of the CRISPR array.This transcript is then cleaved by Cas proteins to form crRNAs. The mechanism to produce crRNAs differs among CRISPR-Cas systems. In type I-E and type I-F systems, the proteins Cas6e and Cas6f respectively, recognise stem-loopscreated by the pairing of identical repeats that flank the crRNA.These Cas proteins cleave the longer transcript at the edge of the paired region, leaving a single crRNA along with a small remnant of the paired repeat region.\n\nType III systems also use Cas6, however, their repeats do not produce stem-loops. Cleavage instead occurs by the longer transcript wrapping around the Cas6 to allow cleavage just upstream of the repeat sequence.\n\nType II systems lack the Cas6 gene and instead utilize RNaseIII for cleavage. Functional type II systems encode an extra small RNA that is complementary to the repeat sequence, known as atrans-activating crRNA(tracrRNA).Transcription of the tracrRNA and the primary CRISPR transcript results in base pairing and the formation of dsRNA at the repeat sequence, which is subsequently targeted by RNaseIII to produce crRNAs. Unlike the other two systems, the crRNA does not contain the full spacer, which is instead truncated at one end.\n\nCrRNAs associate with Cas proteins to form ribonucleotide complexes that recognize foreign nucleic acids. CrRNAs show no preference between the coding and non-coding strands, which is indicative of an RNA-guided DNA-targeting system.The type I-E complex (commonly referred to as Cascade) requires five Cas proteins bound to a single crRNA.\n\nDuring the interference stage in type I systems, the PAM sequence is recognized on the crRNA-complementary strand and is required along with crRNA annealing. In type I systems correct base pairing between the crRNA and the protospacer signals a conformational change in Cascade that recruitsCas3for DNA degradation.\n\nType II systems rely on a single multifunctional protein,Cas9, for the interference step.Cas9 requires both the crRNA and the tracrRNA to function and cleave DNA using its dual HNH and RuvC/RNaseH-like endonuclease domains. Basepairing between the PAM and the phage genome is required in type II systems. However, the PAM is recognized on the same strand as the crRNA (the opposite strand to type I systems).\n\nType III systems, like type I require six or seven Cas proteins binding to crRNAs.The type III systems analysed fromS. solfataricusandP. furiosusboth target the mRNA of phages rather than phage DNA genome,which may make these systems uniquely capable of targeting RNA-based phage genomes.Type III systems were also found to target DNA in addition to RNA using a different Cas protein in the complex, Cas10.The DNA cleavage was shown to be transcription dependent.\n\nThe mechanism for distinguishing self from foreign DNA during interference is built into the crRNAs and is therefore likely common to all three systems. Throughout the distinctive maturation process of each major type, all crRNAs contain a spacer sequence and some portion of the repeat at one or both ends. It is the partial repeat sequence that prevents the CRISPR-Cas system from targeting the chromosome as base pairing beyond the spacer sequence signals self and prevents DNA cleavage.RNA-guided CRISPR enzymes are classified astype V restriction enzymes.\n\nThe cas genes in the adaptor and effector modules of the CRISPR-Cas system are believed to have evolved from two different ancestral modules. Atransposon-like element calledcasposonencoding the Cas1-like integrase and potentially other components of the adaptation module was inserted next to the ancestral effector module, which likely functioned as an independent innate immune system.The highly conserved cas1 and cas2 genes of the adaptor module evolved from the ancestral module while a variety of class 1 effector cas genes evolved from the ancestral effector module.The evolution of these various class 1 effector module cas genes was guided by various mechanisms, such as duplication events.On the other hand, each type of class 2 effector module arose from subsequent independent insertions of mobile genetic elements.These mobile genetic elements took the place of the multiple gene effector modules to create single gene effector modules that produce large proteins which perform all the necessary tasks of the effector module.The spacer regions of CRISPR-Cas systems are taken directly from foreign mobile genetic elements and thus their long-term evolution is hard to trace.The non-random evolution of these spacer regions has been found to be highly dependent on the environment and the particular foreign mobile genetic elements it contains.\n\nCRISPR-Cas can immunize bacteria against certain phages and thus halt transmission. For this reason,Koonindescribed CRISPR-Cas as aLamarckianinheritance mechanism.However, this was disputed by a critic who noted, \"We should remember [Lamarck] for the good he contributed to science, not for things that resemble his theory only superficially. Indeed, thinking of CRISPR and other phenomena as Lamarckian only obscures the simple and elegant way evolution really works\".But as more recent studies have been conducted, it has become apparent that the acquired spacer regions of CRISPR-Cas systems are indeed a form of Lamarckian evolution because they are genetic mutations that are acquired and then passed on.On the other hand, the evolution of the Cas gene machinery that facilitates the system evolves through classic Darwinian evolution.\n\nAnalysis of CRISPR sequences revealedcoevolutionof host and viral genomes.\n\nThe basic model of CRISPR evolution is newly incorporated spacers driving phages to mutate their genomes to avoid the bacterial immune response, creating diversity in both the phage and host populations. To resist a phage infection, the sequence of the CRISPR spacer must correspond perfectly to the sequence of the target phage gene. Phages can continue to infect their hosts' given point mutations in the spacer.Similar stringency is required in PAM or the bacterial strain remains phage sensitive.\n\nA study of 124S. thermophilusstrains showed that 26% of all spacers were unique and that different CRISPR loci showed different rates of spacer acquisition.Some CRISPR loci evolve more rapidly than others, which allowed the strains' phylogenetic relationships to be determined. Acomparative genomicanalysis showed thatE. coliandS. entericaevolve much more slowly thanS. thermophilus. The latter's strains that diverged 250,000 years ago still contained the same spacer complement.\n\nMetagenomicanalysis of two acid-mine-drainagebiofilmsshowed that one of the analyzed CRISPRs contained extensive deletions and spacer additions versus the other biofilm, suggesting a higher phage activity/prevalence in one community than the other.In the oral cavity, a temporal study determined that 7–22% of spacers were shared over 17 months within an individual while less than 2% were shared across individuals.\n\nFrom the same environment, a single strain was tracked usingPCRprimers specific to its CRISPR system. Broad-level results of spacer presence/absence showed significant diversity. However, this CRISPR added three spacers over 17 months,suggesting that even in an environment with significant CRISPR diversity some loci evolve slowly.\n\nCRISPRs were analysed from the metagenomes produced for theHuman Microbiome Project.Although most were body-site specific, some within a body site are widely shared among individuals. One of these loci originated fromstreptococcalspecies and contained ≈15,000 spacers, 50% of which were unique. Similar to the targeted studies of the oral cavity, some showed little evolution over time.\n\nCRISPR evolution was studied inchemostatsusingS. thermophilusto directly examine spacer acquisition rates. In one week,S. thermophilusstrains acquired up to three spacers when challenged with a single phage.During the same interval, the phage developedsingle-nucleotide polymorphismsthat became fixed in the population, suggesting that targeting had prevented phage replication absent these mutations.\n\nAnotherS. thermophilusexperiment showed that phages can infect and replicate in hosts that have only one targeting spacer. Yet another showed that sensitive hosts can exist in environments with high-phage titres.The chemostat and observational studies suggest many nuances to CRISPR and phage (co)evolution.\n\nCRISPRs are widely distributed among bacteria and archaeaand show some sequence similarities.Their most notable characteristic is their repeating spacers and direct repeats. This characteristic makes CRISPRs easily identifiable in long sequences of DNA, since the number of repeats decreases the likelihood of a false positive match.\n\nAnalysis of CRISPRs in metagenomic data is more challenging, as CRISPR loci do not typically assemble, due to their repetitive nature or through strain variation, which confuses assembly algorithms. Where many reference genomes are available,polymerase chain reaction(PCR) can be used to amplify CRISPR arrays and analyse spacer content.However, this approach yields information only for specifically targeted CRISPRs and for organisms with sufficient representation in public databases to design reliable polymerase PCR primers. Degenerate repeat-specific primers can be used to amplify CRISPR spacers directly from environmental samples; amplicons containing two or three spacers can be then computationally assembled to reconstruct long CRISPR arrays.\n\nThe alternative is to extract and reconstruct CRISPR arrays from shotgun metagenomic data. This is computationally more difficult, particularly with second generation sequencing technologies (e.g. 454, Illumina), as the short read lengths prevent more than two or three repeat units appearing in a single read. CRISPR identification in raw reads has been achieved using purelyde novoidentificationor by using direct repeat sequences in partially assembled CRISPR arrays fromcontigs(overlapping DNA segments that together represent a consensus region of DNA)and direct repeat sequences from published genomesas a hook for identifying direct repeats in individual reads.\n\nAnother way for bacteria to defend against phage infection is by havingchromosomal islands. A subtype of chromosomal islands called phage-inducible chromosomal island (PICI) is excised from a bacterial chromosome upon phage infection and can inhibit phage replication.PICIs are induced, excised, replicated, and finally packaged into small capsids by certain staphylococcal temperate phages. PICIs use several mechanisms to block phage reproduction. In the first mechanism, PICI-encoded Ppi differentially blocks phage maturation by binding or interacting specifically with phage TerS, hence blocking phage TerS/TerL complex formation responsible for phage DNA packaging. In the second mechanism PICI CpmAB redirects the phage capsid morphogenetic protein to make 95% of SaPI-sized capsid and phage DNA can package only 1/3rd of their genome in these small capsids and hence become nonviable phage.The third mechanism involves two proteins, PtiA and PtiB, that target the LtrC, which is responsible for the production of virion and lysis proteins. This interference mechanism is modulated by a modulatory protein, PtiM, binds to one of the interference-mediating proteins, PtiA, and hence achieves the required level of interference.\n\nOne study showed that lytic ICP1 phage, which specifically targetsVibrio choleraeserogroupO1, has acquired a CRISPR-Cas system that targets aV. choleraPICI-like element. The system has 2 CRISPR loci and 9 Cas genes. It seems to behomologousto the I-F system found inYersinia pestis. Moreover, like the bacterial CRISPR-Cas system, ICP1 CRISPR-Cas can acquire new sequences, which allows phage and host to co-evolve.\n\nCertain archaeal viruses were shown to carry mini-CRISPR arrays containing one or two spacers. It has been shown that spacers within the virus-borne CRISPR arrays target other viruses and plasmids, suggesting that mini-CRISPR arrays represent a mechanism of heterotypic superinfection exclusion and participate in interviral conflicts.\n\nCRISPR gene editing is a revolutionary technology that allows for precise, targeted modifications to the DNA of living organisms. Developed from a natural defense mechanism found in bacteria, CRISPR-Cas9 is the most commonly used system. Gene editing with CRISPR-Cas9 involves aCas9 nucleaseand an engineeredguide RNA, which come together to allow for the precise \"cutting\" of one or both strands of DNA at specific locations within the genome.It makes use of the cell's natural DNA repair systems, includingnon-homologous end joining,homology-directed repair, ormismatch repair, to modify, insert, or delete genetic material at these specific cut sites.This technology has transformed fields such as genetics, medicine,and agriculture,offering potential treatments for genetic disorders, advancements in crop engineering, and research into the fundamental workings of life. However, its ethical implications and potential unintended consequences have sparked significant debate.",
      "sections": [
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 3,
          "heading": "Repeated sequences"
        },
        {
          "level": 3,
          "heading": "CRISPR-associated systems"
        },
        {
          "level": 3,
          "heading": "Cas9"
        },
        {
          "level": 3,
          "heading": "Cas12a"
        },
        {
          "level": 3,
          "heading": "Cas13a"
        },
        {
          "level": 2,
          "heading": "Locus structure"
        },
        {
          "level": 3,
          "heading": "Repeats and spacers"
        },
        {
          "level": 3,
          "heading": "CRISPR RNA structures"
        },
        {
          "level": 3,
          "heading": "Cas genes and CRISPR subtypes"
        },
        {
          "level": 2,
          "heading": "Mechanism"
        },
        {
          "level": 3,
          "heading": "Spacer acquisition"
        },
        {
          "level": 3,
          "heading": "Biogenesis"
        },
        {
          "level": 3,
          "heading": "Interference"
        },
        {
          "level": 2,
          "heading": "Evolution"
        },
        {
          "level": 3,
          "heading": "Coevolution"
        },
        {
          "level": 3,
          "heading": "Rates"
        },
        {
          "level": 2,
          "heading": "Identification"
        },
        {
          "level": 2,
          "heading": "Use by phages"
        },
        {
          "level": 2,
          "heading": "Applications"
        },
        {
          "level": 3,
          "heading": "Protein Data Bank"
        }
      ],
      "raw_content_length": 1008712,
      "cleaned_content_length": 29267,
      "scraped_at": "2025-09-02 15:29:58",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "CRISPR-Cas9 genome editing technology",
      "discoverers": [
        "Jennifer Doudna",
        "Emmanuelle Charpentier",
        "Francisco Mojica",
        "Yoshizumi Ishino",
        "Rodolphe Barrangou",
        "Ruud Jansen"
      ],
      "discovery_years": [
        "2012"
      ],
      "discovery_timeline": [
        "1987: First description of CRISPR by Yoshizumi Ishino",
        "1993: Research on Mycobacterium tuberculosis",
        "2000: Mojica identifies interrupted repeats in microbes",
        "2001: Proposal of the acronym CRISPR",
        "2005: Discovery of phage DNA-derived spacers",
        "2007: First evidence of CRISPR as an adaptive immune system",
        "2012: Doudna and Charpentier simplify CRISPR-Cas9 system"
      ],
      "mechanism": "CRISPR-Cas9 uses RNA sequences derived from CRISPR to guide the Cas9 enzyme to specific DNA sequences, allowing for targeted cutting and editing of the DNA.",
      "key_features": [
        "Targeted gene editing",
        "High precision",
        "Versatile applications",
        "Ability to edit multiple genes simultaneously"
      ],
      "applications": [
        "Basic biological research",
        "Development of biotechnological products",
        "Gene therapy",
        "Agricultural biotechnology"
      ],
      "significance": "CRISPR-Cas9 has revolutionized genetic engineering, providing a powerful tool for gene editing that has broad implications in medicine, agriculture, and biological research.",
      "institutions": [
        "Osaka University",
        "University of Alicante",
        "Danish food company Danisco"
      ],
      "awards": [
        "Nobel Prize in Chemistry 2020"
      ],
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/RNA_vaccine",
      "title": "mRNA vaccine",
      "main_content": "AnmRNAvaccineis a type ofvaccinethat uses a copy of a molecule calledmessenger RNA(mRNA) to produce an immune response.The vaccinedeliversmolecules ofantigen-encoding mRNA intocells, which use the designed mRNA as a blueprint to build foreignproteinthat would normally be produced by apathogen(such as avirus) or by acancer cell. These protein molecules stimulate anadaptive immune responsethat teaches the body to identify and destroy the corresponding pathogen or cancer cells.The mRNA isdeliveredby a co-formulation of theRNAencapsulated inlipid nanoparticlesthat protect the RNA strands and help their absorption into the cells.\n\nReactogenicity, the tendency of a vaccine to produce adverse reactions, is similar to that of conventional non-RNA vaccines.People susceptible to anautoimmune responsemay have an adverse reaction to messenger RNA vaccines.The advantages of mRNA vaccines over traditional vaccines are ease of design, speed and lower cost of production, the induction of bothcellularandhumoral immunity, and lack of interaction with thegenomic DNA.While some messenger RNA vaccines, such as thePfizer–BioNTech COVID-19 vaccine, have the disadvantage of requiringultracold storagebefore distribution,other mRNA vaccines, such as theModerna vaccine, do not have such requirements.\n\nInRNA therapeutics, messenger RNA vaccines have attracted considerable interest asCOVID-19 vaccines.In December 2020,Pfizer–BioNTechandModernaobtained authorization for their mRNA-based COVID-19 vaccines. On 2 December, the UKMedicines and Healthcare products Regulatory Agency(MHRA) became thefirstmedicines regulatorto approve an mRNA vaccine, authorizing the Pfizer–BioNTech vaccine for widespread use.On 11 December, the USFood and Drug Administration(FDA) issued anemergency use authorizationfor the Pfizer–BioNTech vaccineand a week later similarly authorized the Moderna vaccine.In 2023 theNobel Prize in Physiology or Medicinewas awarded toKatalin KarikóandDrew Weissmanfor their discoveries concerningmodified nucleosidesthat enabled the development of effective mRNA vaccines against COVID-19.\n\nThe first successfultransfectionof designed mRNA packaged within a liposomalnanoparticleinto a cell was published in 1989.\"Naked\" (or unprotected) lab-made mRNA was injected a year later into the muscle of mice.These studies were the first evidence thatin vitrotranscribed mRNA with a chosen gene was able to deliver the genetic information to produce a desired protein within living cell tissueand led to the concept proposal of messenger RNA vaccines.\n\nLiposome-encapsulated mRNA encoding a viralantigenwas shown in 1993 to stimulateT cellsin mice.The following year self-amplifying mRNA was developed by including both a viral antigen andreplicaseencoding gene.The method was used in mice to elicit both ahumoralandcellular immuneresponse against a viral pathogen.The next year mRNA encoding atumor antigenwas shown to elicit a similar immune response against cancer cells in mice.\n\nThe first human clinical trial usingex vivodendritic cellstransfected with mRNA encoding tumor antigens (therapeutic cancer mRNA vaccine) was started in 2001.Four years later, the successful use ofmodified nucleosidesas a method to transport mRNA inside cells without setting off the body's defense system was reported.Clinical trial results of anmRNA vaccine directly injected into the body against cancer cellswere reported in 2008.\n\nBioNTechin 2008, andModernain 2010, were founded to develop mRNA biotechnologies.The US research agencyDARPAlaunched at this time thebiotechnologyresearch program ADEPT to develop emerging technologies for theUS military.The agency recognized the potential of nucleic acid technology for defense againstpandemicsand began to invest in the field.DARPA grants were seen as a vote of confidence that in turn encouraged other government agencies and private investors to invest in mRNA technology.DARPA awarded at the time a $25 million grant to Moderna.\n\nThe first human clinical trials using an mRNA vaccine against an infectious agent (rabies) began in 2013.Over the next few years, clinical trials of mRNA vaccines for a number of other viruses were started. mRNA vaccines for human use were studied for infectious agents such asinfluenza,Zika virus,cytomegalovirus, andChikungunya virus.\n\nTheCOVID-19 pandemic, and sequencing of the causative virusSARS-CoV-2at the beginning of 2020, led to the rapid development of the first approved mRNA vaccines.BioNTech and Moderna in December of the same year obtained approval for their mRNA-basedCOVID-19 vaccines. On 2 December, seven days after its final eight-week trial, the UKMedicines and Healthcare products Regulatory Agency(MHRA) became the first global medicines regulatorin historyto approve an mRNA vaccine, granting emergency authorization forPfizer–BioNTech's BNT162b2 COVID-19 vaccinefor widespread use.On 11 December, theFDAgaveemergency use authorizationfor the Pfizer–BioNTech COVID-19 vaccine and a week later similar approval for theModerna COVID-19 vaccine.\n\nThe goal of a vaccine is to stimulate theadaptive immune systemto createantibodiesthat precisely target that particularpathogen. The markers on the pathogen that the antibodies target are calledantigens.\n\nTraditional vaccines stimulate an antibody response by injecting eitherantigens, anattenuated(weakened) virus, aninactivated(dead) virus, or a recombinant antigen-encodingviral vector(harmless carrier virus with an antigentransgene) into the body. These antigens and viruses are prepared and grown outside the body.\n\nIn contrast, mRNA vaccines introduce a short-livedsynthetically createdfragment of the RNA sequence of a virus into the individual being vaccinated. These mRNA fragments are taken up bydendritic cellsthroughphagocytosis.The dendritic cells use their internal machinery (ribosomes) to read the mRNA and produce the viral antigens that the mRNA encodes.The bodydegradesthe mRNA fragments within a few days of introduction.Although non-immune cells can potentially also absorb vaccine mRNA, produce antigens, and display the antigens on their surfaces, dendritic cells absorb the mRNA globules much more readily.The mRNA fragments are translated in thecytoplasmand do not affect the body's genomic DNA, located separately in thecell nucleus.\n\nOnce the viral antigens are produced by the host cell, the normal adaptive immune system processes are followed. Antigens are broken down byproteasomes. Class I and class IIMHC moleculesthen attach to the antigen and transport it to the cellular membrane, \"activating\" the dendritic cell.Once activated, dendritic cells migrate tolymph nodes, where theypresent the antigentoT cellsandB cells.This triggers the production of antibodies specifically targeted to the antigen, ultimately resulting inimmunity.\n\nThe central component of a mRNA vaccine is its mRNA construct.Thein vitrotranscribedmRNA is generated from an engineeredplasmidDNA, which has anRNA polymerase promoterand sequence which corresponds to the mRNA construct. By combiningT7 phageRNA polymeraseand the plasmid DNA, the mRNA can be transcribed in the lab. Efficacy of the vaccine is dependent on the stability and structure of the designed mRNA.\n\nThein vitrotranscribed mRNA has the same structural components as natural mRNA ineukaryotic cells. It has a5' cap, a5'-untranslated region(UTR) and3'-UTR, anopen reading frame(ORF), which encodes the relevant antigen, and a3'-poly(A) tail. By modifying these different components of the synthetic mRNA, the stability and translational ability of the mRNA can be enhanced, and in turn, the efficacy of the vaccine improved.\n\nThe mRNA can be improved by using synthetic 5'-cap analogues which enhance the stability and increase protein translation. Similarly,regulatory elementsin the 5'-untranslated region and the 3'-untranslated region can be altered, and the length of the poly(A) tail optimized, to stabilize the mRNA and increase protein production. The mRNAnucleotidescan be modified to both decreaseinnate immune activationand increase the mRNA'shalf-lifein the host cell. Thenucleic acid sequenceandcodon usageimpacts protein translation. Enriching the sequence withguanine-cytosine contentimproves mRNA stability and half-life and, in turn, protein production. Replacing rarecodonswithsynonymous codonsfrequently used by the host cell also enhances protein production.\n\nFor a vaccine to be successful, sufficient mRNA must enter the host cellcytoplasmto stimulate production of the specific antigens. Entry of mRNA molecules, however, faces a number of difficulties. Not only are mRNA molecules too large to cross thecell membraneby simplediffusion, they are also negatively charged like the cell membrane, which causes a mutualelectrostatic repulsion. Additionally, mRNA is easily degraded byRNAasesin skin and blood.\n\nVarious methods have been developed to overcome these delivery hurdles. The method of vaccine delivery can be broadly classified by whether mRNA transfer into cells occurs within (in vivo) or outside (ex vivo) the organism.\n\nDendritic cellsdisplay antigens on theirsurfaces, leading to interactions withT cellsto initiate an immune response. Dendritic cells can be collected from patients and programmed with the desired mRNA, then administered back into patients to create an immune response.\n\nThe simplest way thatex vivodendritic cells take up mRNA molecules is throughendocytosis, a fairly inefficient pathway in the laboratory setting that can be significantly improved throughelectroporation.\n\nSince the discovery that the direct administration ofin vitrotranscribed mRNA leads to the expression of antigens in the body,in vivoapproaches have been investigated.They offer some advantages overex vivomethods, particularly by avoiding the cost of harvesting and adapting dendritic cells from patients and by imitating a regular infection.\n\nDifferent routes ofinjection, such asinto the skin,blood, ormuscles, result in varying levels of mRNA uptake, making the choice of administration route a critical aspect ofin vivodelivery. One study showed, in comparing different routes, thatlymph nodeinjection leads to the largest T-cell response.\n\nNaked mRNA injection means that thedeliveryof the vaccine is only done in abuffer solution.This mode of mRNA uptake has been known since the 1990s.The first worldwide clinical studies usedintradermal injectionsof naked mRNA for vaccination.A variety of methods have been used to deliver naked mRNA, such as subcutaneous, intravenous, and intratumoral injections. Although naked mRNA delivery causes an immune response, the effect is relatively weak, and after injection the mRNA is often rapidly degraded.\n\nCationic polymerscan be mixed with mRNA to generate protective coatings calledpolyplexes. These protect the recombinant mRNA fromribonucleasesand assist its penetration in cells.Protamineis a naturalcationicpeptideand has been used to encapsulate mRNA for vaccination.[non-primary source needed]\n\nThe first time the FDA approved the use oflipid nanoparticlesas a drug delivery system was in 2018, when the agency approved the firstsiRNAdrug,Onpattro.Encapsulating the mRNA molecule in lipid nanoparticles was a critical breakthrough for producing viable mRNA vaccines, solving a number of key technical barriers in delivering the mRNA molecule into the host cell.Research into using lipids to deliver siRNA to cells became a foundation for similar research into using lipids to deliver mRNA.However, new lipids had to be invented to encapsulate mRNA strands, which are much longer than siRNA strands.\n\nPrincipally, thelipidprovides a layer of protection against degradation, allowing more robust translational output. In addition, the customization of the lipid's outer layer allows the targeting of desired cell types throughligandinteractions. However, many studies have also highlighted the difficulty of studying this type of delivery, demonstrating that there is an inconsistency betweenin vivoandin vitroapplications of nanoparticles in terms of cellular intake.The nanoparticles can be administered to the body and transported via multiple routes, such asintravenouslyor through thelymphatic system.\n\nOne issue with lipid nanoparticles is that several of the breakthroughs leading to the practical use of that technology involve the use ofmicrofluidics. Microfluidic reaction chambers are difficult to scale up, since the entire point of microfluidics is to exploit the microscale behaviors of liquids. The only way around this obstacle is to run an extensive number of microfluidic reaction chambers in parallel, a novel task requiring custom-built equipment.For COVID-19 mRNA vaccines, this was the main manufacturing bottleneck. Pfizer used such a parallel approach to solve the scaling problem. After verifying that impingement jet mixers could not be directly scaled up,Pfizer made about 100 of the little mixers (each about the size of aU.S. half-dollar coin), connected them together with pumps and filters with a \"maze of piping,\"and set up a computer system to regulate flow and pressure through the mixers.\n\nAnother issue, with the large-scale use of this delivery method, is the availability of the novel lipids used to create lipid nanoparticles, especially ionizable cationic lipids. Before 2020, such lipids were manufactured in small quantities measured in grams or kilograms, and they were used for medical research and a handful of drugs for rare conditions. As the safety and efficacy of mRNA vaccines became clear in 2020, the few companies able to manufacture the requisite lipids were confronted with the challenge of scaling up production to respond to orders for several tons of lipids.\n\nIn addition to non-viral delivery methods,RNA viruseshave beenengineeredto achieve similar immunological responses. Typical RNA viruses used as vectors includeretroviruses,lentiviruses,alphavirusesandrhabdoviruses, each of which can differ in structure and function.Clinical studies have utilized such viruses on a range of diseases inmodel animalssuch asmice,chickenandprimates.\n\nmRNA vaccines offer specific advantages over traditionalvaccines.Because mRNA vaccines are not constructed from an active pathogen (or even an inactivated pathogen), they are non-infectious. In contrast, traditional vaccines require the production of pathogens, which, if done at high volumes, could increase the risks of localized outbreaks of the virus at the production facility.Another biological advantage of mRNA vaccines is that since the antigens are produced inside the cell, they stimulatecellular immunity, as well ashumoral immunity.\n\nmRNA vaccines have the production advantage that they can be designed swiftly. Moderna designed theirmRNA-1273vaccine for COVID-19 in 2 days.They can also be manufactured faster, more cheaply, and in a more standardized fashion (with fewer error rates in production), which can improve responsiveness to serious outbreaks.\n\nThe Pfizer–BioNTech vaccine originally required 110 days to mass-produce (before Pfizer began to optimize the manufacturing process to only 60 days), which was substantially faster than traditional flu and polio vaccines.Within that larger timeframe, the actual production time is only about 22 days: two weeks for molecular cloning of DNA plasmids and purification of DNA, four days for DNA-to-RNAtranscriptionand purification of mRNA, and four days to encapsulate mRNA in lipid nanoparticles followed byfill and finish.The majority of the days needed for each production run are allocated to rigorous quality control at each stage.\n\nIn addition to sharing the advantages of theoreticalDNA vaccinesover established traditionalvaccines, mRNA vaccines also have additional advantages over DNA vaccines. ThemRNAistranslatedin thecytosol, so there is no need for the RNA to enter thecell nucleus, and the risk of being integrated into the hostgenomeis averted.Modified nucleosides(for example,pseudouridines, 2'-O-methylated nucleosides) can be incorporated to mRNA to suppressimmune responsestimulation to avoid immediate degradation and produce a more persistent effect through enhanced translation capacity.Theopen reading frame (ORF)anduntranslated regions (UTR)of mRNA can be optimized for different purposes (a process called sequence engineering of mRNA), for example through enriching theguanine-cytosine contentor choosing specific UTRs known to increase translation.An additional ORF coding for areplicationmechanism can be added to amplify antigen translation and therefore immune response, decreasing the amount of starting material needed.\n\nBecause mRNA is fragile, some vaccines must be kept at very low temperatures to avoid degrading and thus giving little effective immunity to the recipient. Pfizer–BioNTech'sBNT162b2mRNA vaccine has to be kept between −80 and −60 °C (−112 and −76 °F).Moderna says theirmRNA-1273vaccine can be stored between −25 and −15 °C (−13 and 5 °F),which is comparable to a home freezer,and that it remains stable between 2 and 8 °C (36 and 46 °F) for up to 30 days.In November 2020,Naturereported, \"While it's possible that differences in LNP formulations or mRNA secondary structures could account for the thermostability differences [between Moderna and BioNtech], many experts suspect both vaccine products will ultimately prove to have similar storage requirements and shelf lives under various temperature conditions.\"Several platforms are being studied that may allow storage at higher temperatures.\n\nBefore 2020, no mRNA technology platform (drug or vaccine) had been authorized for use in humans, so there was a risk of unknown effects.The 2020 COVID-19 pandemic required faster production capability of mRNA vaccines, which made them attractive to national health organisations, and led to debate about the type of initial authorization mRNA vaccines should get (includingemergency use authorizationorexpanded access authorization) after the eight-week period of post-final human trials.\n\nReactogenicityis similar to that of conventional, non-RNA vaccines. However, those susceptible to anautoimmune responsemay have an adverse reaction to mRNA vaccines.The mRNA strands in the vaccine may elicit an unintended immune reaction – this entails thebody believing itself to be sick, and the person feeling as if they are as a result. To minimize this, mRNA sequences in mRNA vaccines are designed to mimic those produced by host cells.\n\nStrong but transient reactogenic effects were reported in trials of novel COVID-19 mRNA vaccines; most people will not experience severe side effects which include fever and fatigue. Severe side effects are defined as those that prevent daily activity.\n\nThe COVID-19 mRNA vaccines from Moderna and Pfizer–BioNTech had short-term efficacy rates of over 90 percent against the original SARS-CoV-2 virus. Prior to mRNA, drug trials on pathogens other than COVID-19 were not effective and had to be abandoned in the early phases of trials. The reason for the efficacy of the new mRNA vaccines is not clear.\n\nPhysician-scientistMargaret Liustated that the efficacy of the new COVID-19 mRNA vaccines could be due to the \"sheer volume of resources\" that went into development, or that the vaccines might be \"triggering a nonspecific inflammatory response to the mRNA that could be heightening its specific immune response, given that themodified nucleoside techniquereduced inflammation but hasn't eliminated it completely\", and that \"this may also explain the intense reactions such as aches and fevers reported in some recipients of the mRNA SARS-CoV-2 vaccines\". These reactions though severe were transient and another view is that they were believed to be a reaction to the lipid drug delivery molecules.In June 2021, the USFDAadded a warning about the possibility of increased risk of myocarditis and pericarditis for some people.\n\nThere is misinformation implying that mRNA vaccines could alter DNA in the nucleus.mRNA in thecytosolis very rapidly degraded before it would have time to gain entry into the cell nucleus. In fact, mRNA vaccines must be stored at very low temperature and free fromRNAsesto prevent mRNA degradation.Retroviruscan be single-stranded RNA (just as manySARS-CoV-2vaccines are single-stranded RNA) which enters the cell nucleus and usesreverse transcriptaseto make DNA from the RNA in the cell nucleus. A retrovirus has mechanisms to be imported into the nucleus, but other mRNA (such as the vaccine) lack these mechanisms. Once inside the nucleus, creation of DNA from RNA cannot occur without areverse transcriptase and appropriate primers, which both accompany a retrovirus, but which would not be present for other exogenous mRNA (such as a vaccine) even if it could enter the nucleus.\n\nmRNA vaccines use either non-amplifying (conventional) mRNA or self-amplifying mRNA.Pfizer–BioNTech and Moderna vaccines use non-amplifying mRNA. Both mRNA types continue to be investigated as vaccine methods against other potential pathogens and cancer.\n\nThe initial mRNA vaccines use a non-amplifying mRNA construct.Non-amplifying mRNA has only oneopen reading framethat codes for the antigen of interest.The total amount of mRNA available to the cell is equal to the amount delivered by the vaccine. Dosage strength is limited by the amount of mRNA that can be delivered by the vaccine.Non-amplifying vaccines replaceuridinewithN1-Methylpseudouridinein an attempt to reduce toxicity.\n\nSelf-amplifying mRNA (saRNA) vaccines replicate their mRNA after transfection.Self-amplifying mRNA has twoopen reading frames. The first frame, like conventional mRNA, codes for the antigen of interest. The second frame codes for anRNA-dependent RNA polymerase(and its helper proteins) which replicates the mRNA construct in the cell. This allows smaller vaccine doses.The mechanisms and consequently the evaluation of self-amplifying mRNA may be different, as self-amplifying mRNA is a much bigger molecule.\n\nSaRNA vaccines being researched include amalaria vaccine.The first saRNA Covid vaccine authorised was Gemcovac, in India in June 2022.The second wasARCT-154, developed by Arcturus Therapeutics. A version manufactured by Meiji Seika Pharma was authorised in Japan in November 2023.\n\nGSKbegan aphase 1 trialof an saRNA COVID-19 vaccine in 2021.Gritstone biostarted also started a phase 1 trial of an saRNA COVID-19 vaccine in 2021, used as abooster vaccine, with interim results published in 2023.The vaccine is designed to target both the spike protein of theSARS‑CoV‑2virus, and viral proteins that may be less prone to genetic variation, to provide greater protection against SARS‑CoV‑2 variants.saRNA vaccines must use uridine, which is required for reproduction to occur.",
      "sections": [
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 3,
          "heading": "Early research"
        },
        {
          "level": 3,
          "heading": "Development"
        },
        {
          "level": 3,
          "heading": "Acceleration"
        },
        {
          "level": 2,
          "heading": "Mechanism"
        },
        {
          "level": 2,
          "heading": "mRNA"
        },
        {
          "level": 2,
          "heading": "Delivery"
        },
        {
          "level": 3,
          "heading": "Ex vivo"
        },
        {
          "level": 3,
          "heading": "In vivo"
        },
        {
          "level": 2,
          "heading": "Advantages"
        },
        {
          "level": 3,
          "heading": "Traditional vaccines"
        },
        {
          "level": 3,
          "heading": "DNA vaccines"
        },
        {
          "level": 2,
          "heading": "Disadvantages"
        },
        {
          "level": 3,
          "heading": "Storage"
        },
        {
          "level": 3,
          "heading": "Recent"
        },
        {
          "level": 3,
          "heading": "Side effects"
        },
        {
          "level": 2,
          "heading": "Efficacy"
        },
        {
          "level": 2,
          "heading": "Hesitancy"
        },
        {
          "level": 2,
          "heading": "Amplification"
        },
        {
          "level": 3,
          "heading": "Non-amplifying"
        },
        {
          "level": 3,
          "heading": "Self-amplifying"
        }
      ],
      "raw_content_length": 1015539,
      "cleaned_content_length": 22741,
      "scraped_at": "2025-09-02 15:30:06",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "mRNA Vaccine",
      "discoverers": [
        "Katalin Karikó",
        "Drew Weissman"
      ],
      "discovery_years": [
        "2020"
      ],
      "discovery_timeline": [
        "1989: First successful transfection of designed mRNA into a cell published.",
        "1990: Naked mRNA injected into mice.",
        "1993: Liposome-encapsulated mRNA shown to stimulate T cells in mice.",
        "1994: Self-amplifying mRNA developed.",
        "1995: mRNA encoding a tumor antigen shown to elicit immune response in mice.",
        "2001: First human clinical trial of therapeutic cancer mRNA vaccine.",
        "2005: Modified nucleosides reported for mRNA transport.",
        "2008: Clinical trial results of mRNA vaccine against cancer.",
        "2008: BioNTech founded.",
        "2010: Moderna founded.",
        "2013: First human clinical trials of mRNA vaccine against rabies.",
        "2020: Pfizer-BioNTech and Moderna obtain authorization for COVID-19 vaccines."
      ],
      "mechanism": "mRNA vaccines use a copy of messenger RNA to instruct cells to produce a foreign protein that triggers an immune response against pathogens or cancer cells.",
      "key_features": [
        "Ease of design",
        "Speed of production",
        "Lower cost compared to traditional vaccines",
        "Induction of both cellular and humoral immunity",
        "No interaction with genomic DNA"
      ],
      "applications": [
        "COVID-19 vaccination",
        "Cancer immunotherapy",
        "Infectious disease prevention"
      ],
      "significance": "mRNA vaccines represent a novel approach to vaccination, enabling rapid development and deployment against emerging infectious diseases, exemplified by their use in the COVID-19 pandemic.",
      "institutions": [
        "BioNTech",
        "Moderna",
        "DARPA"
      ],
      "awards": [
        "2023 Nobel Prize in Physiology or Medicine awarded to Katalin Karikó and Drew Weissman"
      ],
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Gravitational_wave",
      "title": "Gravitational wave",
      "main_content": "Gravitational wavesare oscillations of thegravitational fieldthattravelthrough space at thespeed of light; they are generated by the relative motion ofgravitatingmasses.They were proposed byOliver Heavisidein 1893 and then later byHenri Poincaréin 1905 as the gravitational equivalent ofelectromagnetic waves.In 1916,Albert Einsteindemonstrated that gravitational waves result from hisgeneral theory of relativityas ripples inspacetime.\n\nGravitational waves transport energy asgravitational radiation, a form ofradiant energysimilar toelectromagnetic radiation.Newton's law of universal gravitation, part ofclassical mechanics, does not provide for their existence, instead asserting that gravity has instantaneous effect everywhere. Gravitational waves therefore stand as an important relativistic phenomenon that is absent from Newtonian physics.\n\nGravitational-wave astronomyhas the advantage that, unlike electromagnetic radiation, gravitational waves are not affected by intervening matter. Sources that can be studied this way includebinary starsystems composed ofwhite dwarfs,neutron stars,andblack holes; events such assupernovae; and the formation of theearly universeshortly after theBig Bang.\n\nThe first indirect evidence for the existence of gravitational waves came in 1974 from the observed orbital decay of theHulse–Taylor binary pulsar, which matched the decay predicted by general relativity for energy lost to gravitational radiation. In 1993,Russell Alan HulseandJoseph Hooton Taylor Jr.received theNobel Prize in Physicsfor this discovery.\n\nThe firstdirect observation of gravitational waveswas made in September 2015, when a signal generated by the merger of two black holes was received by theLIGOgravitational wave detectorsin Livingston, Louisiana, and in Hanford, Washington. The 2017 Nobel Prize in Physics was subsequently awarded toRainer Weiss,Kip ThorneandBarry Barishfor their role in the direct detection of gravitational waves.\n\nInAlbert Einstein'sgeneral theory of relativity, gravity is treated as a phenomenon resulting from thecurvature of spacetime. This curvature is caused by the presence of mass.(See:Stress–energy tensor)If the masses move, the curvature of spacetime changes. If the motion is not spherically symmetric, the motion can cause gravitational waves which propagate away at thespeed of light.\n\nAs a gravitational wave passes an observer, that observer will find spacetime distorted by the effects ofstrain. Distances between objects increase and decrease rhythmically as the wave passes, at a frequency equal to that of the wave. The magnitude of this effect isinversely proportionalto the distance (not distance squared) from the source.\n\nInspiralingbinary neutron starsare predicted to be a powerful source of gravitational waves as theycoalesce, due to the very large acceleration of their masses as theyorbitclose to one another. However, due to the astronomical distances to these sources, the effects when measured on Earth are predicted to be very small, having strains of less than 1 part in 1020.\n\nScientists demonstrate the existence of these waves with highly-sensitive detectors at multiple observation sites. As of 2012[update], theLIGOandVirgoobservatories were the most sensitive detectors, operating at resolutions of about one part in5×1022.The Japanese detectorKAGRAwas completed in 2019; its first joint detection with LIGO and VIRGO was reported in 2021.Another European ground-based detector, theEinstein Telescope, is under development. A space-based observatory, theLaser Interferometer Space Antenna(LISA), is also being developed by theEuropean Space Agency.\n\nGravitational waves do not strongly interact with matter in the way that electromagnetic radiation does.This allows for the observation of events involving exotic objects in the distant universe that cannot be observed with more traditional means such asoptical telescopesorradio telescopes; accordingly,gravitational wave astronomygives new insights into the workings of the universe.\n\nIn particular, gravitational waves could be of interest to cosmologists as they offer a possible way of observing the very early universe. This is not possible with conventional astronomy, since beforerecombinationthe universe was opaque to electromagnetic radiation.Precise measurements of gravitational waves will also allow scientists to test more thoroughly the general theory of relativity.\n\nIn principle, gravitational waves can exist at any frequency. Very low frequency waves can be detected usingpulsar timing arrays. In this technique, the timing of approximately 100 pulsars spread widely across our galaxy is monitored over the course of years. Detectable changes in the arrival time of their signals can result from passing gravitational waves generated by mergingsupermassive black holes(SMBH) with wavelengths measured in lightyears. These timing changes can be used to locate the source of the waves.\n\nUsing this technique, astronomers have discovered the 'hum' of various SMBH mergers occurring in the universe.Stephen HawkingandWerner Israellist different frequency bands for gravitational waves that could plausibly be detected, ranging from 10−7Hz up to 1011Hz.\n\nThe speed of gravitational waves in thegeneral theory of relativityis equal to thespeed of lightin vacuum,c.Within the theory ofspecial relativity, the constantcis not only about light; instead it is the highest possible speed for any interaction in nature. Formally,cis a conversion factor for changing the unit of time to the unit of space.This makes it the only speed which does not depend either on the motion of an observer or a source of light and/or gravity.\n\nThus, the speed of \"light\" is also the speed of gravitational waves, and, further, the speed of any massless particle. Such particles include thegluon(carrier of the strong force), thephotonsthat make up light (hence carrier of electromagnetic force), and the hypotheticalgravitons(which are the presumptive field particles associated with gravity; however, an understanding of the graviton, if any exist, requires an as-yet unavailable theory ofquantum gravity).\n\nIn August 2017, theLIGOand Virgo detectors received a gravitational wave signal,GW170817, at nearly the same time as gamma ray satellites and optical telescopes received signals from its source in galaxyNGC 4993, about 130 million light years away.This measurement constrained the experimental difference between the speed of gravitational waves and light to be smaller than one part in 10−15.\n\nThe possibility of gravitational waves and that those might travel at the speed of light was discussed in 1893 byOliver Heaviside, using the analogy between the inverse-square law of gravitation and theelectrostatic force.In 1905,Henri Poincaréproposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformationsand suggested that, in analogy to an acceleratingelectrical chargeproducingelectromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves.\n\nIn 1915 Einstein published hisgeneral theory of relativity, a complete relativistic theory of gravitation. He conjectured, like Poincaré, that the equation would produce gravitational waves, but, as he mentions in a letter to Schwarzschild in February 1916,these could not be similar to electromagnetic waves. Electromagnetic waves can be produced by dipole motion, requiring both a positive and a negative charge. Gravitation has no equivalent to negative charge. Einstein continued to work through the complexity of the equations of general relativity to find an alternative wave model. The result was published in June 1916,and there he came to the conclusion that the gravitational wave must propagate with the speed of light, and there must, in fact, be three types of gravitational waves dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse byHermann Weyl.\n\nHowever, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922,Arthur Eddingtonshowed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they \"propagate at the speed of thought\".This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at thespeed of lightregardless of coordinate system. In 1936, Einstein andNathan Rosensubmitted a paper toPhysical Reviewin which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed byHoward P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar with the concept of peer review, angrily withdrew the manuscript, never to publish inPhysical Reviewagain. Nonetheless, his assistantLeopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere.In 1956,Felix Piraniremedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observableRiemann curvature tensor.\n\nAt the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmitenergy. This matter was settled by a thought experiment proposed byRichard Feynmanduring the first \"GR\" conference atChapel Hillin 1957. In short, his argument known as the \"sticky bead argument\" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had donework. Shortly after,Hermann Bondipublished a detailed version of the \"sticky bead argument\".This later led to a series of articles (1959 to 1989) by Bondi and Pirani that established the existence ofplane wave solutions for gravitational waves.\n\nPaul Diracfurther postulated the existence of gravitational waves, declaring them to have \"physical significance\" in his 1959 lecture at theLindau Meetings.Further, it was Dirac who predicted gravitational waves with a well-defined energy density in 1964.\n\nAfter the Chapel Hill conference,Joseph Weberstarted designing and building the first gravitational wave detectors now known asWeber bars. In 1969, Weber claimed to have detected the first gravitational waves, and by 1970 he was \"detecting\" signals regularly from theGalactic Center; however, the frequency of detection soon raised doubts on the validity of his observations as the implied rate of energy loss of theMilky Waywould drain our galaxy of energy on a timescale much shorter than its inferred age. These doubts were strengthened when, by the mid-1970s, repeated experiments from other groups building their own Weber bars across the globe failed to find any signals, and by the late 1970s consensus was that Weber's results were spurious.\n\nIn the same period, the first indirect evidence of gravitational waves was discovered. In 1974,Russell Alan HulseandJoseph Hooton Taylor, Jr.discovered thefirst binary pulsar, which earned them the 1993Nobel Prize in Physics.Pulsar timing observations over the next decade showed a gradual decay of the orbital period of the Hulse–Taylor pulsar that matched the loss of energy and angular momentum in gravitational radiation predicted by general relativity.\n\nThis indirect detection of gravitational waves motivated further searches, despite Weber's discredited result. Some groups continued to improve Weber's original concept, while others pursued the detection of gravitational waves using laser interferometers. The idea of using a laser interferometer for this seems to have been floated independently by various people, including M.E. Gertsenshtein and V. I. Pustovoit in 1962,and Vladimir B. Braginskiĭ in 1966. The first prototypes were developed in the 1970s byRobert L. Forwardand Rainer Weiss.In the decades that followed, ever more sensitive instruments were constructed, culminating in the construction ofGEO600,LIGO, andVirgo.\n\nAfter years of producing null results, improved detectors became operational in 2015. On 11 February 2016, theLIGO-Virgocollaborations announced thefirst observation of gravitational waves,from a signal (dubbedGW150914) detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36solar massesmerging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times thepowerof all the stars in the observable universe combined.The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second.The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves.The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from theSouthern Celestial Hemisphere, in the rough direction of (but much farther away than) theMagellanic Clouds.The confidence level of this being an observation of gravitational waves was 99.99994%.\n\nA year earlier, the BICEP2 collaboration claimed that they had detected the imprint of gravitational waves in thecosmic microwave background. However, they were later forced to retract this result.\n\nIn 2017, theNobel Prize in Physicswas awarded toRainer Weiss,Kip ThorneandBarry Barishfor their role in the detection of gravitational waves.\n\nIn 2023, NANOGrav, EPTA, PPTA, InPTA, and CPTA announced that they found evidence of agravitational wave background.North American Nanohertz Observatory for Gravitational Wavesstates, that they were created over cosmological time scales by supermassive black holes, identifying the distinctiveHellings-Downs curvein 15 years of radio observations of 67 pulsars.Similar results are published by European Pulsar Timing Array, who claimed a3σ{\\displaystyle 3\\sigma }-significance. They expect that a5σ{\\displaystyle 5\\sigma }-significance will be achieved by 2025 by combining the measurements of several collaborations.\n\nGravitational waves are constantly passingEarth; however, even the strongest have a minuscule effect since their sources are generally at a great distance. For example, the waves given off by the cataclysmic final merger ofGW150914reached Earth after travelling over a billionlight-years, as a ripple inspacetimethat changed the length of a 4 km LIGO arm by a thousandth of the width of aproton, proportionally equivalent to changing the distance to thenearest staroutside the Solar System by one hair's width.This tiny effect from even extreme gravitational waves makes them observable on Earth only with the most sophisticated detectors.\n\nThe effects of a passing gravitational wave, in an extremely exaggerated form, can be visualized by imagining a perfectly flat region ofspacetimewith a group of motionless test particles lying in a plane, e.g., the surface of a computer screen. As a gravitational wave passes through the particles along a line perpendicular to the plane of the particles, i.e., following the observer's line of vision into the screen, the particles will follow the distortion in spacetime, oscillating in a \"cruciform\" manner, as shown in the animations. The area enclosed by the test particles does not change and there is no motion along the direction of propagation.[citation needed]\n\nThe oscillations depicted in the animation are exaggerated for the purpose of discussion – in reality a gravitational wave has a very smallamplitude(as formulated inlinearized gravity). However, they help illustrate the kind of oscillations associated with gravitational waves as produced by a pair of masses in acircular orbit. In this case the amplitude of the gravitational wave is constant, but its plane ofpolarizationchanges or rotates at twice the orbital rate, so the time-varying gravitational wave size, or 'periodic spacetime strain', exhibits a variation as shown in the animation.If the orbit of the masses is elliptical then the gravitational wave's amplitude also varies with time according to Einstein'squadrupole formula.\n\nAs with otherwaves, there are a number of characteristics used to describe a gravitational wave:\n\nThe speed, wavelength, and frequency of a gravitational wave are related by the equationc=λf, just like the equation for alight wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth.\n\nIn the above example, it is assumed that the wave islinearly polarizedwith a \"plus\" polarization, writtenh+. Polarization of a gravitational wave is just like polarization of a light wave except that the polarizations of a gravitational wave are 45 degrees apart, as opposed to 90 degrees.In particular, in a \"cross\"-polarized gravitational wave,h×, the effect on the test particles would be basically the same, but rotated by 45 degrees, as shown in the second animation. Just as with light polarization, the polarizations of gravitational waves may also be expressed in terms ofcircularly polarizedwaves. Gravitational waves are polarized because of the nature of their source.\n\nIn general terms, gravitational waves are radiated by large, coherent motions of immense mass, especially in regions where gravity is so strong thatNewtonian gravitybegins to fail.\n\nThe effect does not occur in a purely spherically symmetric system.A simple example of this principle is a spinningdumbbell. If the dumbbell spins around its axis of symmetry, it will not radiate gravitational waves; if it tumbles end over end, as in the case of two planets orbiting each other, it will radiate gravitational waves. The heavier the dumbbell, and the faster it tumbles, the greater is the gravitational radiation it will give off. In an extreme case, such as when the two weights of the dumbbell are massive stars like neutron stars or black holes, orbiting each other quickly, then significant amounts of gravitational radiation would be given off.\n\nSome more detailed examples:\n\nMore technically, the second time derivative of thequadrupole moment(or thel-th time derivative of thel-thmultipole moment) of an isolated system'sstress–energy tensormust be non-zero in order for it to emit gravitational radiation. This is analogous to the changing dipole moment of charge or current that is necessary for the emission ofelectromagnetic radiation.\n\nGravitational waves carry energy away from their sources and, in the case of orbiting bodies, this is associated with an in-spiral or decrease in orbit.Imagine for example a simple system of two masses – such as the Earth–Sun system – moving slowly compared to the speed of light in circular orbits. Assume that these two masses orbit each other in a circular orbit in thex–yplane. To a good approximation, the masses follow simple Keplerianorbits. However, such an orbit represents a changingquadrupole moment. That is, the system will give off gravitational waves.\n\nIn theory, the loss of energy through gravitational radiation could eventually drop the Earth into theSun. However, the total energy of the Earth orbiting the Sun (kinetic energy+gravitational potential energy) is about 1.14×1036joulesof which only 200watts(joules per second) is lost through gravitational radiation, leading to adecay in the orbitby about 1×10−15meters per day or roughly the diameter of aproton. At this rate, it would take the Earth approximately 3×1013times more than the currentage of the universeto spiral onto the Sun. This estimate overlooks the decrease inrover time, but the radius varies only slowly for most of the time and plunges at later stages, asr(t)=r0(1−ttcoalesce)1/4,{\\displaystyle r(t)=r_{0}\\left(1-{\\frac {t}{t_{\\text{coalesce}}}}\\right)^{1/4},}withr0{\\displaystyle r_{0}}the initial radius andtcoalesce{\\displaystyle t_{\\text{coalesce}}}the total time needed to fully coalesce.\n\nMore generally, the rate of orbital decay can be approximated by\n\nwhereris the separation between the bodies,ttime,Gthegravitational constant,cthespeed of light, andm1andm2the masses of the bodies. This leads to an expected time to merger of\n\nCompact starslikewhite dwarfsandneutron starscan be constituents of binaries. For example, a pair ofsolar massneutron stars in a circular orbit at a separation of 1.89×108m (189,000 km) has an orbital period of 1,000 seconds, and an expected lifetime of 1.30×1013seconds or about 414,000 years. Such a system could be observed byLISAif it were not too far away. A far greater number of white dwarf binaries exist with orbital periods in this range. White dwarf binaries havemasses in the order of the Sun, and diameters in the order of the Earth. They cannot get much closer together than 10,000 km before they willmergeand explode in asupernovawhich would also end the emission of gravitational waves. Until then, their gravitational radiation would be comparable to that of a neutron star binary.\n\nWhen the orbit of a neutron star binary has decayed to 1.89×106m (1890 km), its remaining lifetime is about 130,000 seconds or 36 hours. The orbital frequency will vary from 1 orbit per second at the start, to 918 orbits per second when the orbit has shrunk to 20 km at merger. The majority of gravitational radiation emitted will be at twice the orbital frequency. Just before merger, the inspiral could be observed by LIGO if such a binary were close enough. LIGO has only a few minutes to observe this merger out of a total orbital lifetime that may have been billions of years. In August 2017, LIGO and Virgo observed the first binary neutron star inspiral inGW170817, and 70 observatories collaborated to detect the electromagnetic counterpart, akilonovain the galaxyNGC 4993, 40megaparsecsaway, emitting a shortgamma ray burst(GRB 170817A) seconds after the merger, followed by a longer optical transient (AT 2017gfo) powered byr-processnuclei. Advanced LIGO detectors should be able to detect such events up to 200 megaparsecs away; at this range, around 40 detections per year would be expected.\n\nBlack hole binaries emit gravitational waves during their in-spiral,merger, and ring-down phases. Hence, in the early 1990s the physics community rallied around a concerted effort to predict the waveforms of gravitational waves from these systems with theBinary Black Hole Grand Challenge Alliance.The largest amplitude of emission occurs during the merger phase, which can be modeled with the techniques of numerical relativity.The first direct detection of gravitational waves,GW150914, came from the merger of two black holes.\n\nA supernova is atransient astronomical eventthat occurs during the last stellar evolutionary stages of a massive star's life, whose dramatic and catastrophic destruction is marked by one final titanic explosion. This explosion can happen in one of many ways, but in all of them a significant proportion of the matter in the star is blown away into the surrounding space at extremely high velocities (up to 10% of the speed of light). Unless there is perfect spherical symmetry in these explosions (i.e., unless matter is spewed out evenly in all directions), there will be gravitational radiation from the explosion. This is because gravitational waves aregenerated by a changing quadrupole moment, which can happen only when there is asymmetrical movement of masses. Since the exact mechanism by which supernovae take place is not fully understood, it is not easy to model the gravitational radiation emitted by them.\n\nAs noted above, a mass distribution will emit gravitational radiation only when there is spherically asymmetric motion among the masses. Aspinning neutron starwill generally emit no gravitational radiation because neutron stars are highly dense objects with a strong gravitational field that keeps them almost perfectly spherical. In some cases, however, there might be slight deformities on the surface called \"mountains\", which are bumps extending no more than 10 centimeters (4 inches) above the surface,that make the spinning spherically asymmetric. This gives the star a quadrupole moment that changes with time, and it will emit gravitational waves until the deformities are smoothed out.\n\nGravitational waves from the early universe could provide a unique probe for cosmology. Because these wave interact very weakly with matter they would propagate freely from very early time when other signals are trapped by the large density of energy. If this gravitational radiation could be detected it would begravitational wave backgroundcomplementary to thecosmic microwave backgrounddata.However the existence of primordial gravitational waves can also be inferred from their effects.\nModels of \"slow-roll\"cosmic inflationin theearly universepredicts primordialgravitational wavesthat would impact the polarisation of the cosmic microwave background, creating a specific pattern ofB-mode polarization. Detection of this pattern would support the theory of inflation and their strength can confirm and exclude different models of inflation.While claims that this characteristic pattern of B-mode polarization had been measured byBICEP2instrumentwere later attributed tocosmic dustdue to new results of thePlanck experiment,subsequent reanalysis with compensation for foreground dust show limits in agreement with results fromLambda-CDMmodels.\n\nWater waves, sound waves, and electromagnetic waves are able to carryenergy,momentum, andangular momentumand by doing so they carry those away from the source.Gravitational waves perform the same function. Thus, for example, a binary system loses angular momentum as the two orbiting objects spiral towards each other – the angular momentum is radiated away by gravitational waves.\n\nThe waves can also carry off linear momentum, a possibility that has some interesting implications forastrophysics.After two supermassive black holes coalesce, emission of linear momentum can produce a \"kick\" with amplitude as large as 4000 km/s. This is fast enough to eject the coalesced black hole completely from its host galaxy. Even if the kick is too small to eject the black hole completely, it can remove it temporarily from the nucleus of the galaxy, after which it will oscillate about the center, eventually coming to rest.A kicked black hole can also carry a star cluster with it, forming ahyper-compact stellar system.Or it may carry gas, allowing the recoiling black hole to appear temporarily as a \"naked quasar\".\nThequasarSDSS J092712.65+294344.0is thought to contain a recoiling supermassive black hole.\n\nLikeelectromagnetic waves, gravitational waves should exhibitshifting of wavelengthand frequency due to the relative velocities of the source and observer (theDoppler effect), but also due to distortions ofspacetime, such ascosmic expansion.Redshiftingofgravitational waves is different from redshiftingdue togravity (gravitational redshift).\n\nIn the framework ofquantum field theory, thegravitonis the name given to a hypotheticalelementary particlespeculated to be theforce carrierthat mediatesgravity. However the graviton is not yet proven to exist, and noscientific modelyet exists that successfully reconcilesgeneral relativity, which describes gravity, and theStandard Model, which describes all otherfundamental forces. Attempts, such asquantum gravity, have been made, but are not yet accepted.\n\nIf such a particle exists, it is expected to bemassless(because the gravitational force appears to have unlimited range) and must be aspin-2boson. It can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field must couple to (interact with) the stress-energy tensor in the same way that the gravitational field does; therefore if a massless spin-2 particle were ever discovered, it would be likely to be the graviton without further distinction from other massless spin-2 particles.Such a discovery would unite quantum theory with gravity.\n\nDue to the weakness of the coupling of gravity to matter, gravitational waves experience very little absorption or scattering, even as they travel over astronomical distances. In particular, gravitational waves are expected to be unaffected by the opacity of the very early universe. In these early phases, space had not yet become \"transparent\", so observations based upon light, radio waves, and other electromagnetic radiation that far back into time are limited or unavailable. Therefore, gravitational waves are expected in principle to have the potential to provide a wealth of observational data about the very early universe.\n\nThe difficulty in directly detecting gravitational waves means it is also difficult for a single detector to identify by itself the direction of a source. Therefore, multiple detectors are used, both to distinguish signals from other \"noise\" by confirming the signal is not of earthly origin, and also to determine direction by means oftriangulation. This technique uses the fact that the waves travel at thespeed of lightand will reach different detectors at different times depending on their source direction. Although the differences in arrival time may be just a fewmilliseconds, this is sufficient to identify the direction of the origin of the wave with considerable precision.\n\nOnly in the case ofGW170814were three detectors operating at the time of the event, therefore, the direction is precisely defined. The detection by all three instruments led to a very accurate estimate of the position of the source, with a 90% credible region of just 60deg2, a factor 20 more accurate than before.\n\nDuring the past century,astronomyhas been revolutionized by the use of new methods for observing the universe. Astronomical observations were initially made usingvisible light.Galileo Galileipioneered the use of telescopes to enhance these observations. However, visible light is only a small portion of theelectromagnetic spectrum, and not all objects in the distant universe shine strongly in this particular band. More information may be found, for example, in radio wavelengths. Usingradio telescopes, astronomers have discoveredpulsarsandquasars, for example. Observations in themicrowaveband led to the detection offaint imprintsof theBig Bang, a discoveryStephen Hawkingcalled the \"greatest discovery of the century, if not all time\". Similar advances in observations usinggamma rays,x-rays,ultraviolet light, andinfrared lighthave also brought new insights to astronomy. As each of these regions of the spectrum has opened, new discoveries have been made that could not have been made otherwise. The astronomy community hopes that the same holds true of gravitational waves.\n\nGravitational waves have two important and unique properties. First, there is no need for any type of matter to be present nearby in order for the waves to be generated by a binary system of uncharged black holes, which would emit no electromagnetic radiation. Second, gravitational waves can pass through any intervening matter without being scattered significantly. Whereas light from distant stars may be blocked out byinterstellar dust, for example, gravitational waves will pass through essentially unimpeded. These two features allow gravitational waves to carry information about astronomical phenomena heretofore never observed by humans.\n\nThe sources of gravitational waves described above are in the low-frequency end of the gravitational-wave spectrum (10−7to 105Hz). An astrophysical source at the high-frequency end of the gravitational-wave spectrum (above 105Hz and probably 1010Hz) generates[clarification needed]relic gravitational waves that are theorized to be faint imprints of the Big Bang like the cosmic microwave background.At these high frequencies it is potentially possible that the sources may be \"man made\"that is, gravitational waves generated and detected in the laboratory.\n\nAsupermassive black hole, created from the merger of the black holes at the center of two merging galaxies detected by theHubble Space Telescope, is theorized to have been ejected from the merger center by gravitational waves.\n\nAlthough the waves from the Earth–Sun system are minuscule, astronomers can point to other sources for which the radiation should be substantial. One important example is theHulse–Taylor binary– a pair of stars, one of which is apulsar.The characteristics of their orbit can be deduced from theDoppler shiftingof radio signals given off by the pulsar. Each of the stars is about 1.4M☉and the size of their orbits is about 1/75 of theEarth–Sun orbit, just a few times larger than the diameter of our own Sun. The combination of greater masses and smaller separation means that the energy given off by the Hulse–Taylor binary will be far greater than the energy given off by the Earth–Sun system – roughly 1022times as much.\n\nThe information about the orbit can be used to predict how much energy (and angular momentum) would be radiated in the form of gravitational waves. As the binary system loses energy, the stars gradually draw closer to each other, and the orbital period decreases. The resulting trajectory of each star is an inspiral, a spiral with decreasing radius. General relativity precisely describes these trajectories; in particular, the energy radiated in gravitational waves determines the rate of decrease in the period, defined as the time interval between successive periastrons (points of closest approach of the two stars). For the Hulse–Taylor pulsar, the predicted current change in radius is about 3 mm per orbit, and the change in the 7.75 hr period is about 2 seconds per year. Following a preliminary observation showing an orbital energy loss consistent with gravitational waves,careful timing observations by Taylor and Joel Weisberg dramatically confirmed the predicted period decrease to within 10%.With the improved statistics of more than 30 years of timing data since the pulsar's discovery, the observed change in the orbital period currently matches the prediction from gravitational radiation assumed by general relativity to within 0.2 percent.In 1993, spurred in part by this indirect detection of gravitational waves, the Nobel Committee awarded the Nobel Prize in Physics to Hulse and Taylor for \"the discovery of a new type of pulsar, a discovery that has opened up new possibilities for the study of gravitation.\"The lifetime of this binary system, from the present to merger is estimated to be a few hundred million years.\n\nInspirals are very important sources of gravitational waves. Any time two compact objects (white dwarfs, neutron stars, orblack holes) are in close orbits, they send out intense gravitational waves. As they spiral closer to each other, these waves become more intense. At some point they should become so intense that direct detection by their effect on objects on Earth or in space is possible. This direct detection is the goal of several large-scale experiments.\n\nThe only difficulty is that most systems like the Hulse–Taylor binary are so far away. The amplitude of waves given off by the Hulse–Taylor binary at Earth would be roughlyh≈ 10−26. There are some sources, however, that astrophysicists expect to find that produce much greater amplitudes ofh≈ 10−20. At least eight other binary pulsars have been discovered.\n\nGravitational waves are not easily detectable. When they reach the Earth, they have a small amplitude with strain approximately 10−21, meaning that an extremely sensitive detector is needed, and that other sources of noise can overwhelm the signal.Gravitational waves are expected to have frequencies 10−16Hz <f< 104Hz.\n\nThough the Hulse–Taylor observations were very important, they give onlyindirectevidence for gravitational waves. A more conclusive observation would be adirectmeasurement of the effect of a passing gravitational wave, which could also provide more information about the system that generated it. Any such direct detection is complicated by theextraordinarily smalleffect the waves would produce on a detector. The amplitude of a spherical wave will fall off as the inverse of the distance from the source (the 1/Rterm in the formulas forhabove). Thus, even waves from extreme systems like merging binary black holes die out to very small amplitudes by the time they reach the Earth. Astrophysicists expect that some gravitational waves passing the Earth may be as large ash≈ 10−20, but generally no bigger.\n\nA simple device theorised to detect the expected wave motion is called aWeber bar– a large, solid bar of metal isolated from outside vibrations. This type of instrument was the first type of gravitational wave detector. Strains in space due to an incident gravitational wave excite the bar'sresonant frequencyand could thus be amplified to detectable levels. Conceivably, a nearby supernova might be strong enough to be seen without resonant amplification. With this instrument,Joseph Weberclaimed to have detected daily signals of gravitational waves. His results, however, were contested in 1974 by physicistsRichard GarwinandDavid Douglass. Modern forms of the Weber bar are still operated,cryogenicallycooled, withsuperconducting quantum interference devicesto detect vibration. Weber bars are not sensitive enough to detect anything but extremely powerful gravitational waves.\n\nMiniGRAILis a spherical gravitational wave antenna using this principle. It is based atLeiden University, consisting of an exactingly machined 1,150 kg sphere cryogenically cooled to 20 millikelvins.The spherical configuration allows for equal sensitivity in all directions, and is somewhat experimentally simpler than larger linear devices requiring high vacuum. Events are detected by measuringdeformation of the detector sphere. MiniGRAIL is highly sensitive in the 2–4 kHz range, suitable for detecting gravitational waves from rotating neutron star instabilities or small black hole mergers.\n\nThere are currently two detectors focused on the higher end of the gravitational wave spectrum (10−7to 105Hz): one atUniversity of Birmingham, England,and the other atINFNGenoa, Italy. A third is under development atChongqing University, China. The Birmingham detector measures changes in the polarization state of amicrowavebeam circulating in a closed loop about one meter across. Both detectors are expected to be sensitive to periodic spacetime strains ofh~2×10−13/√Hz, given as anamplitude spectral density. The INFN Genoa detector is a resonant antenna consisting of two coupled sphericalsuperconductingharmonic oscillators a few centimeters in diameter. The oscillators are designed to have (when uncoupled) almost equal resonant frequencies. The system is currently expected to have a sensitivity to periodic spacetime strains ofh~2×10−17/√Hz, with an expectation to reach a sensitivity ofh~2×10−20/√Hz. The Chongqing University detector is planned to detect relic high-frequency gravitational waves with the predicted typical parameters ≈1011Hz (100 GHz) andh≈10−30to 10−32.\n\nA more sensitive class of detector uses a laserMichelson interferometerto measure gravitational-wave induced motion between separated 'free' masses.This allows the masses to be separated by large distances (increasing the signal size); a further advantage is that it is sensitive to a wide range of frequencies (not just those near a resonance as is the case for Weber bars). After years of development ground-based interferometers made the first detection of gravitational waves in 2015.\n\nCurrently, the most sensitive isLIGO– the Laser Interferometer Gravitational Wave Observatory. LIGO has three detectors: one inLivingston, Louisiana, one at theHanford siteinRichland, Washingtonand a third (formerly installed as a second detector at Hanford) that is planned to be moved toIndia. Each observatory has twolight storage armsthat are 4 kilometers in length. These are at 90 degree angles to each other, with the light passing through 1 m diameter vacuum tubes running the entire 4 kilometers. A passing gravitational wave will slightly stretch one arm as it shortens the other. This is the motion to which an interferometer is most sensitive.\n\nEven with such long arms, the strongest gravitational waves will only change the distance between the ends of the arms by at most roughly 10−18m. LIGO should be able to detect gravitational waves as small ash~5×10−22. Upgrades to LIGO andVirgoshould increase the sensitivity still further. Another highly sensitive interferometer,KAGRA, which is located in theKamioka Observatoryin Japan, is in operation since February 2020. A key point is that a tenfold increase in sensitivity (radius of 'reach') increases the volume of space accessible to the instrument by one thousand times. This increases the rate at which detectable signals might be seen from one per tens of years of observation, to tens per year.\n\nInterferometric detectors are limited at high frequencies byshot noise, which occurs because the lasers produce photons randomly; one analogy is to rainfall – the rate of rainfall, like the laser intensity, is measurable, but the raindrops, like photons, fall at random times, causing fluctuations around the average value. This leads to noise at the output of the detector, much like radio static. In addition, for sufficiently high laser power, the random momentum transferred to the test masses by the laser photons shakes the mirrors, masking signals of low frequencies. Thermal noise (e.g.,Brownian motion) is another limit to sensitivity. In addition to these 'stationary' (constant) noise sources, all ground-based detectors are also limited at low frequencies byseismicnoise and other forms of environmental vibration, and other 'non-stationary' noise sources; creaks in mechanical structures, lightning or other large electrical disturbances, etc. may also create noise masking an event or may even imitate an event. All of these must be taken into account and excluded by analysis before detection may be considered a true gravitational wave event.\n\nThe simplest gravitational waves are those with constant frequency. The waves given off by a spinning, non-axisymmetric neutron star would be approximatelymonochromatic: apure toneinacoustics. Unlike signals from supernovae or binary black holes, these signals evolve little in amplitude or frequency over the period it would be observed by ground-based detectors. However, there would be some change in the measured signal, because ofDoppler shiftingcaused by the motion of the Earth. Despite the signals being simple, detection is extremely computationally expensive, because of the long stretches of data that must be analysed.\n\nTheEinstein@Homeproject is adistributed computingproject similar toSETI@homeintended to detect this type of gravitational wave. By taking data from LIGO and GEO, and sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise.\n\nSpace-based interferometers, such asLISAandDECIGO, are also being developed. LISA's design calls for three test masses forming an equilateral triangle, with lasers from each spacecraft to each other spacecraft forming two independent interferometers. LISA is planned to occupy a solar orbit trailing the Earth, with each arm of the triangle being 2.5 million kilometers.This puts the detector in an excellent vacuum far from Earth-based sources of noise, though it will still be susceptible to heat,shot noise, and artifacts caused bycosmic raysandsolar wind.\n\nPulsarsare rapidly rotating stars. A pulsar emits beams of radio waves that, like lighthouse beams, sweep through the sky as the pulsar rotates. The signal from a pulsar can be detected by radio telescopes as a series of regularly spaced pulses, essentially like the ticks of a clock. GWs affect the time it takes the pulses to travel from the pulsar to a telescope on Earth. Apulsar timing arrayusesmillisecond pulsarsto seek out perturbations due to GWs in measurements of the time of arrival of pulses to a telescope, in other words, to look for deviations in the clock ticks. To detect GWs, pulsar timing arrays search for a distinct quadrupolar pattern of correlation and anti-correlation between the time of arrival of pulses from different pulsar pairs as a function of their angular separation in the sky.Although pulsar pulses travel through space for hundreds or thousands of years to reach us, pulsar timing arrays are sensitive to perturbations in their travel time of much less than a millionth of a second.\n\nThe most likely source of GWs to which pulsar timing arrays are sensitive are supermassive black hole binaries, which form from the collision of galaxies.In addition to individual binary systems, pulsar timing arrays are sensitive to a stochastic background of GWs made from the sum of GWs from many galaxy mergers. Other potential signal sources includecosmic stringsand the primordial background of GWs fromcosmic inflation.\n\nGlobally there are seven active pulsar timing array projects. TheNorth American Nanohertz Observatory for Gravitational Waves(NANOGrav) uses data collected by theArecibo Radio Telescope,Green Bank Telescope,Very Large Array, and theCanadian Hydrogen Intensity Mapping Experiment. The AustralianParkes Pulsar Timing Array(PPTA) uses data from theParkes radio-telescope. TheEuropean Pulsar Timing Array(EPTA) uses data from the four largest telescopes in Europe: theLovell Telescope, theWesterbork Synthesis Radio Telescope, theEffelsberg Telescopeand theNancay Radio Telescope. TheIndian Pulsar Timing Array(InPTA) uses data from theGiant Metrewave Radio Telescope, and theMeerKAT Pulsar Timing Array(MPTA) uses data from theMeerKATradio telescope. With theAfrican Pulsar Timing(APT) group, these collaborations also collaborate under the title of theInternational Pulsar Timing Arrayproject.Additionally, theChinese Pulsar Timing Array(CPTA) uses data from theFive-hundred-meter Aperture Spherical Telescope.\n\nIn June 2023, NANOGrav, EPTA, InPTA, PPTA, and CPTA published the first evidence for a stochasticgravitational wave background.In particular, they announced evidence for theHellings-Downs curve, the tell-tale sign of the gravitational wave origin of the observed background.In December 2024, MPTA also published evidence for the gravitational wave background.\n\nPrimordial gravitational waves are gravitational waves observed in thecosmic microwave background. They were allegedly detected by theBICEP2instrument, an announcement made on 17 March 2014, which was withdrawn on 30 January 2015 (\"the signal can be entirely attributed todustin the Milky Way\").\n\nOn 11 February 2016, theLIGOcollaboration announced thefirst observation of gravitational waves, from a signal detected at 09:50:45 GMT on 14 September 2015of two black holes with masses of 29 and 36solar massesmerging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times thepowerof all the stars in the observable universe combined.The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second.The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves.The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from theSouthern Celestial Hemisphere, in the rough direction of (but much farther away than) theMagellanic Clouds.The gravitational waves were observed in the region more than 5 sigma(in other words, 99.99997% chances of showing/getting the same result), the probability of finding enough to have been assessed/considered as the evidence/proof in anexperimentofstatistical physics.\n\nSince then LIGO and Virgo have reported moregravitational wave observationsfrom merging black hole binaries.\n\nOn 16 October 2017, the LIGO and Virgo collaborations announced the first-ever detection of gravitational waves originating from the coalescence of a binary neutron star system. The observation of theGW170817transient, which occurred on 17 August 2017, allowed for constraining the masses of the neutron stars involved between 0.86 and 2.26 solar masses. Further analysis allowed a greater restriction of the mass values to the interval 1.17–1.60 solar masses, with the total system mass measured to be 2.73–2.78 solar masses. The inclusion of the Virgo detector in the observation effort allowed for an improvement of the localization of the source by a factor of 10. This in turn facilitated the electromagnetic follow-up of the event. The signal lasted about 100 seconds, much longer than the few seconds measured from binary black holes.Also in contrast to the case of binary black hole mergers, binary neutron star mergers were expected to yield an electromagnetic counterpart, that is, a light signal associated with the event. A gamma-ray burst (GRB 170817A) was detected by theFermi Gamma-ray Space Telescope, occurring 1.7 seconds after the gravitational wave transient. The signal, originating near the galaxyNGC 4993, was associated with the neutron star merger. This was corroborated by the electromagnetic follow-up of the event (AT 2017gfo), involving 70 telescopes and observatories and yielding observations over a large region of the electromagnetic spectrum which further confirmed the neutron star nature of the merged objects and the associatedkilonova.\n\nIn 2021, the detection of the first two neutron star-black hole binaries by the LIGO and VIRGO detectors was published in the Astrophysical Journal Letters, allowing to first set bounds on the quantity of such systems. No neutron star-black hole binary had ever been observed using conventional means before the gravitational observation.\n\nIn 1964,L. Halpernand B. Laurent theoretically proved that gravitational spin-2 electron transitions are possible in atoms. Compared to electric and magnetic transitions the emission probability is extremely low. Stimulated emission was discussed for increasing the efficiency of the process. Due to the lack of mirrors or resonators for gravitational waves, they determined that a single pass GASER (a kind of laser emitting gravitational waves) is practically unfeasible.\n\nIn 1998, the possibility of a different implementation of the above theoretical analysis was proposed by Giorgio Fontana. The required coherence for a practical GASER could be obtained byCooper pairsinsuperconductorsthat are characterized by a macroscopic collective wave-function. Cupratehigh temperature superconductorsare characterized by the presence of s-wave and d-waveCooper pairs. Transitions between s-wave and d-wave are gravitational spin-2. Out of equilibrium conditions can be induced by injecting s-wave Cooper pairs from a low temperature superconductor, for instanceleadorniobium, which is pure s-wave, by means of aJosephson junctionwith high critical current. The amplification mechanism can be described as the effect ofsuperradiance, and 10 cubic centimeters of cuprate high temperature superconductor seem sufficient for the mechanism to properly work. A detailed description of the approach can be found in \"High Temperature Superconductors as Quantum Sources of Gravitational Waves: The HTSC GASER\". Chapter 3 of this book.\n\nAn episode of the 1962 Russian science-fiction novelSpace ApprenticebyArkady and Boris Strugatskyshows an experiment monitoring the propagation of gravitational waves at the expense of annihilating a chunk of asteroid15 Eunomiathe size ofMount Everest.\n\nInStanislaw Lem's 1986 novelFiasco, a \"gravity gun\" or \"gracer\" (gravity amplification by collimated emission of resonance) is used to reshape a collapsar, so that the protagonists can exploit the extreme relativistic effects and make an interstellar journey.\n\nInGreg Egan's 1997 novelDiaspora, the analysis of a gravitational wave signal from the inspiral of a nearby binary neutron star reveals that its collision and merger is imminent, implying a large gamma-ray burst is going to impact the Earth.\n\nInLiu Cixin's 2006Remembrance of Earth's Pastseries, gravitational waves are used as an interstellar broadcast signal, which serves as a central plot point in the conflict between civilizations within the galaxy.",
      "sections": [
        {
          "level": 2,
          "heading": "Introduction"
        },
        {
          "level": 2,
          "heading": "Speed of gravity"
        },
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 2,
          "heading": "Effects of passing"
        },
        {
          "level": 3,
          "heading": "Binaries"
        },
        {
          "level": 3,
          "heading": "Black hole binaries"
        },
        {
          "level": 3,
          "heading": "Supernova"
        },
        {
          "level": 3,
          "heading": "Spinning neutron stars"
        },
        {
          "level": 3,
          "heading": "Primordial gravitational wave"
        },
        {
          "level": 2,
          "heading": "Properties and behaviour"
        },
        {
          "level": 3,
          "heading": "Energy, momentum, and angular momentum"
        },
        {
          "level": 3,
          "heading": "Redshifting"
        },
        {
          "level": 3,
          "heading": "Quantum gravity, wave-particle aspects, and graviton"
        },
        {
          "level": 3,
          "heading": "Significance for study of the early universe"
        },
        {
          "level": 3,
          "heading": "Determining direction of travel"
        },
        {
          "level": 2,
          "heading": "Gravitational wave astronomy"
        },
        {
          "level": 2,
          "heading": "Detection"
        },
        {
          "level": 3,
          "heading": "Indirect detection"
        },
        {
          "level": 3,
          "heading": "Difficulties"
        },
        {
          "level": 3,
          "heading": "Ground-based detectors"
        },
        {
          "level": 3,
          "heading": "Space-based interferometers"
        },
        {
          "level": 3,
          "heading": "Using pulsar timing arrays"
        },
        {
          "level": 3,
          "heading": "Primordial gravitational wave"
        },
        {
          "level": 3,
          "heading": "LIGO and Virgo observations"
        },
        {
          "level": 2,
          "heading": "In fiction"
        }
      ],
      "raw_content_length": 926355,
      "cleaned_content_length": 53098,
      "scraped_at": "2025-09-02 15:30:14",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Gravitational Waves",
      "discoverers": [
        "Oliver Heaviside",
        "Henri Poincaré",
        "Albert Einstein",
        "Russell Alan Hulse",
        "Joseph Hooton Taylor Jr.",
        "Rainer Weiss",
        "Kip Thorne",
        "Barry Barish"
      ],
      "discovery_years": [
        "2015"
      ],
      "discovery_timeline": [
        "1893: Proposal by Oliver Heaviside",
        "1905: Proposal by Henri Poincaré",
        "1916: Albert Einstein's demonstration of gravitational waves",
        "1974: Indirect evidence from Hulse–Taylor binary pulsar",
        "2015: First direct observation by LIGO",
        "2017: Nobel Prize awarded for direct detection"
      ],
      "mechanism": "Gravitational waves are ripples in spacetime caused by the acceleration of masses, propagating at the speed of light.",
      "key_features": [
        "Not affected by intervening matter",
        "Can observe distant astronomical events",
        "Provides insights into the early universe"
      ],
      "applications": [
        "Gravitational-wave astronomy",
        "Studying binary star systems",
        "Observing supernovae",
        "Investigating the early universe"
      ],
      "significance": "Gravitational waves provide a new way to observe cosmic events and test general relativity, offering insights into the fundamental workings of the universe.",
      "institutions": [
        "LIGO",
        "Virgo",
        "KAGRA",
        "European Space Agency"
      ],
      "awards": [
        "Nobel Prize in Physics 1993",
        "Nobel Prize in Physics 2017"
      ],
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Higgs_boson",
      "title": "Higgs boson",
      "main_content": "TheHiggs boson, sometimes called theHiggs particle,is anelementary particlein theStandard Modelofparticle physicsproduced by thequantum excitationof theHiggs field,one of thefieldsinparticle physicstheory.In the Standard Model, the Higgs particle is a massivescalar bosonthatcouplesto (interacts with) particles whose mass arises from their interactions with the Higgs Field, has zerospin, even (positive)parity, noelectric charge, and nocolour charge.It is also very unstable,decayinginto other particles almost immediately upon generation.\n\nThe Higgs field is ascalar fieldwith two neutral and two electrically charged components that form a complexdoubletof theweak isospinSU(2) symmetry. Its \"sombrero potential\" leads it to take a nonzero valueeverywhere(including otherwise empty space), whichbreakstheweak isospinsymmetry of theelectroweak interactionand, via theHiggs mechanism, gives a rest mass to all massive elementary particles of the Standard Model, including the Higgs boson itself. The existence of the Higgs field became the last unverified part of the Standard Model of particle physics, and for several decades was considered \"the central problem in particle physics\".\n\nBoth the field and thebosonare named after physicistPeter Higgs, who in 1964,along with five other scientistsin three teams, proposed theHiggs mechanism, a way forsome particles to acquire mass. All fundamental particles known at the timeshould be massless at very high energies, but fully explaining how some particles gain mass at lower energies had been extremely difficult. If these ideas were correct, a particle known as a scalar boson (with certain properties) should also exist. This particle was called the Higgs boson and could be used to test whether the Higgs field was the correct explanation.\n\nAfter a40-year search, a subatomic particle with the expected properties was discovered in 2012 by theATLASandCMSexperiments at theLarge Hadron Collider(LHC) atCERNnearGeneva, Switzerland. The new particle was subsequently confirmed to match the expected properties of a Higgs boson. Physicists from two of the three teams,Peter HiggsandFrançois Englert, were awarded theNobel Prize in Physicsin 2013 for their theoretical predictions. Although Higgs's name has come to be associated with this theory, several researchers between about 1960 and 1972 independently developed different parts of it.\n\nIn the media, the Higgs boson has often been called the \"God particle\" after the 1993 bookThe God Particleby Nobel LaureateLeon M. Lederman. The name has been criticised by physicists,including Peter Higgs.\n\nPhysicists explain thefundamental particlesandforcesof the universe in terms of theStandard Model– a widely accepted framework based onquantum field theorythat predicts almost all known particles and forces aside fromgravitywith great accuracy. (A separate theory,general relativity, is used for gravity.) In the Standard Model, the particles and forces in nature (aside from gravity) arise from properties ofquantum fieldsknown asgauge invarianceandsymmetries. Forces in the Standard Model aretransmitted by particlesknown asgauge bosons.\n\nGauge-invariant theoriesare theories with a useful feature, namely that changes to certain quantities make no difference to experimental outcomes. For example, increasing theelectric potentialof anelectromagnetby 100 volts does not itself cause any change to themagnetic fieldthat it produces. Similarly, the measuredspeed of lightin vacuum remains unchanged, whatever the location in time and space, and whatever the localgravitational field.\n\nIn these theories, the gauge is a quantity that can be changed with no resultant effect. This independence of the results from some changes is called gauge invariance, and these changes reflect symmetries of the underlying physics. These symmetries provide constraints on the fundamental forces and particles of the physical world. Gauge invariance is therefore an important property within particle physics theory. The gauge symmetries are closely connected toconservation lawsand are described mathematically usinggroup theory. Quantum field theory and the Standard Model are both gauge-invariant theories – meaning that the gauge symmetries allow theoretical derivation of properties of the universe.\n\nQuantum field theories based on gauge invariance had been used with great success in understanding theelectromagneticandstrong forces, but by around 1960, all attempts to create agauge invarianttheory for theweak force(and its combination with the electromagnetic force, known together as theelectroweak interaction) had consistently failed. As a result of these failures, gauge theories began to fall into disrepute. The problem was thatsymmetry requirementsfor these two forces incorrectly predicted that the weak force's gauge bosons (W and Z) would have zero mass (in the specialized terminology of particle physics, \"mass\" refers specifically to a particle'srest mass). But experiments showed the W and Z gauge bosons had non-zero (rest) mass.\n\nFurther, many promising solutions seemed to require the existence of extra particles known asGoldstone bosons, but evidence suggested these did not exist. This meant that either gauge invariance was an incorrect approach, or something unknown was giving the weak force's W and Z bosons their mass, and doing it in a way that did not imply the existence of Goldstone bosons. By the late 1950s and early 1960s, physicists were at a loss as to how to resolve these issues, or how to create a comprehensive theory for particle physics.\n\nIn the late 1950s,Yoichiro Namburecognised thatspontaneous symmetry breaking, a process whereby a symmetric system becomes asymmetric, could occur under certain conditions.Symmetry breaking is when some variable takes on a value that does not reflect the symmetries that the underlying laws have, such as when the space of all stable configurations possesses a given symmetry but the stable configurations do not individually possess that symmetry.In 1962, physicistPhilip Anderson, an expert incondensed matter physics, observed that symmetry breaking plays a role insuperconductivity, and suggested that it could also be part of the answer to the problem of gauge invariance in particle physics.\n\nSpecifically, Anderson suggested that theGoldstone bosonsthat would result from symmetry breaking might instead, in some circumstances, be \"absorbed\"by the masslessW and Z bosons. If so, perhaps the Goldstone bosons would not exist, and the W and Z bosons couldgain mass, solving both problems at once. Similar behaviour was already theorised in superconductivity.In 1964, this was shown to be theoretically possible by physicistsAbraham KleinandBenjamin Lee, at least for some limited (non-relativistic) cases.\n\nFollowing the 1963and early 1964papers, three groups of researchers independently developed these theories more completely, in what became known as the1964 PRL symmetry breaking papers. All three groups reached similar conclusions and for all cases, not just some limited cases. They showed that the conditions for electroweak symmetry would be \"broken\" if an unusual type offieldexisted throughout the universe, and indeed, there would be no Goldstone bosons and some existing bosons wouldacquire mass.\n\nThe field required for this to happen (which was purely hypothetical at the time) became known as theHiggs field(afterPeter Higgs, one of the researchers) and the mechanism by which it led to symmetry breaking became known as theHiggs mechanism. A key feature of the necessary field is that the field would havelessenergy when it had a non-zero value than when it was zero, unlike every other known field; therefore, the Higgs field has a non-zero value (orvacuum expectation) everywhere. This non-zero value could in theory break electroweak symmetry. It was the first proposal that was able to show, within a gauge invariant theory, how the weak force gauge bosons could have mass despite their governing symmetry.\n\nAlthough these ideas did not gain much initial support or attention, by 1972 they had been developed into a comprehensive theory and gave\"sensible\" resultsthat accurately described particles known at the time, and which, with exceptional accuracy,predicted several other particles, which were discovered during the following years.During the 1970s, these theories rapidly became theStandard Modelof particle physics.\n\nTo allow symmetry breaking, the Standard Model includes afieldof the kind needed to \"break\" electroweak symmetry and give particles their correct mass. This field, which became known as the Higgs field, was hypothesized to exist throughout space, and to break some symmetry laws of theelectroweak interaction, triggering the Higgs mechanism. It would therefore cause the W and Z gauge bosons of the weak force to be massive at all temperatures below an extremely high value.When the weak force bosons acquire mass, this affects the distance they can freely travel, which becomes very small, also matching experimental findings.Furthermore, it was later realised that the same field would also explain, in a different way, why other fundamental constituents of matter (includingelectronsandquarks) have mass.\n\nUnlike all other known fields, such as theelectromagnetic field, the Higgs field is ascalar field, and has a non-zero average value invacuum.\n\nPrior to the discovery of the Higgs Boson, there was no direct evidence that the Higgs field exists, but even without direct evidence, the accuracy of predictions within the Standard Model led scientists to believe the theory might be correct. By the 1980s, the question of whether the Higgs field exists, and whether the entire Standard Model is correct, had come to be regarded as one of the most importantunanswered questions in particle physics. The existence of the Higgs field became the last unverified part of the Standard Model of particle physics, and for several decades was considered \"the central problem in particle physics\".\n\nFor many decades, scientists had no way to determine whether the Higgs field exists because the technology needed for its detection did not exist at that time. If the Higgs field did exist, then it would be unlike any other known fundamental field, but it also was possible that these key ideas, or even the entire Standard Model, were somehow incorrect.\n\nThe hypothesised Higgs theory made several key predictions.One crucial prediction was that a matchingparticle, called the Higgs boson, should also exist. Proving the existence of the Higgs boson would prove the existence of the Higgs field, and therefore finally prove the Standard Model. Therefore, there was an extensivesearch for the Higgs bosonas a way to prove the Higgs field itself exists.\n\nAlthough the Higgs field would exist and be nonzero everywhere, proving its existence was far from easy. In principle, it can be proved to exist by detecting itsexcitations, which manifest as Higgs particles (Higgs bosons), but these are extremely difficult to produce and detect due to the energy required to produce them and their very rare production even if there is sufficient energy available. It was, therefore, several decades before the first evidence of the Higgs boson would be found.Particle colliders, detectors, and computers capable of looking for Higgs bosons took more than 30 years(c.1980–2010)to develop. The importance of thisfundamental questionled to a40-year search, and the construction of one of the world's mostexpensive and complex experimental facilityto date,CERN'sLarge Hadron Collider(LHC),in an attempt to create Higgs bosons and other particles for observation and study.\n\nOn 4 July 2012, the discovery of a new particle with a mass between125 and 127GeV/c2was announced; physicists suspected that it was the Higgs boson.Since then, the particle has been shown to behave, interact, and decay in many of the ways predicted for Higgs particles by the Standard Model, including having evenparityand zerospin,two fundamental attributes of a Higgs boson. This also means it is the first elementaryscalar particlediscovered in nature.\n\nBy March 2013, the existence of the Higgs boson was confirmed, and therefore the concept of some type of Higgs field throughout space is strongly supported.The presence of the field, now confirmed by experimental investigation, explainswhy some fundamental particles have (a rest) mass, despite thesymmetriescontrolling their interactions implying that they should be \"massless\". It also resolves several other long-standing problems, such as the reason for the extremely short distance travelled by theweak forcebosons, and therefore the weak force's extremely short range. As of 2018, in-depth research shows the particle continuing to behave in line with predictions for the Standard Model's Higgs boson. More studies are needed to verify with higher precision that the discovered particle has all of the properties predicted or whether, as described by some theories, multiple Higgs bosons exist.\n\nThe nature and properties of this field are now being investigated further, using more data collected at the LHC.\n\nVarious analogieshave been used to describe the Higgs field and boson, including analogies with well-known symmetry-breaking effects such as therainbowandprism,electric fields, and ripples on the surface of water.\n\nOther analogies based on the resistance of macroscopic objects moving through media (such as people moving through crowds, or some objects moving throughsyrupormolasses) are commonly used but misleading, since the Higgs field does not actually resist particles, and the effect of mass is not caused by resistance.\n\nIn the Standard Model, theHiggsbosonis a massivescalar bosonwhose mass must be found experimentally. Its mass has been determined to be125.35±0.15 GeV/c2by CMS (2022)and125.11±0.11 GeV/c2by ATLAS (2023). It is the only particle that remains massive even at very high energies. It has zerospin, even (positive)parity, noelectric charge, nocolour charge, and itcouplesto (interacts with) mass.It is also very unstable,decayinginto other particles almost immediately via several possible pathways.\n\nTheHiggs fieldis ascalar field, with two neutral and two electrically charged components that form a complexdoubletof theweak isospinSU(2) symmetry. Unlike any other known quantum field, it has asombrero potential. This shape means that below extremely high cross-over temperature of159.5±1.5GeV/kBsuch asthose seenduring the firstpicosecond(10−12s) of theBig Bang, the Higgs field in itsground statehas less energy when it is nonzero, resulting in a nonzerovacuum expectation value. Therefore, in today's universe the Higgs field has a nonzero value everywhere (including in otherwise empty space). This nonzero value in turn breaks the weak isospin SU(2) symmetry of theelectroweak interactioneverywhere. (Technically the non-zero expectation value converts theLagrangian'sYukawa couplingterms into mass terms.) When this happens, three components of the Higgs field are \"absorbed\" by the SU(2) and U(1)gauge bosons(theHiggs mechanism) to become the longitudinal components of the now-massiveW and Z bosonsof theweak force. The remaining electrically neutral component either manifests as a Higgs boson, or may couple separately to other particles known asfermions(via Yukawa couplings), causing these toacquire massas well.\n\nEven though the knowledge of many of the Higgs boson properties has advanced significantly since its discovery, the Higgs boson’s self-coupling remains unmeasured. The shape of the Higgs potential in theStandard Modelincludes both trilinear and quartic self-couplings, which are key to understanding the complete shape of the potential and the nature of the Higgs field and EWSB.Higgs boson pair productionoffers a direct experimental probe of the self-coupling λ at the electroweak scale.\n\nEvidence for the Higgs field and its properties has been extremely significant for many reasons. The primary importance of the Higgs boson is that it completes the mechanism by which the heavy electroweak bosons acquire mass, and it is fortunate that the mass is such that it is able to be examined using existing experimental technology, as a way to confirm and study the entire Higgs field theory.Conversely, evidence that the Higgs field and boson didnotexist within the expected mass range would have also been significant.\n\nThe Higgs boson validates theStandard Modelmechanism ofmass generationfor the weak bosons, and can provide masses to the fermions. As more precise measurements of its properties are made, more advanced extensions may be suggested or excluded. As experimental means to measure the Higgs field behaviour and interactions are developed, this fundamental field may be better understood. If the Higgs boson had not been discovered, the Standard Model would have needed to be modified or superseded.\n\nRelated to this, it is a widely held belief among many physicists that there is likely to be \"new\"physics beyond the Standard Model, and the Standard Model will at some point be extended or superseded. The discovery of the Higgs boson, as well as the many measured collisions occurring at the LHC, provide physicists with a sensitive tool to search their data for any evidence of the failure of the Standard Model, and might provide considerable evidence to guide researchers into future theoretical developments.\n\nBelow an extremely high temperature,electroweak symmetry breakingcauses theelectroweak interactionto manifest in part as the short-rangedweak force, which is carried by massivegauge bosons. In thehistory of the universe, electroweak symmetry breaking is believed to have happened at about1picosecond(10−12s)after theBig Bang, when the universe was at a temperature159.5±1.5GeV/kB.This symmetry breaking is required foratomsand other structures to form, as well as for nuclear reactions in stars, such as theSun. The Higgs field is responsible for this symmetry breaking.\n\nThe Higgs field is pivotal ingenerating the massesofquarksand chargedleptons(through Yukawa coupling) and theW and Z gauge bosons(through the Higgs mechanism), although it was the generation of mass for the weak bosons which is the most significant factor – providing terms in the Standard Model Lagrangian that allow for the generation of fermion masses, was a useful, but less significant by product. The fermion masses must be entered by hand, essentially determining the relative strength of the coupling of the fermion to the Higgs field.\n\nThe Higgs field does not \"create\" massout of nothing, nor is the Higgs field responsible for the mass of all particles. For example, approximately 99% of the mass ofbaryons(composite particlessuch as theprotonandneutron), is due instead toquantum chromodynamic binding energy, which is the sum of thekinetic energiesof quarks and theenergiesof the masslessgluonsmediating thestrong interactioninside the baryons.In Higgs-based theories, the property of \"mass\" is a manifestation ofpotential energytransferred to fundamental particles when they interact (\"couple\") with the Higgs field.\n\nThe Higgs field is the only scalar (spin-0) field to be detected; all the other fundamental fields in the Standard Model are spin-⁠1/2⁠fermionsor spin-1 bosons.According toRolf-Dieter Heuer, director general of CERN when the Higgs boson was discovered, this existence proof of a scalar field is almost as important as the Higgs's role in determining the mass of other particles. It suggests that other hypothetical scalar fields suggested by other theories, from theinflatontoquintessence, could perhaps exist as well.\n\nThere has been considerable scientific research on possible links between the Higgs field and theinflaton– a hypothetical field suggested as the explanation for theexpansion of spaceduringthe first fraction of a secondof theuniverse(known as the \"inflationary epoch\"). Some theories suggest that a fundamental scalar field might be responsible for this phenomenon; the Higgs field is such a field, and its existence has led to papers analysing whether it could also be theinflatonresponsible for thisexponentialexpansion of the universe during theBig Bang. Such theories are highly tentative and face significant problems related tounitarity, but may be viable if combined with additional features such as large non-minimal coupling, aBrans–Dickescalar, or other \"new\" physics, and they have received treatments suggesting that Higgs inflation models are still of interest theoretically.\n\nIn the Standard Model, there exists the possibility that the underlying state of our universe – known as the \"vacuum\" – islong-lived, but not completely stable. In this scenario, the universe as we know it could effectively be destroyed by collapsing into amore stable vacuum state.This was sometimes misreported as the Higgs boson \"ending\" the universe.If the masses of the Higgs boson andtop quarkare known more precisely, and the Standard Model provides an accurate description of particle physics up to extreme energies of thePlanck scale, then it is possible to calculate whether the vacuum is stable or merely long-lived.A Higgs mass of125–127 GeV/c2seems to be extremely close to the boundary for stability, but a definitive answer requires much more precise measurements of thepole massof the top quark.New physics can change this picture.\n\nIf measurements of the Higgs boson suggest that our universe lies within afalse vacuumof this kind, then it would imply – more than likely in many billions of years– that the universe's forces, particles, and structures could cease to exist as we know them (and be replaced by different ones), if a true vacuum happened tonucleate.It also suggests that the Higgsself-couplingλand itsβλfunction could be very close to zero at the Planck scale, with \"intriguing\" implications, including theories of gravity and Higgs-based inflation.A future electron–positron collider would be able to provide the precise measurements of the top quark needed for such calculations.\n\nMore speculatively, the Higgs field has also been proposed as theenergy of the vacuum, which at the extreme energies of the first moments of theBig Bangcaused the universe to be a kind of featureless symmetry of undifferentiated, extremely high energy. In this kind of speculation, the single unified field of aGrand Unified Theoryis identified as (or modelled upon) the Higgs field, and it is through successive symmetry breakings of the Higgs field, or some similar field, atphase transitionsthat the presently known forces and fields of the universe arise.\n\nThe relationship (if any) between the Higgs field and the presently observedvacuum energy densityof the universe has also come under scientific study. As observed, the present vacuum energy density is extremely close to zero, but the energy densities predicted from the Higgs field, supersymmetry, and other current theories are typically many orders of magnitude larger. It is unclear how these should be reconciled. Thiscosmological constantproblem remains a majorunanswered problemin physics.\n\nParticle physicists studymattermade fromfundamental particleswhose interactions are mediated by exchange particles –gauge bosons– acting asforce carriers. At the beginning of the 1960s a number of these particles had been discovered or proposed, along with theories suggesting how they relate to each other, some of which had already been reformulated asfield theoriesin which the objects of study are not particles and forces, butquantum fieldsand theirsymmetries.However, attempts to produce quantum field models for two of the four knownfundamental forces– theelectromagnetic forceand theweak nuclear force– and then tounify these interactions, were still unsuccessful.\n\nOne known problem was thatgauge invariantapproaches, includingnon-abelianmodels such asYang–Mills theory(1954), which held great promise for unified theories, also seemed to predict known massive particles as massless.Goldstone's theorem, relating tocontinuous symmetrieswithin some theories, also appeared to rule out many obvious solutions,since it appeared to show that zero-mass particles known asGoldstone bosonswould also have to exist that simply were \"not seen\".According toGuralnik, physicists had \"no understanding\" how these problems could be overcome.\n\nParticle physicist and mathematician Peter Woit summarised the state of research at the time:\n\nYang and Mills work onnon-abelian gauge theoryhad one huge problem: inperturbation theoryit has massless particles which don't correspond to anything we see. One way of getting rid of this problem is now fairly well understood, the phenomenon ofconfinementrealized inQCD, where the strong interactions get rid of the massless \"gluon\" states at long distances. By the very early sixties, people had begun to understand another source of massless particles: spontaneous symmetry breaking of a continuous symmetry. WhatPhilip Andersonrealized and worked out in the summer of 1962 was that, when you havebothgauge symmetryandspontaneous symmetry breaking, the massless Nambu–Goldstone mode [which gives rise to Goldstone bosons] can combine with the massless gauge field modes [which give rise to massless gauge bosons] to produce a physical massive vector field [gauge bosons with mass]. This is what happens insuperconductivity, a subject about which Anderson was (and is) one of the leading experts.[text condensed]\n\nThe Higgs mechanism is a process by whichvector bosonscan acquirerest masswithoutexplicitly breaking gauge invariance, as a byproduct ofspontaneous symmetry breaking.Initially, the mathematical theory behind spontaneous symmetry breaking was conceived and published within particle physics byYoichiro Nambuin 1960(andsomewhat anticipatedbyErnst Stueckelbergin 1938), and the concept that such a mechanism could offer a possible solution for the \"mass problem\" was originally suggested in 1962 by Philip Anderson, who had previously written papers on broken symmetry and its outcomes in superconductivity.Anderson concluded in his 1963 paper on the Yang–Mills theory, that \"considering the superconducting analog ... [t]hese two types of bosons seem capable of canceling each other out ... leaving finite mass bosons\"),and in March 1964,Abraham KleinandBenjamin Leeshowed that Goldstone's theorem could be avoided this way in at least some non-relativistic cases, and speculated it might be possible in truly relativistic cases.\n\nThese approaches were quickly developed into a fullrelativisticmodel, independently and almost simultaneously, by three groups of physicists: byFrançois EnglertandRobert Broutin August 1964;byPeter Higgsin October 1964;and byGerald Guralnik,Carl Hagen, andTom Kibble(GHK) in November 1964.Higgs also wrote a short, but important,response published in September 1964 to an objection byGilbert,which showed that if calculating within the radiation gauge, Goldstone's theorem and Gilbert's objection would become inapplicable.Higgs later described Gilbert's objection as prompting his own paper.Properties of the model were further considered by Guralnik in 1965,by Higgs in 1966,by Kibble in 1967,and further by GHK in 1967.The original three 1964 papers demonstrated that when agauge theoryis combined with an additional charged scalar field that spontaneously breaks the symmetry, the gauge bosons may consistently acquire a finite mass.In 1967,Steven WeinbergandAbdus Salamindependently showed how a Higgs mechanism could be used to break the electroweak symmetry ofSheldon Glashow'sunified model for the weak and electromagnetic interactions,(itself an extension of work bySchwinger), forming what became theStandard Modelof particle physics. Weinberg was the first to observe that this would also provide mass terms for the fermions.\n\nAt first, these seminal papers on spontaneous breaking of gauge symmetries were largely ignored, because it was widely believed that the (non-Abelian gauge) theories in question were a dead-end, and in particular that they could not berenormalised. In 1971–72,Martinus VeltmanandGerard 't Hooftproved renormalisation of Yang–Mills was possible in two papers covering massless, and then massive, fields.Their contribution, and the work of others on therenormalisation group– including \"substantial\" theoretical work by Russian physicistsLudvig Faddeev,Andrei Slavnov,Efim Fradkin, andIgor Tyutin– was eventually \"enormously profound and influential\",but even with all key elements of the eventual theory published there was still almost no wider interest. For example,Colemanfound in a study that \"essentially no-one paid any attention\" to Weinberg's paper prior to 1971and discussed byDavid Politzerin his 2004 Nobel speech.– now the most cited in particle physics– and even in 1970 according to Politzer, Glashow's teaching of the weak interaction contained no mention of Weinberg's, Salam's, or Glashow's own work.In practice, Politzer states, almost everyone learned of the theory due to physicistBenjamin Lee, who combined the work of Veltman and 't Hooft with insights by others, and popularised the completed theory.In this way, from 1971, interest and acceptance \"exploded\"and the ideas were quickly absorbed in the mainstream.\n\nThe resulting electroweak theory and Standard Model haveaccurately predicted(among other things)weak neutral currents,three bosons, thetopandcharm quarks, and with great precision, the mass and other properties of some of these.Many of those involved eventually won Nobel Prizes or other renowned awards. A 1974 paper and comprehensive review inReviews of Modern Physicscommented that \"while no one doubted the [mathematical] correctness of these arguments, no one quite believed that nature was diabolically clever enough to take advantage of them\",adding that the theory had so far produced accurate answers that accorded with experiment, but it was unknown whether the theory was fundamentally correct.By 1986 and again in the 1990s it became possible to write that understanding and proving the Higgs sector of the Standard Model was \"the central problem today in particle physics\".\n\nThe three papers written in 1964 were each recognised as milestone papers duringPhysical Review Letters's50th anniversary celebration.Their six authors were also awarded the 2010J. J. Sakurai Prize for Theoretical Particle Physicsfor this work.(A controversy also arose the same year, because in the event of a Nobel Prize only up to three scientists could be recognised, with six being credited for the papers.) Two of the three PRL papers (by Higgs and by GHK) contained equations for the hypotheticalfieldthat eventually would become known as the Higgs field and its hypotheticalquantum, the Higgs boson.Higgs's subsequent 1966 paper showed the decay mechanism of the boson; only a massive boson can decay and the decays can prove the mechanism.[citation needed]\n\nIn the paper by Higgs the boson is massive, and in a closing sentence Higgs writes that \"an essential feature\" of the theory \"is the prediction of incomplete multiplets ofscalarandvector bosons\".(Frank Closecomments that 1960s gauge theorists were focused on the problem of masslessvectorbosons, and the implied existence of a massivescalarboson was not seen as important; only Higgs directly addressed it.) In the paper by GHK the boson is massless and decoupled from the massive states.In reviews dated 2009 and 2011, Guralnik states that in the GHK model the boson is massless only in a lowest-order approximation, but it is not subject to any constraint and acquires mass at higher orders, and adds that the GHK paper was the only one to show that there are no masslessGoldstone bosonsin the model and to give a complete analysis of the general Higgs mechanism.All three reached similar conclusions, despite their very different approaches: Higgs's paper essentially used classical techniques, Englert and Brout's involved calculating vacuum polarisation in perturbation theory around an assumed symmetry-breaking vacuum state, and GHK used operator formalism and conservation laws to explore in depth the ways in which Goldstone's theorem was avoided.Some versions of the theory predicted more than one kind of Higgs fields and bosons, and alternative\"Higgsless\" modelswere considered until the discovery of the Higgs boson.\n\nToproduce Higgs bosons, two beams of particles are accelerated to very high energies and allowed to collide within aparticle detector. Occasionally, although rarely, a Higgs boson will be created fleetingly as part of the collision byproducts. Because the Higgs bosondecaysvery quickly, particle detectors cannot detect it directly. Instead the detectors register all the decay products (thedecay signature) and from the data the decay process is reconstructed. If the observed decay products match a possible decay process (known as adecay channel) of a Higgs boson, this indicates that a Higgs boson may have been created. In practice, many processes may produce similar decay signatures. Fortunately, the Standard Model precisely predicts the likelihood of each of these, and each known process, occurring. So, if the detector detects more decay signatures consistently matching a Higgs boson than would otherwise be expected if Higgs bosons did not exist, then this would be strong evidence that the Higgs boson exists.\n\nBecause Higgs boson production in a particle collision is likely to be very rare (1 in 10 billion at the LHC),and many other possible collision events can have similar decay signatures, the data of hundreds of trillions of collisions needs to be analysed and must \"show the same picture\" before a conclusion about the existence of the Higgs boson can be reached. To conclude that a new particle has been found,particle physicistsrequire that thestatistical analysisof two independent particle detectors each indicate that there is less than a one-in-a-million chance that the observed decay signatures are due to just background random Standard Model events – i.e., that the observed number of events is more than fivestandard deviations(sigma) different from that expected if there was no new particle. More collision data allows better confirmation of the physical properties of any new particle observed, and allows physicists to decide whether it is indeed a Higgs boson as described by the Standard Model or some other hypothetical new particle.\n\nTo find the Higgs boson, a powerfulparticle acceleratorwas needed, because Higgs bosons might not be seen in lower-energy experiments. The collider needed to have a highluminosityin order to ensure enough collisions were seen for conclusions to be drawn. Finally, advanced computing facilities were needed to process the vast amount of data (25petabytesper year as of 2012) produced by the collisions.For the announcement of 4 July 2012, a new collider known as theLarge Hadron Colliderwas constructed atCERNwith a planned eventual collision energy of 14TeV– over seven times any previous collider – and over 300 trillion (3×1014) LHC proton–proton collisions were analysed by theLHC Computing Grid, the world's largestcomputing grid(as of 2012), comprising over 170 computing facilities in aworldwide networkacross 36 countries.\n\nThe first extensive search for the Higgs boson was conducted at theLarge Electron–Positron Collider(LEP) at CERN in the 1990s. At the end of its service in 2000, LEP had found no conclusive evidence for the Higgs.This implied that if the Higgs boson were to exist it would have to be heavier than114.4 GeV/c2.\n\nThe search continued atFermilabin the United States, where theTevatron– the collider that discovered thetop quarkin 1995 – had been upgraded for this purpose. There was no guarantee that the Tevatron would be able to find the Higgs, but it was the only supercollider that was operational since theLarge Hadron Collider(LHC) was still under construction and the plannedSuperconducting Super Colliderhad been cancelled in 1993 and never completed. The Tevatron was only able to exclude further ranges for the Higgs mass, and was shut down on 30 September 2011 because it no longer could keep up with the LHC. The final analysis of the data excluded the possibility of a Higgs boson with a mass between147 GeV/c2and180 GeV/c2. In addition, there was a small (but not significant) excess of events possibly indicating a Higgs boson with a mass between115 GeV/c2and140 GeV/c2.\n\nTheLarge Hadron CollideratCERNin Switzerland, was designed specifically to be able to either confirm or exclude the existence of the Higgs boson. Built in a 27 km tunnel under the ground nearGenevaoriginally inhabited by LEP, it was designed to collide two beams of protons, initially at energies of3.5 TeVper beam (7 TeV total), or almost 3.6 times that of the Tevatron, and upgradeable to2 × 7 TeV(14 TeV total) in future. Theory suggested if the Higgs boson existed, collisions at these energy levels should be able to reveal it. As one of themost complicated scientific instrumentsever built, its operational readiness was delayed for 14 months by amagnet quench eventnine days after its inaugural tests, caused by a faulty electrical connection that damaged over 50 superconducting magnets and contaminated the vacuum system.\n\nData collection at the LHC finally commenced in March 2010.By December 2011 the two main particle detectors at the LHC,ATLASandCMS, had narrowed down the mass range where the Higgs could exist to around116–130 GeV/c2(ATLAS) and115–127 GeV/c2(CMS).There had also already been a number of promising event excesses that had \"evaporated\" and proven to be nothing but random fluctuations. However, from around May 2011,both experiments had seen among their results, the slow emergence of a small yet consistent excess of gamma and 4-lepton decay signatures and several other particle decays, all hinting at a new particle at a mass around125 GeV/c2.By around November 2011, the anomalous data at125 GeV/c2was becoming \"too large to ignore\" (although still far from conclusive), and the team leaders at both ATLAS and CMS each privately suspected they might have found the Higgs.On 28 November 2011, at an internal meeting of the two team leaders and the director general of CERN, the latest analyses were discussed outside their teams for the first time, suggesting both ATLAS and CMS might be converging on a possible shared result at125 GeV/c2, and initial preparations commenced in case of a successful finding.While this information was not known publicly at the time, the narrowing of the possible Higgs range to around115–130 GeV/2and the repeated observation of small but consistent event excesses across multiple channels at both ATLAS and CMS in the124–126 GeV/c2region (described as \"tantalising hints\" of around 2–3 sigma) were public knowledge with \"a lot of interest\".It was therefore widely anticipated around the end of 2011, that the LHC would provide sufficient data to either exclude or confirm the finding of a Higgs boson by the end of 2012, when their 2012 collision data (with slightly higher 8 TeV collision energy) had been examined.\n\nOn 22 June 2012CERNannounced an upcoming seminar covering tentative findings for 2012,and shortly afterwards (from around 1 July 2012 according to an analysis of the spreading rumour in social media) rumours began to spread in the media that this would include a major announcement, but it was unclear whether this would be a stronger signal or a formal discovery.Speculation escalated to a \"fevered\" pitch when reports emerged thatPeter Higgs, who proposed the particle, was to be attending the seminar,and that \"five leading physicists\" had been invited – generally believed to signify the five living 1964 authors – with Higgs, Englert, Guralnik, Hagen attending and Kibble confirming his invitation (Brout having died in 2011).\n\nOn 4 July 2012 both of the CERN experiments announced they had independently made the same discovery:CMS of a previously unknown boson with mass125.3±0.6 GeV/c2and ATLAS of a boson with mass126.0±0.6 GeV/c2.Using the combined analysis of two interaction types (known as 'channels'), both experiments independently reached a local significance of 5 sigma – implying that the probability of getting at least as strong a result by chance alone is less than one in three million. When additional channels were taken into account, the CMS significance was reduced to 4.9 sigma.\n\nThe two teams had been working 'blinded' from each other from around late 2011 or early 2012,meaning they did not discuss their results with each other, providing additional certainty that any common finding was genuine validation of a particle.This level of evidence, confirmed independently by two separate teams and experiments, meets the formal level of proof required to announce a confirmed discovery.\n\nOn 31 July 2012, the ATLAS collaboration presented additional data analysis on the \"observation of a new particle\", including data from a third channel, which improved the significance to 5.9 sigma (1 in 588 million chance of obtaining at least as strong evidence by random background effects alone) and mass126.0 ± 0.4 (stat) ± 0.4 (sys) GeV/c2,and CMS improved the significance to 5-sigma and mass125.3 ± 0.4 (stat) ± 0.5 (sys) GeV/c2.\n\nFollowing the 2012 discovery, it was still unconfirmed whether the125 GeV/c2particle was a Higgs boson. On one hand, observations remained consistent with the observed particle being the Standard Model Higgs boson, and the particle decayed into at least some of the predicted channels. Moreover, the production rates and branching ratios for the observed channels broadly matched the predictions by the Standard Model within the experimental uncertainties. However, the experimental uncertainties currently still left room for alternative explanations, meaning an announcement of the discovery of a Higgs boson would have been premature.To allow more opportunity for data collection, the LHC's proposed 2012 shutdown and 2013–14 upgrade were postponed by seven weeks into 2013.\n\nIn November 2012, in a conference in Kyoto researchers said evidence gathered since July was falling into line with the basic Standard Model more than its alternatives, with a range of results for several interactions matching that theory's predictions.PhysicistMatt Strasslerhighlighted \"considerable\" evidence that the new particle is not apseudoscalarnegativeparityparticle (consistent with this required finding for a Higgs boson), \"evaporation\" or lack of increased significance for previous hints of non-Standard Model findings, expected Standard Model interactions withW and Z bosons, absence of \"significant new implications\" for or againstsupersymmetry, and in general no significant deviations to date from the results expected of a Standard Model Higgs boson.However some kinds of extensions to the Standard Model would also show very similar results;so commentators noted that based on other particles that are still being understood long after their discovery, it may take years to be sure, and decades to fully understand the particle that has been found.\n\nThese findings meant that as of January 2013, scientists were very sure they had found an unknown particle of mass ~125 GeV/c2, and had not been misled by experimental error or a chance result. They were also sure, from initial observations, that the new particle was some kind of boson. The behaviours and properties of the particle, so far as examined since July 2012, also seemed quite close to the behaviours expected of a Higgs boson. Even so, it could still have been a Higgs boson or some other unknown boson, since future tests could show behaviours that do not match a Higgs boson, so as of December 2012 CERN still only stated that the new particle was \"consistent with\" the Higgs boson,and scientists did not yet positively say it was the Higgs boson.Despite this, in late 2012, widespread media reports announced (incorrectly) that a Higgs boson had been confirmed during the year.\n\nIn January 2013, CERN director-generalRolf-Dieter Heuerstated that based on data analysis to date, an answer could be possible 'towards' mid-2013,and the deputy chair of physics atBrookhaven National Laboratorystated in February 2013 that a \"definitive\" answer might require \"another few years\" after thecollider's 2015 restart.In early March 2013, CERN Research Director Sergio Bertolucci stated that confirming spin-0 was the major remaining requirement to determine whether the particle is at least some kind of Higgs boson.\n\nOn 14 March 2013 CERN confirmed the following:\n\nCMS and ATLAS have compared a number of options for the spin-parity of this particle, and these all prefer no spin and even parity [two fundamental criteria of a Higgs boson consistent with the Standard Model]. This, coupled with the measured interactions of the new particle with other particles, strongly indicates that it is a Higgs boson.\n\nThis also makes the particle the first elementaryscalar particleto be discovered in nature.\n\nThe following are examples of tests used to confirm that the discovered particle is the Higgs boson:\n\nIn July 2017, CERN confirmed that all measurements still agree with the predictions of the Standard Model, and called the discovered particle simply \"the Higgs boson\".As of 2019, theLarge Hadron Colliderhas continued to produce findings that confirm the 2013 understanding of the Higgs field and particle.\n\nThe LHC's experimental work since restarting in 2015 has included probing the Higgs field and boson to a greater level of detail, and confirming whether less common predictions were correct. In particular, exploration since 2015 has provided strong evidence of the predicted direct decay intofermionssuch as pairs ofbottom quarks(3.6σ) – described as an \"important milestone\" in understanding its short lifetime and other rare decays – and also to confirm decay into pairs oftau leptons(5.9σ). This was described by CERN as being \"of paramount importance to establishing the coupling of the Higgs boson to leptons and represents an important step towards measuring its couplings to third generation fermions, the very heavy copies of the electrons and quarks, whose role in nature is a profound mystery\".Published results as of 19 March 2018 at 13 TeV for ATLAS and CMS had their measurements of the Higgs mass at124.98±0.28 GeV/c2and125.26±0.21 GeV/c2respectively.\n\nIn July 2018, the ATLAS and CMS experiments reported observing the Higgs boson decay into a pair of bottom quarks, which makes up approximately 60% of all of its decays.\n\nGauge invarianceis an important property of modern particle theories such as theStandard Model, partly due to its success in other areas of fundamental physics such aselectromagnetismand thestrong interaction(quantum chromodynamics). However, beforeSheldon Glashowextended theelectroweak unificationmodels in 1961, there were great difficulties in developing gauge theories for theweak nuclear forceor a possible unifiedelectroweak interaction.Fermionswith a mass term would violate gauge symmetry and therefore cannot be gauge invariant. (This can be seen by examining theDirac Lagrangianfor a fermion in terms of left and right handed components; we find none of the spin-half particles could ever fliphelicityas required for mass, so they must be massless.)W and Z bosonsare observed to have mass, but a boson mass term contains terms which clearly depend on the choice of gauge, and therefore these masses too cannot be gauge invariant. Therefore, it seems thatnoneof the standard model fermionsorbosons could \"begin\" with mass as an inbuilt property except by abandoning gauge invariance. If gauge invariance were to be retained, then these particles had to be acquiring their mass by some other mechanism or interaction.\n\nAdditionally, solutions based on spontaneous symmetry breaking appeared to fail, seemingly an inevitable result ofGoldstone's theorem. Because there is no potential energy cost to moving around the complex plane's \"circular valley\" responsible for spontaneous symmetry breaking, the resulting quantum excitation is pure kinetic energy, and therefore a massless boson (\"Goldstone boson\"), which in turn implies a new long range force. But no new long range forces or massless particles were detected either. So whatever was giving these particles their mass had to not \"break\" gauge invariance as the basis for other parts of the theories where it worked well,andhad to not require or predict unexpected massless particles or long-range forces which did not actually seem to exist in nature.\n\nA solution to all of these overlapping problems came from the discovery of a previously unnoticed borderline case hidden in the mathematics of Goldstone's theorem,that under certain conditions itmighttheoretically be possible for a symmetry to be brokenwithoutdisrupting gauge invariance andwithoutany new massless particles or forces, and having \"sensible\" (renormalisable) results mathematically. This became known as theHiggs mechanism.\n\nThe Standard Model hypothesises afieldwhich is responsible for this effect, called the Higgs field (symbol:ϕ{\\displaystyle \\phi }), which has the unusual property of a non-zero amplitude in itsground state; i.e., a non-zerovacuum expectation value. It can have this effect because of its unusual \"sombrero\" shaped potential whose lowest \"point\" is not at its \"centre\". In simple terms, unlike all other known fields, the Higgs field requireslessenergy to have a non-zero value than a zero value, so it ends up having a non-zero valueeverywhere. Below a certain extremely high energy level the existence of this non-zero vacuum expectationspontaneously breakselectroweakgauge symmetrywhich in turn gives rise to the Higgs mechanism and triggers the acquisition of mass by those particles interacting with the field. This effect occurs becausescalar fieldcomponents of the Higgs field are \"absorbed\" by the massive bosons asdegrees of freedom, and couple to the fermions viaYukawa coupling, thereby producing the expected mass terms. When symmetry breaks under these conditions, theGoldstone bosonsthat ariseinteractwith the Higgs field (and with other particles capable of interacting with the Higgs field) instead of becoming new massless particles. The intractable problems of both underlying theories \"neutralise\" each other, and the residual outcome is that elementary particles acquire a consistent mass based on how strongly they interact with the Higgs field. It is the simplest known process capable of giving mass to thegauge bosonswhile remaining compatible withgauge theories.Itsquantumwould be ascalar boson, known as the Higgs boson.\n\nThe proposed Higgs mechanism arose as a result of theories proposed to explain observations insuperconductivity. A superconductor does not allow penetration by external magnetic fields (theMeissner effect). This strange observation implies that somehow, the electromagnetic field becomes short ranged during this phenomenon. Successful theories arose to explain this during the 1950s, first for fermions (Ginzburg–Landau theory, 1950), and then for bosons (BCS theory, 1957).\n\nIn these theories, superconductivity is interpreted as arising from acharged condensatefield. Initially, the condensate value does not have any preferred direction, implying it is scalar, but itsphaseis capable of defining a gauge, in gauge based field theories. To do this, the field must be charged. A charged scalar field must also be complex (or described another way, it contains at least two components, and a symmetry capable of rotating each into the other(s)). In naïve gauge theory, a gauge transformation of a condensate usually rotates the phase. But in these circumstances, it instead fixes a preferred choice of phase. However, it turns out that fixing the choice of gauge so that the condensate has the same phase everywhere also causes the electromagnetic field to gain an extra term. This extra term causes the electromagnetic field to become short range.\n\nOnce attention was drawn to this theory within particle physics, the parallels were clear. A change of the usually long range electromagnetic field to become short ranged, within a gauge invariant theory, was exactly the needed effect sought for the weak force bosons (because a long range force has massless gauge bosons, and a short ranged force implies massive gauge bosons, suggesting that a result of this interaction is that the field's gauge bosons acquired mass, or a similar and equivalent effect). The features of a field required to do this were also quite well defined – it would have to be a charged scalar field, with at least two components, and complex in order to support a symmetry able to rotate these into each other.\n\nThe Minimal Standard Model as described above is the simplest known model for the Higgs mechanism with just one Higgs field. However, an extended Higgs sector with additional Higgs particle doublets or triplets is also possible, and many extensions of the Standard Model have this feature. The non-minimal Higgs sector favoured by theory are thetwo-Higgs-doublet models(2HDM), which predict the existence of aquintetof scalar particles: twoCP-evenneutral Higgs bosons h0and H0, a CP-odd neutral Higgs boson A0, and two charged Higgs particles H±.Supersymmetry(\"SUSY\") also predicts relations between the Higgs-boson masses and the masses of the gauge bosons, and could accommodate a125 GeV/c2neutral Higgs boson.\n\nThe key method to distinguish between these different models involves study of the particles' interactions (\"coupling\") and exact decay processes (\"branching ratios\"), which can be measured and tested experimentally in particle collisions. In the Type-I 2HDM model one Higgs doublet couples to up and down quarks, while the second doublet does not couple to quarks. This model has two interesting limits, in which the lightest Higgs couples to just fermions (\"gauge-phobic\") or just gauge bosons (\"fermiophobic\"), but not both. In the Type-II 2HDM model, one Higgs doublet only couples to up-type quarks, the other only couples to down-type quarks.The heavily researchedMinimal Supersymmetric Standard Model(MSSM) includes a Type-II 2HDM Higgs sector, so it could be disproven by evidence of a Type-I 2HDM Higgs.[citation needed]\n\nIn other models the Higgs scalar is a composite particle. For example, intechnicolourthe role of the Higgs field is played by strongly bound pairs of fermions calledtechniquarks. Other models feature pairs oftop quarks(seetop quark condensate). In yet other models, there isno Higgs field at alland the electroweak symmetry is broken using extra dimensions.\n\nThe Standard Model leaves the mass of the Higgs boson as aparameterto be measured, rather than a value to be calculated. This is seen as theoretically unsatisfactory, particularly as quantum corrections (related to interactions withvirtual particles) should apparently cause the Higgs particle to have a mass immensely higher than that observed, but at the same time the Standard Model requires a massof the order of100 to 1000 GeV/c2to ensureunitarity(in this case, to unitarise longitudinal vector boson scattering).Reconciling these points appears to require explaining why there is an almost-perfect cancellation resulting in the visible mass of ~125 GeV/c2, and it is not clear how to do this. Because the weak force is about 1032times stronger than gravity, and (linked to this) the Higgs boson's mass is so much less than thePlanck massor thegrand unification energy, it appears that either there is some underlying connection or reason for these observations which is unknown and not described by the Standard Model, or some unexplained and extremely precisefine-tuningof parameters – however at present neither of these explanations is proven. This is known as ahierarchy problem.More broadly, the hierarchy problem amounts to the worry thata future theory of fundamental particles and interactionsshould not have excessive fine-tunings or unduly delicate cancellations, and should allow masses of particles such as the Higgs boson to be calculable. The problem is in some ways unique to spin-0 particles (such as the Higgs boson), which can give rise to issues related to quantum corrections that do not affect particles with spin.Anumber of solutions have been proposed, includingsupersymmetry, conformal solutions and solutions via extra dimensions such asbraneworldmodels.\n\nThere are also issues ofquantum triviality, which suggests that it may not be possible to create a consistent quantum field theory involving elementary scalar particles. This can also lead to a predictable Higgs mass inasymptotic safetyscenarios.\n\nIn the Standard Model, the Higgs field is a scalartachyonicfield –scalarmeaning it does not transform underLorentz transformations, andtachyonicmeaning the field (butnotthe particle) hasimaginary mass, and in certain configurations must undergosymmetry breaking. It consists of four components: Two neutral ones and two charged componentfields. Both of the charged components and one of the neutral fields areGoldstone bosons, which act as the longitudinal third-polarisation components of the massiveW+, W−, and Z bosons. The quantum of the remaining neutral component corresponds to (and is theoretically realised as) the massive Higgs boson.This component can interact withfermionsviaYukawa couplingto give them mass as well.\n\nMathematically, the Higgs field has imaginary mass and is therefore atachyonicfield.Whiletachyons(particlesthat movefaster than light) are a purely hypothetical concept,fieldswith imaginary mass have come to play an important role in modern physics.Under no circumstances do any excitations ever propagate faster than light in such theories – the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation ofcausality).Instead of faster-than-light particles, the imaginary mass creates an instability: Any configuration in which one or more field excitations are tachyonic must spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known astachyon condensation, and is now believed to be the explanation for how the Higgs mechanism itself arises in nature, and therefore the reason behind electroweak symmetry breaking.\n\nAlthough the notion of imaginary mass might seem troubling, it is only the field, and not the mass itself, that is quantised. Therefore, thefield operatorsatspacelikeseparated points stillcommute (or anticommute), and information and particles still do not propagate faster than light.Tachyon condensation drives a physical system that has reached a local limit – and might naively be expected to produce physical tachyons – to an alternate stable state where no physical tachyons exist. Once a tachyonic field such as the Higgs field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles such as the Higgs boson.\n\nSince the Higgs field isscalar, the Higgs boson has nospin. The Higgs boson is also its ownantiparticle, isCP-even, and has zeroelectricandcolour charge.\n\nThe Standard Model does not predict the mass of the Higgs boson.If that mass is between115 and 180 GeV/c2(consistent with empirical observations of125 GeV/c2), then the Standard Model can be valid at energy scales all the way up to thePlanck scale(1019GeV/c2).It should be the only particle in the Standard Model that remains massive even at high energies. Many theorists expect newphysics beyond the Standard Modelto emerge at the TeV-scale, based on unsatisfactory properties of the Standard Model.The highest possible mass scale allowed for the Higgs boson (or some other electroweak symmetry breaking mechanism) is 1.4 TeV; beyond this point, the Standard Model becomes inconsistent without such a mechanism, becauseunitarityis violated in certain scattering processes.\n\nIt is also possible, although experimentally difficult, to estimate the mass of the Higgs boson indirectly: In the Standard Model, the Higgs boson has a number of indirect effects; most notably, Higgs loops result in tiny corrections to masses of the W and Z bosons. Precision measurements of electroweak parameters, such as theFermi constantand masses of the W and Z bosons, can be used to calculate constraints on the mass of the Higgs. As of July 2011, the precision electroweak measurements tell us that the mass of the Higgs boson is likely to be less than about161 GeV/c2at 95%confidence level.These indirect constraints rely on the assumption that the Standard Model is correct. It may still be possible to discover a Higgs boson above these masses, if it is accompanied by other particles beyond those accommodated by the Standard Model.\n\nThe LHC cannot directly measure the Higgs boson's lifetime, due to its extreme brevity. It is predicted as1.56×10−22sbased on the predicteddecay widthof4.07×10−3GeV.However it can be measured indirectly, based upon comparing masses measured from quantum phenomena occurring in theon shellproduction pathways and in the, much rarer,off shellproduction pathways, derived from Dalitz decay via a virtual photon(H → γ*γ → ℓℓγ). Using this technique, the lifetime of the Higgs boson was tentatively measured in 2021 as1.2 –4.6×10−22s, at sigma 3.2 (1 in 1000) significance.\n\nIf Higgs particle theories are valid, then a Higgs particle can be produced much like other particles that are studied, in aparticle collider. This involves accelerating a large number of particles to extremely high energies and extremely close to thespeed of light, then allowing them to smash together.Protonsand leadions(the barenucleiof leadatoms) are used at the LHC. In the extreme energies of these collisions, the desired esoteric particles will occasionally be produced and this can be detected and studied; any absence or difference from theoretical expectations can also be used to improve the theory. The relevant particle theory (in this case the Standard Model) will determine the necessary kinds of collisions and detectors. The Standard Model predicts that Higgs bosons could be formed in a number of ways,although the probability of producing a Higgs boson in any collision is always expected to be very small – for example, only one Higgs boson per 10 billion collisions in the Large Hadron Collider.The most common expected processes for Higgs boson production are:\n\nQuantum mechanics predicts that if it is possible for a particle to decay into a set of lighter particles, then it will eventually do so.This is also true for the Higgs boson. The likelihood with which this happens depends on a variety of factors including: the difference in mass, the strength of the interactions, etc. Most of these factors are fixed by the Standard Model, except for the mass of the Higgs boson itself. For a Higgs boson with a mass of125 GeV/c2the SM predicts a mean life time of about1.6×10−22s.\n\nSince it interacts with all the massive elementary particles of the SM, the Higgs boson has many different processes through which it can decay. Each of these possible processes has its own probability, expressed as thebranching ratio; the fraction of the total number decays that follows that process. The SM predicts these branching ratios as a function of the Higgs mass (see plot).\n\nOne way that the Higgs can decay is by splitting into a fermion–antifermion pair. As general rule, the Higgs is more likely to decay into heavy fermions than light fermions, because the mass of a fermion is proportional to the strength of its interaction with the Higgs.By this logic the most common decay should be into atop–antitop quark pair. However, such a decay would only be possible if the Higgs were heavier than ~346 GeV/c2, twice the mass of the top quark. For a Higgs mass of125 GeV/c2the SM predicts that the most common decay is into abottom–antibottom quark pair, which happens 57.7% of the time.The second most common fermion decay at that mass is atau–antitau pair, which happens only about 6.3% of the time.\n\nAnother possibility is for the Higgs to split into a pair of massive gauge bosons. The most likely possibility is for the Higgs to decay into a pair of W bosons (the light blue line in the plot), which happens about 21.5% of the time for a Higgs boson with a mass of125 GeV/c2.The W bosons can subsequently decay either into a quark and an antiquark or into a charged lepton and a neutrino. The decays of W bosons into quarks are difficult to distinguish from the background, and the decays into leptons cannot be fully reconstructed (because neutrinos are impossible to detect in particle collision experiments). A cleaner signal is given by decay into a pair of Z-bosons (which happens about 2.6% of the time for a Higgs with a mass of125 GeV/c2),if each of the bosons subsequently decays into a pair of easy-to-detect charged leptons (electronsormuons).\n\nDecay into massless gauge bosons (i.e.,gluonsorphotons) is also possible, but requires intermediate loop of virtual heavy quarks (top or bottom) or massive gauge bosons.The most common such process is the decay into a pair of gluons through a loop of virtual heavy quarks. This process, which is the reverse of the gluon fusion process mentioned above, happens approximately 8.6% of the time for a Higgs boson with a mass of125 GeV/c2.Much rarer is the decay into a pair of photons mediated by a loop of W bosons or heavy quarks, which happens only twice for every thousand decays.However, this process is very relevant for experimental searches for the Higgs boson, because the energy and momentum of the photons can be measured very precisely, giving an accurate reconstruction of the mass of the decaying particle.\n\nIn 2021 the extremely rare Dalitz decay was tentatively observed,[citation needed]into twoleptons(electrons or muons) and a photon (ℓℓγ), viavirtual photondecay. This can happen in three ways; Higgs to virtual photon to ℓℓγ in which the virtual photon (γ*) has very small but nonzero mass, Higgs to Z boson to ℓℓγ, or Higgs to two leptons, one of which emits a final-state photon leading to ℓℓγ. ATLAS searched for evidence of the first of these(H → γ*γ → ℓℓγ)at low di-lepton mass(≤30 GeV/c2), where this process should dominate. The observation is at sigma 3.2 (1 in 1000) significance.This decay path is important because it facilitates measuring theon- and off-shellmass of the Higgs boson (allowing indirect measurement of decay time), and the decay into two charged particles allows exploration ofcharge conjugationandcharge parity (CP) violation.\n\nAt the Large Hadron Collider (LHC),Higgs boson pair productionoccurs through several distinct mechanisms, similarly as single Higgs production:\n\nEach of these production mechanisms offers different levels of sensitivity to the Higgs self-coupling λ, making them essential components in a comprehensive search for deviations from the Standard Model prediction.\n\nHiggs boson pairs can decay through various channels. The most studied final states include:\n\nThe choice of decay mode affects the sensitivity of LHC experiments to the HH signal.\n\nThe name most strongly associated with the particle and field is the Higgs bosonand Higgs field. For some time the particle was known by a combination of its PRL author names (including at times Anderson), for example the Brout–Englert–Higgs particle, the Anderson–Higgs particle, or the Englert–Brout–Higgs–Guralnik–Hagen–Kibble mechanism,and these are still used at times.Fuelled in part by the issue of recognition and a potential shared Nobel Prize,the most appropriate name was still occasionally a topic of debate until 2013.Higgs himself preferred to call the particle either by an acronym of all those involved, or \"the scalar boson\", or \"the so-called Higgs particle\".\n\nA considerable amount has been written on how Higgs's name came to be exclusively used. Two main explanations are offered. The first is that Higgs undertook a step which was either unique, clearer or more explicit in his paper in formally predicting and examining the particle. Of the PRL papers' authors, only the paper by Higgsexplicitlyoffered as a prediction that a massive particle would exist and calculated some of its properties;he was therefore \"the first to postulate the existence of a massive particle\" according toNature.Physicist and authorFrank Closeand physicist-bloggerPeter Woitboth comment that the paper by GHK was also completed after Higgs and Brout–Englert were submitted toPhysical Review Letters,and that Higgs alone had drawn attention to a predicted massivescalarboson, while all others had focused on the massivevectorbosons.In this way, Higgs's contribution also provided experimentalists with a crucial \"concrete target\" needed to test the theory.\n\nHowever, in Higgs's view, Brout and Englert did not explicitly mention the boson since its existence is plainly obvious in their work,while according to Guralnik the GHK paper was a complete analysis of the entire symmetry breaking mechanism whosemathematical rigouris absent from the other two papers, and a massive particle may exist in some solutions.Higgs's paper also provided an \"especially sharp\" statement of the challenge and its solution according toscience historianDavid Kaiser.\n\nThe alternative explanation is that the name was popularised in the 1970s due to its use as a convenient shorthand or because of a mistake in citing. Many accounts(including Higgs's own)credit the \"Higgs\" name to physicistBenjamin Lee.Lee was a significant populariser of the theory in its early days, and habitually attached the name \"Higgs\" as a \"convenient shorthand\" for its components from 1972,and in at least one instance from as early as 1966.Although Lee clarified in his footnotes that \"'Higgs' is an abbreviation for Higgs, Kibble, Guralnik, Hagen, Brout, Englert\",his use of the term (and perhaps also Steven Weinberg's mistaken cite of Higgs's paper as the first in his seminal 1967 paper) meant that by around 1975–1976 others had also begun to use the name \"Higgs\" exclusively as a shorthand.In 2012, physicistFrank Wilczek, who was credited for naming the elementary particle, theaxion(over an alternative proposal \"Higglet\", by Weinberg), endorsed the \"Higgs boson\" name, stating \"History is complicated, and wherever you draw the line, there will be somebody just below it.\"\n\nThe Higgs boson is often referred to as the \"God particle\" in popular media outside the scientific community.The nickname comes from the title of the 1993 book on the Higgs boson and particle physics,The God Particle: If the Universe Is the Answer, What Is the Question?byPhysics Nobel Prize winnerandFermilabdirectorLeon M. Lederman.Lederman wrote it in the context of failing US government support for theSuperconducting Super Collider,a partially constructed titaniccompetitor to theLarge Hadron Colliderwith planned collision energies of2 × 20 TeVthat was championed by Lederman since its 1983 inceptionand shut down in 1993. The book sought in part to promote awareness of the significance and need for such a project in the face of its possible loss of funding.Lederman, a leading researcher in the field, writes that he wanted to title his bookThe Goddamn Particle: If the Universe is the Answer, What is the Question?Lederman's editor decided that the title was too controversial and convinced him to change the title toThe God Particle: If the Universe is the Answer, What is the Question?\n\nWhile media use of this term may have contributed to wider awareness and interest,many scientists feel the name is inappropriatesince it is sensationalhyperboleand misleads readers;the particle also has nothing to do with any God, leaves open numerousquestions in fundamental physics, and does not explain the ultimateorigin of the universe.Higgs, anatheist, was reported to be displeased and stated in a 2008 interview that he found it \"embarrassing\" because it was \"the kind of misuse[...] which I think might offend some people\".The nickname has been satirised in mainstream media as well.Science writer Ian Sample stated in his 2010 book on the search that the nickname is \"universally hate[d]\" by physicists and perhaps the \"worst derided\" in thehistory of physics, but that (according to Lederman) the publisher rejected all titles mentioning \"Higgs\" as unimaginative and too unknown.\n\nLederman begins with a review of the long human search for knowledge, and explains that his tongue-in-cheek title draws an analogy between the impact of the Higgs field on the fundamental symmetries at theBig Bang, and the apparent chaos of structures, particles, forces and interactions that resulted and shaped our present universe, with the biblical story ofBabelin which the primordial single language of earlyGenesiswasfragmented into many disparate languagesand cultures.\n\nToday[...] we have the standard model, which reduces all of reality to a dozen or so particles and four forces[...] It's a hard-won simplicity [and] remarkably accurate. But it is also incomplete and, in fact, internally inconsistent[...] This boson is so central to the state of physics today, so crucial to our final understanding of the structure of matter, yet so elusive, that I have given it a nickname: the God Particle. Why God Particle? Two reasons. One, the publisher wouldn't let us call it the Goddamn Particle, though that might be a more appropriate title, given its villainous nature and the expense it is causing. And two, there is a connection, of sorts, toanother book, amucholder one ...\n\n— Lederman & Teresi\n\nLederman asks whether the Higgs boson was added just to perplex and confound those seeking knowledge of the universe, and whether physicists will be confounded by it as recounted in that story, or ultimately surmount the challenge and understand \"how beautiful is the universe [God has] made\".\n\nA renaming competition by British newspaperThe Guardianin 2009 resulted in their science correspondent choosing the name \"thechampagne bottleboson\" as the best submission: \"The bottom of a champagne bottle is in the shape of theHiggs potentialand is often used as an illustration in physics lectures. So it's not an embarrassingly grandiose name, it is memorable, and [it] has some physics connection too.\"The nameHiggsonwas suggested as well, in an opinion piece in theInstitute of Physics' online publicationphysicsworld.com.\n\nThere has been considerable public discussion of analogies and explanations for the Higgs particle and how the field creates mass,including coverage of explanatory attempts in their own right and a competition in 1993 for the best popular explanation by then-UK Minister for ScienceSir William Waldegraveand articles in newspapers worldwide.\n\nAn educational collaboration involving an LHC physicist and aHigh School Teachers at CERNeducator suggests thatdispersion of light– responsible for therainbowanddispersive prism– is a useful analogy for the Higgs field's symmetry breaking and mass-causing effect.\n\nMatt Strassler uses electric fields as an analogy:\n\nSome particles interact with the Higgs field while others don't. Those particles that feel the Higgs field act as if they have mass. Something similar happens in anelectric field– charged objects are pulled around and neutral objects can sail through unaffected. So you can think of the Higgs search as an attempt to make waves in the Higgs field [create Higgs bosons] to prove it's really there.\n\nA similar explanation was offered byThe Guardian:\n\nThe Higgs boson is essentially a ripple in a field said to have emerged at the birth of the universe and to span the cosmos to this day ... The particle is crucial however: It is thesmoking gun, the evidence required to show the theory is right.\n\nThe Higgs field's effect on particles was famously described by physicist David Miller as akin to a room full of political party workers spread evenly throughout a room: The crowd gravitates to and slows down famous people but does not slow down others.He also drew attention to well-known effects insolid state physicswhere an electron's effective mass can be much greater than usual in the presence of a crystal lattice.\n\nAnalogies based ondrageffects, including analogies of \"syrup\" or \"molasses\" are also well known, but can be somewhat misleading since they may be understood (incorrectly) as saying that the Higgs field simply resists some particles' motion but not others' – a simple resistive effect could also conflict withNewton's third law.\n\nThe Higgs boson is commonly misunderstood as responsible for mass, rather than the Higgs field, and as relating to most mass in the universe.About 91% of the proton mass is due to the quark and gluon fields and the QCDconformal anomalyrather than the Higgs interaction.\n\nThere was considerable discussion prior to late 2013 of how to allocate the credit if the Higgs boson is proven, made more pointed as aNobel Prizehad been expected, and the very wide basis of people entitled to consideration. These include a range of theoreticians who made the Higgs mechanism theory possible, the theoreticians of the 1964 PRL papers (including Higgs himself), the theoreticians who derived from these a working electroweak theory and the Standard Model itself, and also the experimentalists at CERN and other institutions who made possible the proof of the Higgs field and boson in reality. The Nobel Prize has a limit of three persons to share an award, and some possible winners are already prize holders for other work, or are deceased (the prize is only awarded to persons in their lifetime). Existing prizes for works relating to the Higgs field, boson, or mechanism include:\n\nEnglert's co-researcherRobert Brouthad died in 2011 and the Nobel Prize isnot ordinarily given posthumously.\n\nAdditionallyPhysical Review Letters50-year review (2008) recognised the1964 PRL symmetry breaking papersand Weinberg's 1967 paperA model of Leptons(the most cited paper in particle physics, as of 2012) \"milestone Letters\".\n\nFollowing reported observation of the Higgs-like particle in July 2012, severalIndian mediaoutlets reported on the supposed neglect of credit to Indian physicistSatyendra Nath Boseafter whose work in the 1920s the class of particles \"bosons\" is named(although physicists have described Bose's connection to the discovery as tenuous).\n\nIn the Standard Model, the Higgs field is a four-component scalar field that forms a complexdoubletof theweak isospinSU(2) symmetry:\n\nwhile the field has charge +⁠1/2⁠under theweak hyperchargeU(1) symmetry.\n\nNote: This article uses the scaling convention where the electric charge,Q, theweak isospin,T3, and the weak hypercharge,YW, are related byQ=T3+YW.Adifferent conventionused in mostother Wikipedia articlesisQ=T3+⁠1/2⁠YW.\n\nThe Higgs part of the Lagrangian is\n\nwhereWμa{\\displaystyle W_{\\mu \\,a}}andBμ{\\displaystyle B_{\\mu }}are thegauge bosonsof the SU(2) and U(1) symmetries,g{\\displaystyle g}andg′{\\displaystyle g'}their respectivecoupling constants,σa{\\displaystyle \\sigma ^{a}}are thePauli matrices(a complete set of generators of the SU(2) symmetry), andλ>0{\\displaystyle \\lambda >0}andμH2>0{\\displaystyle \\mu _{\\text{H}}^{2}>0}, so that theground statebreaks the SU(2) symmetry (see figure).\n\nThe ground state of the Higgs field (the bottom of the potential) is degenerate with different ground states related to each other by a SU(2) gauge transformation. It is always possible topick a gaugesuch that in the ground stateϕ1=ϕ2=ϕ3=0{\\displaystyle \\phi ^{1}=\\phi ^{2}=\\phi ^{3}=0}. The expectation value ofϕ0{\\displaystyle \\phi ^{0}}in the ground state (thevacuum expectation valueor VEV) is then⟨ϕ0⟩=12v{\\displaystyle \\left\\langle \\phi ^{0}\\right\\rangle ={\\tfrac {1}{\\sqrt {2\\,}}}v}, wherev=1λ|μH|{\\displaystyle v={\\tfrac {1}{\\sqrt {\\lambda \\,}}}\\left|\\mu _{\\text{H}}\\right|}. The measured value of this parameter is ~246 GeV/c2.It has units of mass, and is the only free parameter of the Standard Model that is not a dimensionless number. Quadratic terms inWμ{\\displaystyle W_{\\mu }}andBμ{\\displaystyle B_{\\mu }}arise, which give masses to the W and Z bosons:\n\nwith their ratio determining theWeinberg angle,cos⁡θW=mWmZ=|g|g2+g′2{\\textstyle \\cos \\theta _{\\text{W}}={\\frac {m_{\\text{W}}}{\\ m_{\\text{Z}}\\ }}={\\frac {\\left|\\,g\\,\\right|}{\\ {\\sqrt {g^{2}+{g'}^{2}\\ }}\\ }}}, and leave a massless U(1)photon,γ{\\displaystyle \\gamma }. The mass of the Higgs boson itself is given by\n\nThe quarks and the leptons interact with the Higgs field throughYukawa interactionterms:\n\nwhere(d,u,e,ν)L,Ri{\\displaystyle (d,u,e,\\nu )_{\\text{L,R}}^{i}}are left-handed and right-handed quarks and leptons of theithgeneration,λu,d,eij{\\displaystyle \\lambda _{\\text{u,d,e}}^{i\\,j}}are matrices of Yukawa couplings whereh.c.denotes the hermitian conjugate of all the preceding terms. In the symmetry breaking ground state, only the terms containingϕ0{\\displaystyle \\phi ^{0}}remain, giving rise to mass terms for the fermions. Rotating the quark and lepton fields to the basis where the matrices of Yukawa couplings are diagonal, one gets\n\nwhere the masses of the fermions aremu,d,ei=12λu,d,eiv{\\displaystyle m_{\\text{u,d,e}}^{i}={\\tfrac {1}{\\sqrt {2\\ }}}\\lambda _{\\text{u,d,e}}^{i}v}, andλu,d,ei{\\displaystyle \\lambda _{\\text{u,d,e}}^{i}}denote the eigenvalues of the Yukawa matrices.",
      "sections": [
        {
          "level": 2,
          "heading": "Introduction"
        },
        {
          "level": 3,
          "heading": "Standard Model"
        },
        {
          "level": 3,
          "heading": "Gauge-invariant theories and symmetries"
        },
        {
          "level": 3,
          "heading": "Gauge boson rest mass problem"
        },
        {
          "level": 3,
          "heading": "Symmetry breaking"
        },
        {
          "level": 3,
          "heading": "Higgs mechanism"
        },
        {
          "level": 3,
          "heading": "Higgs field"
        },
        {
          "level": 3,
          "heading": "The \"central problem\""
        },
        {
          "level": 3,
          "heading": "Search and discovery"
        },
        {
          "level": 3,
          "heading": "Interpretation"
        },
        {
          "level": 3,
          "heading": "Overview of Higgs boson and field properties"
        },
        {
          "level": 2,
          "heading": "Significance"
        },
        {
          "level": 3,
          "heading": "Particle physics"
        },
        {
          "level": 3,
          "heading": "Cosmology"
        },
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 3,
          "heading": "Theorisation"
        },
        {
          "level": 3,
          "heading": "Experimental search"
        },
        {
          "level": 2,
          "heading": "Theoretical issues"
        },
        {
          "level": 3,
          "heading": "Theoretical need for the Higgs"
        },
        {
          "level": 3,
          "heading": "Simple explanation of the theory, from its origins in superconductivity"
        },
        {
          "level": 3,
          "heading": "Alternative models"
        },
        {
          "level": 3,
          "heading": "Further theoretical issues and hierarchy problem"
        },
        {
          "level": 2,
          "heading": "Properties"
        },
        {
          "level": 3,
          "heading": "Properties of the Higgs field"
        },
        {
          "level": 3,
          "heading": "Properties of the Higgs boson"
        },
        {
          "level": 3,
          "heading": "Production"
        },
        {
          "level": 3,
          "heading": "Decay"
        },
        {
          "level": 3,
          "heading": "Pair Production"
        },
        {
          "level": 2,
          "heading": "Public discussion"
        },
        {
          "level": 3,
          "heading": "Naming"
        },
        {
          "level": 3,
          "heading": "Educational explanations and analogies"
        },
        {
          "level": 3,
          "heading": "Recognition and awards"
        },
        {
          "level": 2,
          "heading": "Technical aspects and mathematical formulation"
        },
        {
          "level": 3,
          "heading": "Standard Model"
        },
        {
          "level": 3,
          "heading": "Other"
        },
        {
          "level": 3,
          "heading": "Popular science, mass media, and general coverage"
        },
        {
          "level": 3,
          "heading": "Significant papers and other"
        },
        {
          "level": 3,
          "heading": "Introductions to the field"
        }
      ],
      "raw_content_length": 1398308,
      "cleaned_content_length": 81400,
      "scraped_at": "2025-09-02 15:30:24",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Higgs boson",
      "discoverers": [
        "Peter Higgs",
        "François Englert"
      ],
      "discovery_years": [
        "2012"
      ],
      "discovery_timeline": [
        "1964: Proposal of the Higgs mechanism by Peter Higgs and others",
        "2012: Discovery of the Higgs boson at the Large Hadron Collider"
      ],
      "mechanism": "The Higgs boson is produced by the quantum excitation of the Higgs field, which gives mass to elementary particles through the Higgs mechanism.",
      "key_features": [
        "Massive scalar boson",
        "Couples to particles to give them mass",
        "Has zero spin and no electric or color charge",
        "Very unstable, decaying into other particles almost immediately"
      ],
      "applications": [
        "Particle physics",
        "Understanding fundamental forces and particles",
        "Testing the Standard Model"
      ],
      "significance": "The discovery of the Higgs boson confirmed the last unverified part of the Standard Model of particle physics, solving a central problem in the field.",
      "institutions": [
        "CERN"
      ],
      "awards": [
        "Nobel Prize in Physics (2013)"
      ],
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Quantum_computing",
      "title": "Quantum computing",
      "main_content": "Aquantum computeris a (real or theoretical)computerthat usesquantum mechanicalphenomena in an essential way: it exploitssuperposedandentangledstates, and the intrinsicallynon-deterministicoutcomes ofquantum measurements, as features of its computation. Quantum computers can be viewed as sampling from quantum systems that evolve in ways classically described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (\"classical\") computers operate according to deterministic rules. Any classical computer can, in principle, be replicated by a (classical) mechanical device such as aTuring machine, with only polynomial overhead in time. Quantum computers, on the other hand are believed to requireexponentiallymore resources to simulate classically. It is widely believed that a scalable quantum computer could performsomecalculations exponentially faster than any classical computer. Theoretically, a large-scale quantum computer couldbreak some widely used public-key cryptographic schemesand aid physicists in performingphysical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks.\n\nThe basicunit of informationin quantum computing, thequbit(or \"quantum bit\"), serves the same function as thebitin ordinary or \"classical\" computing.However, unlike a classical bit, which can be in one of two states (abinary), a qubit can exist in asuperpositionof its two \"basis\" states, a state that is in an abstract sense \"between\" the two basis states. Whenmeasuringa qubit, the result is aprobabilistic outputof a classical bit. If a quantum computer manipulates the qubit in a particular way,wave interferenceeffects can amplify the desired measurement results. The design ofquantum algorithmsinvolves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\n\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficientlyisolatedfrom its environment, it suffers fromquantum decoherence, introducingnoiseinto calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations includesuperconductors(which isolate anelectrical currentby eliminatingelectrical resistance) andion traps(which confine a singleatomic particleusingelectromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage orquantum supremacy. These tasks are not necessarily useful for real-world applications.\n\nFor many years, the fields ofquantum mechanicsandcomputer scienceformed distinct academic communities.Modern quantum theorydeveloped in the 1920s to explain perplexing physical phenomena observed at atomic scales,anddigital computersemerged in the following decades to replacehuman computersfor tedious calculations.Both disciplines had practical applications duringWorld War II; computers played a major role inwartime cryptography,and quantum physics was essential fornuclear physicsused in theManhattan Project.\n\nAsphysicistsapplied quantum mechanical models to computational problems and swapped digitalbitsforqubits, the fields of quantum mechanics and computer science began to converge. In 1980,Paul Benioffintroduced thequantum Turing machine, which uses quantum theory to describe a simplified computer.When digital computers became faster, physicists faced anexponentialincrease in overhead whensimulating quantum dynamics,promptingYuri ManinandRichard Feynmanto independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.In a 1984 paper,Charles BennettandGilles Brassardapplied quantum theory tocryptographyprotocols and demonstrated that quantum key distribution could enhanceinformation security.\n\nQuantum algorithmsthen emerged for solvingoracle problems, such asDeutsch's algorithmin 1985,theBernstein–Vazirani algorithmin 1993,andSimon's algorithmin 1994.These algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying ablack boxwith a quantum state insuperposition, sometimes referred to asquantum parallelism.\n\nPeter Shorbuilt on these results withhis 1994 algorithmfor breaking the widely usedRSAandDiffie–Hellmanencryption protocols,which drew significant attention to the field of quantum computing. In 1996,Grover's algorithmestablished a quantum speedup for the widely applicableunstructuredsearch problem.The same year,Seth Lloydproved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations,validating Feynman's 1982 conjecture.\n\nOver the years,experimentalistshave constructed small-scale quantum computers usingtrapped ionsand superconductors.In 1998, a two-qubit quantum computer demonstrated the feasibility of the technology,and subsequent experiments have increased the number of qubits and reduced error rates.\n\nIn 2019,Google AIandNASAannounced that they had achievedquantum supremacywith a 54-qubit machine, performing a computation that is impossible for any classical computer.\n\nThis announcement was met with a rebuttal from Google's direct competitor, IBM. IBM contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its own Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\".\n\nComputer engineerstypically describe amodern computer's operation in terms ofclassical electrodynamics.\nWithin these \"classical\" computers, some components (such assemiconductorsandrandom number generators) may rely on quantum behavior, but these components are notisolatedfrom their environment, so anyquantum informationquicklydecoheres.\nWhileprogrammersmay depend onprobability theorywhen designing arandomized algorithm, quantum mechanical notions like superposition andinterferenceare largely irrelevant forprogram analysis.\n\nQuantum programs, in contrast, rely on precise control ofcoherentquantum systems. Physicistsdescribe these systems mathematicallyusinglinear algebra.Complex numbersmodelprobability amplitudes,vectorsmodelquantum states, andmatricesmodel the operations that can be performed on these states. Programming a quantum computer is then a matter ofcomposingoperations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\n\nAs physicistCharlie Bennettdescribes the relationship between quantum and classical computers,\n\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\nJust as the bit is the basic concept of classical information theory, thequbitis the fundamental unit ofquantum information. The same termqubitis used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written|0⟩{\\displaystyle |0\\rangle }and|1⟩{\\displaystyle |1\\rangle }serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states|0⟩{\\displaystyle |0\\rangle }and|1⟩{\\displaystyle |1\\rangle }belong to avector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as asuperpositionof|0⟩{\\displaystyle |0\\rangle }and|1⟩{\\displaystyle |1\\rangle }.\n\nA two-dimensionalvectormathematically represents a qubit state. Physicists typically useDirac notationfor quantum mechanicallinear algebra, writing|ψ⟩{\\displaystyle |\\psi \\rangle }'ketpsi'for a vector labeledψ{\\displaystyle \\psi }. Because a qubit is a two-state system, any qubit state takes the formα|0⟩+β|1⟩{\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }, where|0⟩{\\displaystyle |0\\rangle }and|1⟩{\\displaystyle |1\\rangle }are the standardbasis states,andα{\\displaystyle \\alpha }andβ{\\displaystyle \\beta }are theprobability amplitudes,which are in generalcomplex numbers.If eitherα{\\displaystyle \\alpha }orβ{\\displaystyle \\beta }is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such aquantum state vectoracts similarly to a (classical)probability vector, with one key difference: unlike probabilities, probabilityamplitudesare not necessarily positive numbers.Negative amplitudes allow for destructive wave interference.\n\nWhen a qubit ismeasuredin thestandard basis, the result is a classical bit. TheBorn ruledescribes thenorm-squaredcorrespondence between amplitudes and probabilities—when measuring a qubitα|0⟩+β|1⟩{\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }, the statecollapsesto|0⟩{\\displaystyle |0\\rangle }with probability|α|2{\\displaystyle |\\alpha |^{2}}, or to|1⟩{\\displaystyle |1\\rangle }with probability|β|2{\\displaystyle |\\beta |^{2}}.\nAny valid qubit state has coefficientsα{\\displaystyle \\alpha }andβ{\\displaystyle \\beta }such that|α|2+|β|2=1{\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}.\nAs an example, measuring the qubit1/2|0⟩+1/2|1⟩{\\displaystyle 1/{\\sqrt {2}}|0\\rangle +1/{\\sqrt {2}}|1\\rangle }would produce either|0⟩{\\displaystyle |0\\rangle }or|1⟩{\\displaystyle |1\\rangle }with equal probability.\n\nEach additional qubit doubles thedimensionof thestate space.As an example, the vector⁠1/√2⁠|00⟩+⁠1/√2⁠|01⟩represents a two-qubit state, atensor productof the qubit|0⟩with the qubit⁠1/√2⁠|0⟩+⁠1/√2⁠|1⟩.\nThis vector inhabits a four-dimensionalvector spacespanned by the basis vectors|00⟩,|01⟩,|10⟩, and|11⟩.\nTheBell state⁠1/√2⁠|00⟩+⁠1/√2⁠|11⟩is impossible to decompose into the tensor product of two individual qubits—the two qubits areentangledbecause neither qubit has a state vector of its own.\nIn general, the vector space for ann-qubit system is 2n-dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2100classical values.\n\nThe state of this one-qubitquantum memorycan be manipulated by applyingquantum logic gates, analogous to how classical memory can be manipulated withclassical logic gates. One important gate for both classical and quantum computation is the NOT gate, which can be represented by amatrixX:=(0110).{\\displaystyle X:={\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix}}.}Mathematically, the application of such a logic gate to a quantum state vector is modelled withmatrix multiplication. Thus\n\nThe mathematics of single qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are|00⟩:=(1000);|01⟩:=(0100);|10⟩:=(0010);|11⟩:=(0001).{\\displaystyle |00\\rangle :={\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}};\\quad |01\\rangle :={\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}};\\quad |10\\rangle :={\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}};\\quad |11\\rangle :={\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}}.}Thecontrolled NOT (CNOT)gate can then be represented using the following matrix:CNOT:=(1000010000010010).{\\displaystyle \\operatorname {CNOT} :={\\begin{pmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&0&1\\\\0&0&1&0\\end{pmatrix}}.}As a mathematical consequence of this definition,CNOT⁡|00⟩=|00⟩{\\textstyle \\operatorname {CNOT} |00\\rangle =|00\\rangle },CNOT⁡|01⟩=|01⟩{\\textstyle \\operatorname {CNOT} |01\\rangle =|01\\rangle },CNOT⁡|10⟩=|11⟩{\\textstyle \\operatorname {CNOT} |10\\rangle =|11\\rangle }, andCNOT⁡|11⟩=|10⟩{\\textstyle \\operatorname {CNOT} |11\\rangle =|10\\rangle }. In other words, the CNOT applies a NOT gate (X{\\textstyle X}from before) to the second qubit if and only if the first qubit is in the state|1⟩{\\textstyle |1\\rangle }. If the first qubit is|0⟩{\\textstyle |0\\rangle }, nothing is done to either qubit.\n\nIn summary, quantum computation can be described as a network of quantum logic gates and measurements. However, anymeasurement can be deferredto the end of quantum computation, though this deferment may come at a computational cost, so mostquantum circuitsdepict a network consisting only of quantum logic gates and no measurements.\n\nQuantum parallelismis the heuristic that quantum computers can be thought of as evaluating a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a superposition of input states and applying a unitary transformation that encodes the function to be evaluated. The resulting state encodes the function's output values for all input values in the superposition, allowing for the computation of multiple outputs simultaneously. This property is key to the speedup of many quantum algorithms. However, \"parallelism\" in this sense is insufficient to speed up a computation, because the measurement at the end of the computation gives only one value. To be useful, a quantum algorithm must also incorporate some other conceptual ingredient.\n\nThere are a number ofmodels of computationfor quantum computing, distinguished by the basic elements in which the computation is decomposed.\n\nAquantum gate arraydecomposes computation into a sequence of few-qubitquantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\n\nAny quantum computation (which is, in the above formalism, anyunitary matrixof size2n×2n{\\displaystyle 2^{n}\\times 2^{n}}overn{\\displaystyle n}qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as auniversal gate set, since a computer that can run such circuits is auniversal quantum computer. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to theSolovay-Kitaev theorem. Implementation of Boolean functions using the few-qubit quantum gates is presented here.\n\nAmeasurement-based quantum computerdecomposes computation into a sequence ofBell state measurementsand single-qubitquantum gatesapplied to a highly entangled initial state (acluster state), using a technique calledquantum gate teleportation.\n\nAnadiabatic quantum computer, based onquantum annealing, decomposes computation into a slow continuous transformation of an initialHamiltonianinto a final Hamiltonian, whose ground states contain the solution.\n\nNeuromorphic quantum computing (abbreviated as 'n.quantum computing') is an unconventional type of computing that usesneuromorphic computingto perform quantum operations. It was suggested that quantum algorithms, which are algorithms that run on a realistic model of quantum computation, can be computed equally efficiently with neuromorphic quantum computing. Both traditional quantum computing and neuromorphic quantum computing are physics-based unconventional computing approaches to computations and do not follow thevon Neumann architecture. They both construct a system (a circuit) that represents the physical problem at hand and then leverage their respective physics properties of the system to seek the \"minimum\". Neuromorphic quantum computing and quantum computing share similar physical properties during computation.\n\nAtopological quantum computerdecomposes computation into the braiding ofanyonsin a 2D lattice.\n\nAquantum Turing machineis the quantum analog of aTuring machine.All of these models of computation—quantum circuits,one-way quantum computation,adiabatic quantum computation,and topological quantum computation—have been shown to be equivalent to the quantum Turing machine; given a perfect implementation of one such quantum computer, it can simulate all the others with no more than polynomial overhead. This equivalence need not hold for practical quantum computers, since the overhead of simulation may be too large to be practical.\n\nThethreshold theoremshows how increasing the number of qubits can mitigate errors,yet fully fault-tolerant quantum computing remains \"a rather distant dream\".According to some researchers,noisy intermediate-scale quantum(NISQ) machines may have specialized uses in the near future, butnoisein quantum gates limits their reliability.Scientists atHarvardUniversity successfully created \"quantum circuits\" that correct errors more efficiently than alternative methods, which may potentially remove a major obstacle to practical quantum computers.The Harvard research team was supported byMIT,QuEra Computing,Caltech, andPrincetonUniversity and funded byDARPA's Optimization with Noisy Intermediate-Scale Quantum devices (ONISQ) program.\n\nDigital cryptography allows communications without observation by unauthorized parties. Conventional encryption, the obscuring of a message with a key through an algorithm, relies on the algorithm being difficult to reverse. Encryption is also the basis for digital signatures and authentication mechanisms. Quantum computing may be sufficiently more powerful that difficult reversals are feasible, allowing messages relying on conventional encryption to be read.\n\nQuantum cryptography replaces conventional algorithms with computations based on quantum computing. In principle, quantum encryption would be impossible to decode even with a quantum computer. This advantage comes at a significant cost in terms of elaborate infrastructure as well as preventing legitimate decoding of messages by governmental security officials.\n\nOngoing research in quantum andpost-quantum cryptographyhas led to new algorithms forquantum key distribution, initial work on quantumrandom number generationand to some early technology demonstrations.\n\nQuantum cryptographyenables new ways to transmit data securely; for example,quantum key distributionuses entangled quantum states to establish securecryptographic keys.When a sender and receiver exchange quantum states, they can guarantee that anadversarydoes not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change.With appropriatecryptographic protocols, the sender and receiver can thus establish shared private information resistant to eavesdropping.\n\nModernfiber-optic cablescan transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distancequantum networkswith end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhancedquantum sensing.\n\nProgress in findingquantum algorithmstypically focuses on this quantum circuit model, though exceptions like thequantum adiabatic algorithmexist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms.\n\nQuantum algorithms that offer more than a polynomial speedup over the best-known classical algorithm include Shor's algorithm for factoring and the related quantum algorithms for computingdiscrete logarithms, solvingPell's equation, and more generally solving thehidden subgroup problemforabelianfinite groups.These algorithms depend on the primitive of thequantum Fourier transform. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, but evidence suggests that this is unlikely.Certain oracle problems likeSimon's problemand theBernstein–Vazirani problemdo give provable speedups, though this is in thequantum query model, which is a restricted model where lower bounds are much easier to prove and doesn't necessarily translate to speedups for practical problems.\n\nOther problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certainJones polynomials, and thequantum algorithm for linear systems of equations, have quantum algorithms appearing to give super-polynomial speedups and areBQP-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply thatno quantum algorithmgives a super-polynomial speedup, which is believed to be unlikely.\n\nSome quantum algorithms, likeGrover's algorithmandamplitude amplification, give polynomial speedups over corresponding classical algorithms.Though these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems.These speed-ups are, however, over the theoretical worst-case of classical algorithms, and concrete real-world speed-ups over algorithms used in practice have not been demonstrated.\n\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically,quantum simulationmay be an important application of quantum computing.Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside acollider.In June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer.\n\nAbout 2% of the annual global energy output is used fornitrogen fixationto produceammoniafor theHaber processin the agricultural fertilizer industry (even though naturally occurring organisms also produce ammonia). Quantum simulations might be used to understand this process and increase the energy efficiency of production.It is expected that an early use of quantum computing will be modeling that improves the efficiency of the Haber–Bosch processby the mid-2020salthough some have predicted it will take longer.\n\nA notable application of quantum computation is forattackson cryptographic systems that are currently in use.Integer factorization, which underpins the security ofpublic key cryptographicsystems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of fewprime numbers(e.g., products of two 300-digit primes).By comparison, a quantum computer could solve this problem exponentially faster using Shor's algorithm to find its factors.This ability would allow a quantum computer to break many of thecryptographicsystems in use today, in the sense that there would be apolynomial time(in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popularpublic key ciphersare based on the difficulty of factoring integers or thediscrete logarithmproblem, both of which can be solved by Shor's algorithm. In particular, theRSA,Diffie–Hellman, andelliptic curve Diffie–Hellmanalgorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\n\nIdentifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field ofpost-quantum cryptography.Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like theMcEliece cryptosystembased on a problem incoding theory.Lattice-based cryptosystemsare also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving thedihedralhidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem.It has been proven that applying Grover's algorithm to break asymmetric (secret key) algorithmby brute force requires time equal to roughly 2n/2invocations of the underlying cryptographic algorithm, compared with roughly 2nin the classical case,meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (seeKey size).\n\nThe most well-known example of a problem that allows for a polynomial quantum speedup isunstructured search, which involves finding a marked item out of a list ofn{\\displaystyle n}items in a database. This can be solved by Grover's algorithm usingO(n){\\displaystyle O({\\sqrt {n}})}queries to the database, quadratically fewer than theΩ(n){\\displaystyle \\Omega (n)}queries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Many examples of provable quantum speedups for query problems are based on Grover's algorithm, includingBrassard, Høyer, and Tapp's algorithmfor finding collisions in two-to-one functions,and Farhi, Goldstone, and Gutmann's algorithm for evaluating NAND trees.\n\nProblems that can be efficiently addressed with Grover's algorithm have the following properties:\n\nFor problems with all these properties, the running time of Grover's algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be appliedis aBoolean satisfiability problem, where thedatabasethrough which the algorithm iterates is that of all possible answers. An example and possible application of this is apassword crackerthat attempts to guess a password. Breakingsymmetric cipherswith this algorithm is of interest to government agencies.\n\nQuantum annealingrelies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which slowly evolves to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process. Quantum annealing can solveIsing modelsand the (computationally equivalent)QUBOproblem, which in turn can be used to encode a wide range ofcombinatorial optimizationproblems.Adiabatic optimization may be helpful for solvingcomputational biologyproblems.\n\nSince quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed upmachine learningtasks.\n\nFor example, theHHL Algorithm, named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts.Some research groups have recently explored the use of quantum annealing hardware for trainingBoltzmann machinesanddeep neural networks.\n\nDeep generative chemistry models emerge as powerful tools to expeditedrug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problemsand thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative modelsincluding quantum GANsmay eventually be developed into ultimate generative chemistry algorithms.\n\nAs of 2023,[update]classical computers outperform quantum computers for all real-world applications. While current quantum computers may speed up solutions to particular mathematical problems, they give no computational advantage for practical tasks. Scientists and engineers are exploring multiple technologies for quantum computing hardware and hope to develop scalable quantum architectures, but serious obstacles remain.\n\nThere are a number of technical challenges in building a large-scale quantum computer.PhysicistDavid DiVincenzohas listedthese requirementsfor a practical quantum computer:\n\nSourcing parts for quantum computers is also very difficult.Superconducting quantum computers, like those constructed byGoogleandIBM, needhelium-3, anuclearresearch byproduct, and specialsuperconductingcables made only by the Japanese company Coax Co.\n\nThe control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development ofquantum controllersthat enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge.\n\nOne of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation timeT2(forNMRandMRItechnology, also called thedephasing time), typically range between nanoseconds and seconds at low temperatures.Currently, some quantum computers require their qubits to be cooled to 20 millikelvin (usually using adilution refrigerator) in order to prevent significant decoherence.A 2020 study argues thationizing radiationsuch ascosmic rayscan nevertheless cause certain systems to decohere within milliseconds.\n\nAs a result, time-consuming tasks may render some quantum algorithms inoperable, as attempting to maintain the state of qubits for a long enough duration will eventually corrupt the superpositions.\n\nThese issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is opticalpulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time; hence any operation must be completed much more quickly than the decoherence time.\n\nAs described by thethreshold theorem, if the error rate is small enough, it is thought to be possible to usequantum error correctionto suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often-cited figure for the required error rate in each gate for fault-tolerant computation is 10−3, assuming the noise is depolarizing.\n\nMeeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be betweenLandL2, whereLis the number of binary digits in the number to be factored; error correction algorithms would inflate this figure by an additional factor ofL. For a 1000-bit number, this implies a need for about 104bits without error correction.With error correction, the figure would rise to about 107bits. Computation time is aboutL2or about 107steps and at 1MHz, about 10 seconds. However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude. Careful estimatesshow that at least 3million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer. In terms of the number of physical qubits, to date, this remains the lowest estimatefor practically useful integer factorization problem sizing 1,024-bit or larger.\n\nAnother approach to the stability-decoherence problem is to create atopological quantum computerwithanyons,quasi-particlesused as threads, and relying onbraid theoryto form stable logic gates.Non-Abelian anyons can, in effect, remember how they have been manipulated, making them potentially useful in quantum computing.As of 2025, Microsoft and other organizations are investing in quasi-particle research.\n\nPhysicistJohn Preskillcoined the termquantum supremacyto describe the engineering feat of demonstrating that a programmable quantum device can solve a problem beyond the capabilities of state-of-the-art classical computers.The problem need not be useful, so some view the quantum supremacy test only as a potential future benchmark.\n\nIn October 2019, Google AI Quantum, with the help of NASA, became the first to claim to have achieved quantum supremacy by performing calculations on theSycamore quantum computermore than 3,000,000 times faster than they could be done onSummit, generally considered the world's fastest computer.This claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed,and researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputersand even beating it.\n\nIn December 2020, a group atUSTCimplemented a type ofBoson samplingon 76 photons with aphotonic quantum computer,Jiuzhang, to demonstrate quantum supremacy.The authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds.\n\nClaims of quantum supremacy have generated hype around quantum computing,but they are based on contrived benchmark tasks that do not directly imply useful real-world applications.\n\nIn January 2024, a study published inPhysical Review Lettersprovided direct verification of quantum supremacy experiments by computing exact amplitudes for experimentally generated bitstrings using a new-generation Sunway supercomputer, demonstrating a significant leap in simulation capability built on a multiple-amplitude tensor network contraction algorithm. This development underscores the evolving landscape of quantum computing, highlighting both the progress and the complexities involved in validating quantum supremacy claims.\n\nDespite high hopes for quantum computing, significant progress in hardware, and optimism about future applications, a 2023Naturespotlight article summarized current quantum computers as being \"For now, [good for] absolutely nothing\".The article elaborated that quantum computers are yet to be more useful or efficient than conventional computers in any case, though it also argued that in the long term such computers are likely to be useful. A 2023Communications of the ACMarticlefound that current quantum computing algorithms are \"insufficient for practical quantum advantage without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will not achieve quantum advantage with current quantum algorithms in the foreseeable future\", and it identified I/O constraints that make speedup unlikely for \"big data problems, unstructured linear systems, and database search based on Grover's algorithm\".\n\nThis state of affairs can be traced to several current and long-term considerations.\n\nIn particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain sufficiently high degree of entanglement for a long time. When trying to outperform conventional computers, quantum computing researchers often look for new tasks that can be solved on quantum computers, but this leaves the possibility that efficient non-quantum techniques will be developed in response, as seen for Quantum supremacy demonstrations. Therefore, it is desirable to prove lower bounds on the complexity of best possible non-quantum algorithms (which may be unknown) and show that some quantum algorithms asymptomatically improve upon those bounds.\n\nBill Unruhdoubted the practicality of quantum computers in a paper published in 1994.Paul Daviesargued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by theholographic principle.Skeptics likeGil Kalaidoubt that quantum supremacy will ever be achieved.PhysicistMikhail Dyakonovhas expressed skepticism of quantum computing as follows:\n\nA practical quantum computer must use a physical system as a programmable quantum register.Researchers are exploring several technologies as candidates for reliable qubit implementations.Superconductorsandtrapped ionsare some of the most developed proposals, but experimentalists are considering other hardware possibilities as well.For example,topological quantum computerapproaches are being explored for more fault-tolerance computing systems.\n\nThe first quantum logic gates were implemented withtrapped ionsand prototype general purpose machines with up to 20 qubits have been realized. However, the technology behind these devices combines complex vacuum equipment, lasers, microwave and radio frequency equipment making full scale processors difficult to integrate with standard computing equipment. Moreover, the trapped ion system itself has engineering challenges to overcome.\n\nThe largest commercial systems are based onsuperconductordevices and have scaled to 2000 qubits. However, the error rates for larger machines have been on the order of 5%. Technologically these devices are all cryogenic and scaling to large numbers of qubits requires wafer-scale integration, a serious engineering challenge by itself.\n\nWith focus on business management's point of view, the potential applications of quantum computing into four major categories are cybersecurity, data analytics and artificial intelligence, optimization and simulation, and data management and searching.\n\nOther applications include healthcare (ie. drug discovery), financial modeling, and natural language processing.\n\nAnycomputational problemsolvable by a classical computer is also solvable by a quantum computer.Intuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described usingquantum mechanics, which underlies the operation of quantum computers.\n\nConversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer can be simulated by aTuring machine. In other words, quantum computers provide no additional power over classical computers in terms ofcomputability. This means that quantum computers cannot solveundecidable problemslike thehalting problem, and the existence of quantum computers does not disprove theChurch–Turing thesis.\n\nWhile quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficientlyfactor integers, while this is not believed to be the case for classical computers.\n\nThe class ofproblemsthat can be efficiently solved by a quantum computer with bounded error is calledBQP, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart toBPP(\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-timeprobabilistic Turing machineswith bounded error.It is known thatBPP⊆BQP{\\displaystyle {\\mathsf {BPP\\subseteq BQP}}}and is widely suspected thatBQP⊊BPP{\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}}, which intuitively would mean that quantum computers are more powerful than classical computers in terms oftime complexity.\n\nThe exact relationship of BQP toP,NP, andPSPACEis not known. However, it is known thatP⊆BQP⊆PSPACE{\\displaystyle {\\mathsf {P\\subseteq BQP\\subseteq PSPACE}}}; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning there are problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and thediscrete logarithm problemare known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected thatNP⊈BQP{\\displaystyle {\\mathsf {NP\\nsubseteq BQP}}}; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class ofNP-completeproblems (if an NP-complete problem were in BQP, then it would follow fromNP-hardnessthat all problems in NP are in BQP).",
      "sections": [
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 2,
          "heading": "Quantum information processing"
        },
        {
          "level": 3,
          "heading": "Quantum information"
        },
        {
          "level": 3,
          "heading": "Unitary operators"
        },
        {
          "level": 3,
          "heading": "Quantum parallelism"
        },
        {
          "level": 3,
          "heading": "Quantum programming"
        },
        {
          "level": 2,
          "heading": "Communication"
        },
        {
          "level": 2,
          "heading": "Algorithms"
        },
        {
          "level": 3,
          "heading": "Simulation of quantum systems"
        },
        {
          "level": 3,
          "heading": "Post-quantum cryptography"
        },
        {
          "level": 3,
          "heading": "Search problems"
        },
        {
          "level": 3,
          "heading": "Quantum annealing"
        },
        {
          "level": 3,
          "heading": "Machine learning"
        },
        {
          "level": 2,
          "heading": "Engineering"
        },
        {
          "level": 3,
          "heading": "Challenges"
        },
        {
          "level": 3,
          "heading": "Quantum supremacy"
        },
        {
          "level": 3,
          "heading": "Skepticism"
        },
        {
          "level": 3,
          "heading": "Physical realizations"
        },
        {
          "level": 2,
          "heading": "Potential applications"
        },
        {
          "level": 2,
          "heading": "Theory"
        },
        {
          "level": 3,
          "heading": "Computability"
        },
        {
          "level": 3,
          "heading": "Complexity"
        },
        {
          "level": 3,
          "heading": "Textbooks"
        },
        {
          "level": 3,
          "heading": "Academic papers"
        }
      ],
      "raw_content_length": 1025159,
      "cleaned_content_length": 42895,
      "scraped_at": "2025-09-02 15:30:32",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Quantum Computing",
      "discoverers": [
        "Paul Benioff",
        "Yuri Manin",
        "Richard Feynman",
        "Charles Bennett",
        "Gilles Brassard",
        "Peter Shor",
        "Lov Grover",
        "Seth Lloyd"
      ],
      "discovery_years": [
        "1980"
      ],
      "discovery_timeline": [
        "1920s: Development of modern quantum theory",
        "1980: Introduction of the quantum Turing machine by Paul Benioff",
        "1984: Application of quantum theory to cryptography by Bennett and Brassard",
        "1985: Deutsch's algorithm",
        "1993: Bernstein–Vazirani algorithm",
        "1994: Simon's algorithm and Shor's algorithm for breaking RSA",
        "1996: Grover's algorithm for unstructured search",
        "1998: Demonstration of a two-qubit quantum computer",
        "2019: Google AI and NASA announce quantum supremacy"
      ],
      "mechanism": "Quantum computers use qubits that can exist in superposition and entangled states, allowing them to perform calculations on multiple possibilities simultaneously, leveraging quantum phenomena for computation.",
      "key_features": [
        "Exponential speedup for certain calculations compared to classical computers",
        "Ability to break widely used cryptographic protocols",
        "Efficient simulation of quantum systems without exponential overhead"
      ],
      "applications": [
        "Cryptography",
        "Quantum simulations",
        "Complex problem solving",
        "Optimization problems"
      ],
      "significance": "Quantum computing represents a paradigm shift in computation, potentially solving problems that are intractable for classical computers and impacting fields such as cryptography and materials science.",
      "institutions": [
        "Google AI",
        "NASA",
        "IBM"
      ],
      "awards": null,
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Ancient_DNA",
      "title": "Ancient DNA",
      "main_content": "Ancient DNA(aDNA) isDNAisolated from ancient sources (typicallyspecimens, but alsoenvironmental DNA).Due to degradation processes (includingcross-linking,deaminationandfragmentation)ancient DNA is more degraded in comparison with present-day genetic material.Genetic material has been recovered from paleo/archaeological and historical skeletal material,mummifiedtissues, archival collections of non-frozen medical specimens, preserved plant remains, ice and frompermafrostcores, marine and lake sediments andexcavationdirt.\n\nEven under the best preservation conditions, there is an upper boundary of 0.4–1.5 million years for a sample to contain sufficient DNA for sequencing technologies.The oldest DNA sequenced from physical specimens are frommammothmolars in Siberia over 1 million years old.In 2022, two-million-year-old genetic material was recovered from sediments inGreenland, and is currently considered the oldest DNA discovered so far.\n\nThe first study of what would come to be called aDNA was conducted in 1984, when Russ Higuchi and colleagues at theUniversity of California, Berkeleyreported that traces of DNA from a museum specimen of theQuagganot only remained in the specimen over 150 years after the death of the individual, but could be extracted and sequenced.Over the next two years, through investigations into natural and artificially mummified specimens,Svante Pääboconfirmed that this phenomenon was not limited to relatively recent museum specimens but could apparently be replicated in a range ofmummifiedhuman samples that dated as far back as several thousand years.\n\nThe laborious processes that were required at that time to sequence such DNA (throughbacterial cloning) were an effective brake on the study of ancient DNA (aDNA) and the field ofmuseomics. However, with the development of thePolymerase Chain Reaction(PCR) in the late 1980s, the field began to progress rapidly.Double primer PCR amplification of aDNA (jumping-PCR) can produce highly skewed and non-authentic sequence artifacts. Multiple primer,nested PCRstrategy was used to overcome those shortcomings.\n\nThe post-PCR era heralded a wave of publications as numerous research groups claimed success in isolating aDNA. Soon a series of incredible findings had been published, claiming authentic DNA could be extracted from specimens that were millions of years old, into the realms of what Lindahl (1993b) has labelledAntediluvianDNA.The majority of such claims were based on the retrieval of DNA from organisms preserved inamber. Insects such as stingless bees,termites,and wood gnats,as well as plantand bacterialsequences were said to have been extracted fromDominicanamber dating to theOligoceneepoch. Still older sources of Lebanese amber-encasedweevils, dating to within theCretaceousepoch, reportedly also yielded authentic DNA.Claims of DNA retrieval were not limited to amber.\n\nReports of several sediment-preserved plant remains dating to theMiocenewere published.Then in 1994, Woodwardet al.reported what at the time was called the most exciting results to date— mitochondrial cytochrome b sequences that had apparently been extracted from dinosaur bones dating to more than 80 million years ago. When in 1995 two further studies reported dinosaur DNA sequences extracted from a Cretaceous egg,it seemed that the field would revolutionize knowledge of the Earth's evolutionary past. Even these extraordinary ages were topped by the claimed retrieval of 250-million-year-old halobacterial sequences fromhalite.\n\nThe development of a better understanding of the kinetics of DNA preservation, the risks of sample contamination and other complicating factors led the field to view these results more skeptically. Numerous careful attempts failed to replicate many of the findings, and all of the decade's claims of multi-million year old aDNA would come to be dismissed as inauthentic.\n\nSingle primer extension amplification was introduced in 2007 to address postmortem DNA modification damage.Since 2009 the field of aDNA studies has been revolutionized with the introduction of much cheaper research techniques,and since 2010 been able tosequence ancient human DNA, recovering completegenomes.The use of high-throughputNext Generation Sequencing(NGS) techniques in the field of ancient DNA research has been essential for reconstructing the genomes of ancient or extinct organisms. A single-stranded DNA (ssDNA) library preparation method has sparked great interest among ancient DNA (aDNA) researchers.\n\nIn addition to these technical innovations, the start of the decade saw the field begin to develop better standards and criteria for evaluating DNA results, as well as a better understanding of the potential pitfalls.\n\nThe 2022Nobel Prize in Physiology or Medicinewas awarded to Svante Pääbo \"for his discoveries concerning the genomes of extinct hominins and human evolution\".A few days later, on the 7th of December 2022, a study inNaturereported that two-million year old genetic material was found in Greenland, and is currently considered the oldest DNA discovered so far.\n\nDue to degradation processes (includingcross-linking,deaminationandfragmentation),ancient DNA is of lower quality than modern genetic material.The damage characteristics and ability of aDNA to survive through time restricts possible analyses and places an upper limit on the age of successful samples.There is a theoretical correlation between time and DNA degradation,although differences in environmental conditions complicate matters. Samples subjected to different conditions are unlikely to predictably align to a uniform age-degradation relationship.The environmental effects may even matter after excavation, as DNA decay-rates may increase,particularly under fluctuating storage conditions.Even under the best preservation conditions, there is an upper boundary of 0.4 to 1.5 million years for a sample to contain sufficient DNA for contemporary sequencing technologies.\n\nResearch into the decay ofmitochondrialandnuclear DNAinmoabones has modelled mitochondrial DNA degradation to an average length of 1base pairafter 6,830,000 years at −5 °C.The decay kinetics have been measured by accelerated aging experiments, further displaying the strong influence of storage temperature and humidity on DNA decay.Nuclear DNA degrades at least twice as fast as mtDNA. Early studies that reported recovery of much older DNA, for example fromCretaceousdinosaurremains, may have stemmed from contamination of the sample.\n\nA critical review of ancient DNA literature through the development of the field highlights that few studies have succeeded in amplifying DNA from remains older than several hundred thousand years.A greater appreciation for the risks of environmental contamination and studies on thechemical stabilityof DNA have raised concerns over previously reported results. The alleged dinosaur DNA was later revealed to be humanY-chromosome.The DNA reported from encapsulatedhalobacteriahas been criticized based on its similarity to modern bacteria, which hints at contamination,or they may be the product of long-term, low-levelmetabolicactivity.\n\naDNA may contain a large number of postmortemmutations, increasing with time. Some regions of polynucleotide are more susceptible to this degradation, allowing erroneous sequence data to bypass statistical filters used to check the validity of data.Due to sequencing errors, great caution should be applied to interpretation of population size.Substitutions resulting fromdeaminationofcytosineresidues are vastly over-represented in the ancient DNA sequences. Miscoding ofCtoTandGtoAaccounts for the majority of errors.\n\nAnother problem with ancient DNA samples is contamination by modern human DNA and by microbial DNA (most of which is also ancient).New methods have emerged in recent years to prevent possible contamination of aDNA samples, including conducting extractions under extreme sterile conditions, using special adapters to identify endogenous molecules of the sample (distinguished from those introduced during analysis), and applying bioinformatics to resulting sequences based on known reads in order to approximate rates of contamination.\n\nDevelopment in the aDNA field in the 2000s increased the importance of authenticating recovered DNA to confirm that it is indeed ancient and not the result of recent contamination. As DNA degrades over time, the nucleotides that make up the DNA may change, especially at the ends of the DNA molecules. The deamination of cytosine to uracil at the ends of DNA molecules has become a way of authentication. DuringDNA sequencing, the DNA polymerases will incorporate an adenine (A) across from the uracil (U), leading to cytosine (C) to thymine (T) substitutions in the aDNA data.These substitutions increase in frequency as the sample gets older. Frequency measurement of the C-T level, ancient DNA damage, can be made using various software such as mapDamage2.0 or PMDtoolsand interactively on metaDMG.Due to hydrolytic depurination, DNA fragments into smaller pieces, leading to single-stranded breaks. Combined with the damage pattern, this short fragment length can also help differentiate between modern and ancient DNA.\n\nDespite the problems associated with aDNA, a wide and ever-increasing range of aDNA sequences have now been published from a range of animal and planttaxa. Tissues examined include artificially or naturally mummified animal remains,bone,shells,paleofaeces,alcohol preserved specimens,rodent middens,dried plant remains,and recently, extractions of animal and plant DNA directly fromsoilsamples.\n\nIn June 2013, a group of researchers includingEske Willerslev,Marcus Thomas Pius Gilbertand Orlando Ludovic of theCentre for Geogenetics,Natural History Museum of Denmarkat theUniversity of Copenhagen, announced that they had sequenced the DNA of a 560–780 thousand year old horse, using material extracted from a leg bone found buried inpermafrostin Canada'sYukonterritory.A German team also reported in 2013 the reconstructedmitochondrial genomeof a bear,Ursus deningeri, more than 300,000 years old, proving that authentic ancient DNA can be preserved for hundreds of thousand years outside of permafrost.The DNA sequence of even older nuclear DNA was reported in 2021 from the permafrost-preserved teeth of two Siberianmammoths, both over a million years old.\n\nResearchers in 2016 measured chloroplast DNA in marine sediment cores, and found diatom DNA dating back to 1.4 million years.This DNA had a half-life significantly longer than previous research, of up to 15,000 years. Kirkpatrick's team also found that DNA only decayed along a half-life rate until about 100 thousand years, at which point it followed a slower, power-law decay rate.\n\nDue to the considerableanthropological,archaeological, andpublic interestdirected toward human remains, they have received considerable attention from the DNA community. There are also more profound contamination issues, since the specimens belong to the same species as the researchers collecting and evaluating the samples.\n\nDue to themorphologicalpreservation in mummies, many studies from the 1990s and 2000s used mummified tissue as a source of ancient human DNA. Examples include both naturally preserved specimens, such as theÖtzi the Icemanfrozen in a glacierand bodies preserved through rapiddesiccationat high altitude in the Andes,as well as various chemically treated preserved tissue such as the mummies of ancient Egypt.However, mummified remains are a limited resource. The majority of human aDNA studies have focused on extracting DNA from two sources much more common in thearchaeological record:bonesandteeth. The bone that is most often used for DNA extraction is thepetrousear bone, since its dense structure provides good conditions for DNA preservation.Several other sources have also yielded DNA, includingpaleofaeces,andhair.Contamination remains a major problem when working on ancient human material.\n\nAncientpathogenDNA has been successfully retrieved from samples dating to more than 5,000 years old in humans and as long as 17,000 years ago in other species. In addition to the usual sources of mummified tissue, bones and teeth, such studies have also examined a range of other tissue samples, including calcifiedpleura,tissue embedded inparaffin,andformalin-fixed tissue.Efficient computational tools have been developed for pathogen and microorganism aDNA analyses in a small (QIIME) and large scale (FALCON).\n\nTaking preventative measures in their procedure against such contamination though, a 2012 study analyzed bone samples of aNeanderthalgroup in the El Sidrón cave, finding new insights on potential kinship and genetic diversity from the aDNA.In November 2015, scientists reported finding a 110,000-year-old tooth containing DNA from theDenisovan hominin, anextinctspeciesofhumanin the genusHomo.\n\nThe research has added new complexity to the peopling of Eurasia. A study from 2018showed that aBronze Agemass migration had greatly impacted the genetic makeup of the British Isles, bringing with it theBell Beakerculture from mainland Europe.\n\nIt has also revealed new information about links between the ancestors of Central Asians and the indigenous peoples of the Americas. In Africa, older DNA degrades quickly due to the warmer tropical climate,although, in September 2017, ancient DNA samples, as old as 8,100 years old, have been reported.\n\nMoreover, ancient DNA has helped researchers to estimate modern human divergence.By sequencing African genomes from three Stone Age hunter gatherers (2000 years old) and four Iron Age farmers (300 to 500 years old), Schlebusch and colleagues were able to push back the date of the earliest divergence between human populations to 350,000 to 260,000 years ago.\n\nAs of 2021, the oldest completely reconstructed human genomes are~45,000 years old.Such genetic data provides insights into the migration and genetic history – e.g.of Europe– including aboutinterbreeding between archaic and modern humanslike a common admixture between initial European modern humans and Neanderthals.",
      "sections": [
        {
          "level": 2,
          "heading": "History of ancient DNA studies"
        },
        {
          "level": 3,
          "heading": "1980s"
        },
        {
          "level": 3,
          "heading": "1990s"
        },
        {
          "level": 3,
          "heading": "2000s"
        },
        {
          "level": 3,
          "heading": "2020s"
        },
        {
          "level": 2,
          "heading": "Problems and errors"
        },
        {
          "level": 3,
          "heading": "Degradation processes"
        },
        {
          "level": 3,
          "heading": "Age limit"
        },
        {
          "level": 3,
          "heading": "Contamination"
        },
        {
          "level": 2,
          "heading": "Authentication of aDNA"
        },
        {
          "level": 2,
          "heading": "Non-human aDNA"
        },
        {
          "level": 2,
          "heading": "Human aDNA"
        },
        {
          "level": 3,
          "heading": "Results"
        },
        {
          "level": 2,
          "heading": "Researchers specializing in ancient DNA"
        }
      ],
      "raw_content_length": 653556,
      "cleaned_content_length": 14143,
      "scraped_at": "2025-09-02 15:30:38",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Ancient DNA (aDNA)",
      "discoverers": [
        "Russ Higuchi",
        "Svante Pääbo"
      ],
      "discovery_years": [
        "1984"
      ],
      "discovery_timeline": [
        "1984: First study of aDNA by Russ Higuchi",
        "1994: Report of mitochondrial cytochrome b sequences from dinosaur bones",
        "2007: Introduction of single primer extension amplification",
        "2009: Revolution in aDNA studies with cheaper techniques",
        "2010: Ability to sequence ancient human DNA",
        "2022: Recovery of two-million-year-old genetic material from Greenland",
        "2022: Nobel Prize awarded to Svante Pääbo"
      ],
      "mechanism": "Ancient DNA is isolated from ancient sources and is subject to degradation processes, making it more degraded than modern DNA. Techniques like PCR and NGS are used to amplify and sequence aDNA.",
      "key_features": [
        "Ability to recover DNA from ancient specimens",
        "Use of advanced sequencing technologies",
        "Improved standards for evaluating DNA results"
      ],
      "applications": [
        "Paleoarchaeology",
        "Evolutionary biology",
        "Genetic studies of extinct species"
      ],
      "significance": "Ancient DNA research has revolutionized our understanding of evolutionary history and human ancestry, providing insights into extinct species and ancient populations.",
      "institutions": [
        "University of California, Berkeley"
      ],
      "awards": [
        "2022 Nobel Prize in Physiology or Medicine"
      ],
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Water_on_Mars",
      "title": "Water on Mars",
      "main_content": "Although very small amounts of liquidwatermay occur transiently on the surface ofMars, limited to traces of dissolved moisture from the atmosphere and thin films,large quantities of ice are present on and under the surface. Small amounts of water vapor are present inthe atmosphere, and liquid water may be present under the surface. In addition, a large quantity of liquid water was likely present on the surface in the distant past. Currently, ice is mostly present inpolarpermafrost.More than 5 million km3of ice have been detected at or near the surface of Mars, enough to cover the planet to a depth of 35 meters (115 ft).Even more ice might be locked away in the deep subsurface.The chemical signature ofwater vaporon Mars was first unequivocally demonstrated in 1963 byspectroscopyusing an Earth-based telescope. In 2008 and 2013, ice was detected in soil samples taken by the Phoenix lander andCuriosityrover. In 2018, radar findings suggested the presence of liquid water insubglacial lakesand in 2024, seismometer data suggested the presence of liquid waterdeep under the surface.\n\nMost of the ice on Mars is buried. However, ice is present at the surface at several locations. In the mid-latitudes, surface ice is present in impact craters, steep scarps and gullies.At latitudes near the poles, ice is present inglaciers. Ice is visible at the surface at thenorth polar ice cap,and abundant ice is present beneath the permanentcarbon dioxideice cap at the Martian south pole.\n\nThe present-day inventory ofwater on Marscan be estimated from spacecraft images,remote sensingtechniques (spectroscopicmeasurements,ground-penetrating radar,etc.), and surface investigations from landers and rovers including x-ray spectroscopy, neutron spectroscopy and seismography.\n\nBefore about3.8 billion years ago, Mars may have had a denseratmosphereand higher surface temperatures,potentially allowing greater amounts of liquid water on the surface,possibly includinga large oceanthat may have covered one-third of the planet.Water has also apparently flowed across the surface for short periods at various intervals more recently in Mars' history.Aeolis PalusinGale Crater, explored by theCuriosityrover, is the geological remains of an ancientfreshwater lakethat could have been a hospitable environment formicrobial life.\n\nGeologic evidence of past water includes enormousoutflow channelscarved by floods,ancient rivervalley networks,deltas,andlakebeds;and the detection of rocks and minerals on the surface that could only have formed in liquid water.Numerousgeomorphicfeatures suggest the presence of ground ice (permafrost)and the movement of ice inglaciers, both in the recent pastand present.Gulliesandslope lineaealong cliffs and crater walls suggest that flowing water may continue to shape the surface of Mars, although what was thought to be low-volume liquidbrinesin shallowMartian soil, also calledrecurrent slope lineae,may be grains of flowing sand and dust slipping downhill to make dark streaks.\n\nAlthough the surface of Mars was periodically wet and could have been hospitable to microbial life billions of years ago,no definite evidence of life, past or present, has been found on Mars.The best potential locations for discoveringlife on Marsmay be in subsurface environments.A large amount of underground ice, equivalent to the volume of water inLake Superior, has been found underUtopia Planitia.In 2018, based on radar data, scientists reported the discovery of a possiblesubglacial lake on Mars, 1.5 km (0.93 mi) below thesouthern polar ice cap, with a horizontal extent of about 20 km (12 mi),findings that were strengthened by additional radar findings in September 2020,but subsequent work has questioned this detection.\n\nUnderstanding the extent and situation of water on Mars is important to assess the planet's potential for harboring life and for providing usableresources for future human exploration. For this reason, \"Follow the Water\" was the science theme ofNASA'sMars Exploration Program(MEP) in the first decade of the 21st century. NASA andESAmissions including2001 Mars Odyssey,Mars Express,Mars Exploration Rovers(MERs),Mars Reconnaissance Orbiter(MRO), and MarsPhoenixlanderhave provided information about water's abundance and distribution on Mars.Mars Odyssey, Mars Express, MRO, andMars Science LanderCuriosityroverare still operating, and discoveries continue to be made.\n\nIn August 2024, researchers reported that analysis of seismic data from NASA'sInSight Mars Landersuggested the presence of a reservoir of liquid water at depths of 10–20 kilometres (6.2–12.4 mi) under the Martian crust.\n\nThe notion of water on Mars preceded thespace ageby hundreds of years. Earlytelescopicobservers correctly assumed that the white polar caps and clouds were indications of water's presence. These observations, coupled with the fact that Mars has a 24-hour day, led astronomerWilliam Herschelto declare in 1784 that Mars probably offered its inhabitants \"a situation in many respects similar to ours.\"\n\nBy the start of the 20th century, most astronomers recognized that Mars was far colder and drier than Earth. The presence of oceans was no longer accepted, so the paradigm changed to an image of Mars as a \"dying\" planet with only a meager amount of water. The dark areas, which could be seen to change seasonally, were then thought to be tracts of vegetation.The person most responsible for popularizing this view of Mars wasPercival Lowell(1855–1916), who imagined a race of Martians constructing a network ofcanalsto bring water from the poles to the inhabitants at the equator. Although generating tremendous public enthusiasm, Lowell's ideas were rejected by most astronomers. The majority view of the scientific establishment at the time is probably best summarized by English astronomerEdward Walter Maunder(1851–1928) who compared the climate of Mars to conditions atop a twenty-thousand-foot (6,100 m) peak on an arctic islandwhere onlylichenmight be expected to survive.\n\nIn the meantime, many astronomers were refining the tool of planetaryspectroscopyin hope of determining the composition of theMartian atmosphere. Between 1925 and 1943,Walter AdamsandTheodore Dunhamat theMount Wilson Observatorytried to identify oxygen and water vapor in the Martian atmosphere, with generally negative results. The only component of the Martian atmosphere known for certain was carbon dioxide (CO2) identified spectroscopically byGerard Kuiperin 1947.Water vapor was not unequivocally detected on Mars until 1963, at the Mount Wilson Observatory.\n\nThe composition of thepolar caps, assumed to be water ice since the time ofCassini(1666), was questioned by a few scientists in the late 1800s who favored CO2ice, because of the planet's overall low temperature and apparent lack of appreciable water. This hypothesis was confirmed theoretically byRobert LeightonandBruce Murrayin 1966.Today it is known that the winter caps at both poles are primarily composed of CO2ice, but that a permanent (or perennial) cap of water ice remains during the summer at the northern pole. At the southern pole, a small cap of CO2ice remains during summer, but this cap too is underlain by perennial water ice as shown by spectroscopic data from 2004 from the Mars Express orbiter.\n\nThe final piece of the Martian climate puzzle was provided byMariner 4in 1965. Grainy television pictures from the spacecraft showed a surface dominated byimpact craters, which implied that the surface was very old and had not experienced the level of erosion and tectonic activity seen on Earth. Little erosion meant that liquid water had probably not played a large role in the planet'sgeomorphologyfor billions of years.Furthermore, the variations in the radio signal from the spacecraft as it passed behind the planet allowed scientists to calculate the density of the atmosphere. The results showed an atmospheric pressure less than 1% of Earth's at sea level, effectively precluding the existence of liquid water, which would rapidly boil or freeze at such low pressures.Thus, a vision of Mars was born of a world much like the Moon, but with just a wisp of an atmosphere to blow the dust around. This view of Mars would last nearly another decade untilMariner 9showed a much more dynamic Mars with hints that the planet's past environment was more clement than the present one.\n\nFor many years it was thought that the observed remains of floods were caused by the release of water from a global water table, but research published in 2015 reveals regional deposits of sediment and ice emplaced 450 million years earlier to be the source.\"Deposition of sediment from rivers and glacial melt filled giant canyons beneath primordial ocean contained within the planet's northern lowlands. It was the water preserved in these canyon sediments that was later released as great floods, the effects of which can be seen today.\"\n\nIt is widely accepted that Mars had abundant water very early in its history.Minerals that incorporate water or form in the presence of water are generally termed \"aqueous minerals\".Hydrated mineralsare minerals which have undergone a chemical reaction which adds water to their crystal structure.\n\nThe primary rock type on the surface of Mars isbasalt, a fine-grainedigneousrock which on Mars is made up mostly of themaficsilicate mineralsolivine,pyroxene, andplagioclase feldspar.When exposed to water and atmospheric gases, these mineralschemically weatherinto new (secondary) minerals, some of which may incorporate water into their crystalline structures, either as H2O or ashydroxyl(OH). Examples ofhydrated(or hydroxylated) minerals include the iron hydroxidegoethite(a common component of terrestrialsoils); theevaporitemineralsgypsumandkieserite;opalinesilica; andphyllosilicates(also calledclay minerals), such askaoliniteandmontmorillonite. All of these minerals have been detected on Mars.\n\nOne direct effect of chemical weathering is to consume water and other reactive chemical species, taking them from mobile reservoirs like theatmosphereandhydrosphereand sequestering them in rocks and minerals.The amount of water in the Martian crust stored ashydrated mineralsis currently unknown, but may be quite large.For example, mineralogical models of the rock outcroppings examined by instruments on theOpportunityroveratMeridiani Planumsuggest that thesulfatedeposits there could contain up to 22% water by weight.\n\nOn Earth, all chemical weathering reactions involve water to some degree.Many secondary minerals do not actually incorporate water, but still require water to form. Some examples of anhydrous secondary minerals include manycarbonates, somesulfates(e.g.,anhydrite), and metallic oxides such as the iron oxide mineralhematite. On Mars, a few of these weathering products could theoretically form without water or with scant amounts present as ice or in thin molecular-scale films (monolayers).\n\nAqueous minerals are sensitive indicators of the type of environment that existed when the minerals formed. The ease with which aqueous reactions occur (seeGibbs free energy) depends on the pressure, temperature, and on the concentrations of the gaseous and soluble species involved.Two important properties arepHandoxidation-reduction potential (Eh). For example, the sulfate mineraljarositeforms only in low pH (highly acidic) water. Phyllosilicates usually form in water of neutral to high pH (alkaline). Ehis a measure of theoxidation stateof an aqueous system. Together Ehand pH indicate the types of minerals that are thermodynamically most stable and therefore most likely to form from a given set of aqueous components. Thus, past environmental conditions on Mars, including those conducive to life, can be inferred from the types of minerals present in the rocks.\n\nAqueous minerals can also form in the subsurface byhydrothermalfluids migrating through pores and fissures. The heat source driving a hydrothermal system may be nearbymagmabodies or residual heat from largeimpacts.One important type of hydrothermal alteration in the Earth's oceanic crust isserpentinization, which occurs when seawater migrates throughultramaficand basaltic rocks. The water-rock reactions result in the oxidation of ferrous iron in olivine and pyroxene to produce ferric iron (as the mineralmagnetite) yielding molecularhydrogen(H2) as a byproduct. The process creates a highly alkaline and reducing (low Eh) environment favoring the formation of certain phyllosilicates (serpentine minerals) and various carbonate minerals, which together form a rock calledserpentinite.The hydrogen gas produced can be an important energy source forchemosyntheticorganisms or it can react with CO2to producemethanegas, a process that has been considered as a non-biological source for the trace amounts of methane reported in the Martian atmosphere.Serpentine minerals can also store a lot of water (as hydroxyl) in their crystal structure. A recent study has argued that hypothetical serpentinites in the ancient highland crust of Mars could hold as much as a 500 metres (1,600 ft)-thick global equivalent layer (GEL) of water.Although some serpentine minerals have been detected on Mars, no widespread outcroppings are evident from remote sensing data.This fact does not preclude the presence of large amounts of serpentinite hidden at depth in the Martian crust.\n\nThe rates at which primary minerals convert to secondary aqueous minerals vary. Primary silicate minerals crystallize from magma under pressures and temperatures vastly higher than conditions at the surface of a planet. When exposed to a surface environment these minerals are out ofequilibriumand will tend to interact with available chemical components to form more stable mineral phases. In general, the silicate minerals that crystallize at the highest temperatures (solidify first in a cooling magma) weather the most rapidly.On Earth and Mars, the most common mineral to meet this criterion isolivine, which readily weathers toclay mineralsin the presence of water. Olivine is widespread on Mars,suggesting that Mars' surface has not been pervasively altered by water; abundant geological evidence suggests otherwise.\n\nOver 60 meteorites have been found that came from Mars.Some of them contain evidence that they were exposed to water when on Mars. SomeMartian meteoritescalledbasalticshergottites, appear (from the presence of hydratedcarbonatesandsulfates) to have been exposed to liquid water prior to ejection into space.It has been shown that another class of meteorites, thenakhlites, were suffused with liquid water around 620 million years ago and that they were ejected from Mars around 10.75 million years ago by an asteroid impact. They fell to Earth within the last 10,000 years.Martian meteoriteNWA 7034has one order of magnitude more water than most other Martian meteorites. It is similar to the basalts studied by rover missions, and it was formed in the earlyAmazonian epoch.\n\nIn 1996, scientists reported the possible presence of microfossils in theAllan Hills 84001, a meteorite from Mars, which would have been strong evidence for ancient life on Mars.However, the current scientific consensus is that this meteorite does not contain evidence for life.\n\nThe 1971Mariner 9spacecraft caused a revolution in our ideas about water on Mars because the images it took showed ancient river beds. Huge ancient river valleys were found in many areas. Images showed evidence that in the distant past, floods of water broke through dams, carved deep valleys, eroded grooves into bedrock, and traveled thousands of kilometers.Areas of branched streambeds, in the southern hemisphere, suggested that rain once fell.The number of recognised valleys has increased through time. Research published in June 2010 mapped 40,000 river valleys on Mars, roughly quadrupling the number of river valleys that had previously been identified.Martian water-worn features can be classified into two distinct classes: 1) dendritic (branched), terrestrial-scale, widely distributed,Noachian-agevalley networksand 2) exceptionally large, long, single-thread, isolated,Hesperian-ageoutflow channels. Recent work suggests that there may also be a class of currently enigmatic, smaller, younger (HesperiantoAmazonian) channels in the mid-latitudes, perhaps associated with the occasional local melting of ice deposits.\n\nSome parts of Mars showinverted relief, which is created in the following way. First, sediments are deposited on the floor of a stream and then become resistant to erosion by formingcementsmade of calcite or iron oxides. Eventually, physical or chemical processes remove the surrounding weaker materials and the former streambeds become visible since they are resistant to these processes.Mars Global Surveyor found several examples of this process.Many inverted streams have been discovered in various regions of Mars, especially in theMedusae Fossae Formation,Miyamoto Crater,Saheki Crater,and the Juventae Plateau.\n\nA variety of lake basins have been discovered on Mars.Some are comparable in size to the largest lakes on Earth, such as theCaspian Sea,Black Sea, andLake Baikal. Lakes that were fed by valley networks are found in the southern highlands. There are places that are closed depressions with river valleys leading into them. These areas are thought to have once contained lakes; one is inTerra Sirenumthat had its overflow move throughMa'adim VallisintoGusev Crater, explored by theMars Exploration RoverSpirit. Another is nearParana Vallesand Loire Vallis.Some lakes are thought to have formed by precipitation, while others were formed from groundwater.Lakes are estimated to have existed in the Argyre basin,the Hellas basin,and maybe inValles Marineris.It is likely that at times in the Noachian, many craters hosted lakes. These lakes are consistent with a cold, dry (by Earth standards) hydrological environment somewhat like that of theGreat Basinof the western USA during theLast Glacial Maximum.\n\nResearch from 2010 suggests that Mars also had lakes along parts of the equator. Although earlier research had shown that Mars had a warm and wet early history that has long since dried up, these lakes existed in theHesperianEpoch, a much later period. Using detailed images from NASA'sMars Reconnaissance Orbiter, the researchers speculate that there may have been increased volcanic activity, meteorite impacts or shifts in Mars' orbit during this period to warm Mars' atmosphere enough to melt the abundant ice present in the ground. Volcanoes may have released gases that thickened the atmosphere for a temporary period, trapping more sunlight and making it warm enough for liquid water to exist. In this study, channels were discovered that connected lake basins nearAres Vallis. When one lake filled up, its waters overflowed the banks and carved the channels to a lower area where another lake would form.These dry lakes would be targets to look for evidence (biosignatures) of past life.\n\nIn 2012, NASA scientists announced that theCuriosityroverfound evidence for an ancientstreambedinGale Crater, suggesting an ancient \"vigorous flow\" of water on Mars.In particular, analysis of the now dry streambed indicated that the water ran at 3.3 km/h (0.92 m/s),possibly at hip-depth. Proof of running water came in the form of rounded pebbles and gravel fragments that could have only been weathered by strong liquid currents. Their shape and orientation suggests long-distance transport from above the rim of the crater, where a channel namedPeace Vallisfeeds into thealluvial fan.\n\nEridania Lakeis a theorized ancient lake with a surface area of roughly 1.1 million square kilometers.Its maximum depth would have been 2,400 meters and its volume would have been 562,000 km3. It was larger than the largest landlocked sea on Earth, theCaspian Sea, and contained more water than all the other Martian lakes together. The Eridania sea held more than nine times as much water as all of North America'sGreat Lakes.The upper surface of the lake was assumed to be at the elevation of valley networks that surround the lake; they all end at the same elevation, suggesting that they emptied into a lake.Research on this basin with CRISM found thick deposits, greater than 400 meters thick, that contained the mineralssaponite, talc-saponite, Fe-richmica(for example,glauconite-nontronite), Fe- and Mg-serpentine, Mg-Fe-Ca-carbonateand probable Fe-sulfide. The Fe-sulfide probably formed in deep water from water heated byvolcanoes. Such a process, classified ashydrothermalmay have been a place where life on Earth began.\n\nResearchers have found a number of examples ofdeltasthat formed in Martian lakes.Finding deltas is a major sign that Mars once had a lot of liquid water. Deltas usually require deep water over a long period of time to form. Also, the water level needs to be stable to keepsedimentfrom washing away. Deltas have been found over a wide geographical range,though there is some indication that deltas may be concentrated around the edges of the putative formernorthern ocean of Mars.\n\nBy 1979 it was thought thatoutflow channelsformed in single, catastrophic ruptures of subsurface water reservoirs, possibly sealed by ice, discharging colossal quantities of water across an otherwise arid Mars surface.In addition, evidence in favor of heavy or even catastrophic flooding is found in thegiant ripplesin theAthabasca Vallis.Many outflow channels begin atChaosorChasmafeatures, providing evidence for the rupture that could have breached a subsurface ice seal.\n\nThe branchingvalley networksof Mars are not consistent with formation by sudden catastrophic release of groundwater, both in terms of their dendritic shapes that do not come from a single outflow point, and in terms of the discharges that apparently flowed along them.Instead, some authors have argued that they were formed by slow seepage of groundwater from the subsurface essentially as springs.In support of this interpretation, the upstream ends of many valleys in such networks begin withbox canyonor \"amphitheater\" heads, which on Earth are typically associated with groundwater seepage. There is also little evidence of finer scale channels or valleys at the tips of the channels, which some authors have interpreted as showing the flow appeared suddenly from the subsurface with appreciable discharge, rather than accumulating gradually across the surface.Others have disputed the link between amphitheater heads of valleys and formation by groundwater for terrestrial examples,and have argued that the lack of fine scale heads to valley networks is due to their removal byweatheringorimpact gardening.Most authors accept that most valley networks were at least partly influenced and shaped by groundwater seep processes.\n\nGroundwateralso played a vital role in controlling broad scale sedimentation patterns and processes on Mars.According to this hypothesis, groundwater with dissolved minerals came to the surface, in and around craters, and helped to form layers by adding minerals—especially sulfate—andcementing sediments.In other words, some layers may have been formed by groundwater rising up depositing minerals and cementing existing, loose,aeoliansediments. The hardened layers are consequently more protected fromerosion. A study published in 2011 using data from theMars Reconnaissance Orbiter, show that the same kinds of sediments exist in a large area that includesArabia Terra.It has been argued that areas that are rich in sedimentary rocks are also those areas that most likely experienced groundwater upwelling on a regional scale.\n\nIn February 2019, European scientists published geological evidence of an ancient planet-wide groundwater system that was, arguably, connected to a putative vast ocean.\n\nThe Mars ocean hypothesis proposes that theVastitas Borealisbasin was the site of an ocean of liquid water at least once,and presents evidence that nearly a third of thesurfaceof Mars was covered by a liquid ocean early in the planet'sgeologic history.This ocean, dubbedOceanus Borealis,would have filled the Vastitas Borealis basin in the northern hemisphere, a region that lies 4–5 kilometres (2.5–3.1 mi) below the mean planetary elevation. Two major putative shorelines have been suggested: a higher one, dating to a time period of approximately 3.8 billion years ago and concurrent with the formation of thevalley networksin the Highlands, and a lower one, perhaps correlated with the youngeroutflow channels. The higher one, the 'Arabia shoreline', can be traced all around Mars except through the Tharsis volcanic region. The lower, the 'Deuteronilus', follows theVastitas Borealisformation.\n\nA study in June 2010 concluded that the more ancient ocean would have covered 36% of Mars.Data from the Mars Orbiter Laser Altimeter (MOLA), which measures the altitude of all terrain on Mars, was used in 1999 to determine that thewatershedfor such an ocean would have covered about 75% of the planet.Early Mars would have required a warmer climate and denser atmosphere to allow liquid water to exist at the surface.In addition, the large number of valley networks strongly supports the possibility of ahydrological cycleon the planet in the past.\n\nThe existence of a primordial Martian ocean remains controversial among scientists, and the interpretations of some features as 'ancient shorelines' has been challenged.One problem with the conjectured 2-billion-year-old (2Ga) shoreline is that it is not flat—i.e., does not follow a line of constant gravitational potential. This could be due to a change in distribution in Mars's mass, perhaps due to volcanic eruption or meteor impact;the Elysium volcanic province or the massive Utopia basin that is buried beneath the northern plains have been put forward as the most likely causes.\n\nIn March 2015, scientists stated that evidence exists for an ancient Martian ocean, likely in the planet's northern hemisphere and about the size of Earth'sArctic Ocean, or approximately 19% of the Martian surface. This finding was derived from the ratio of water anddeuteriumin the modern Martian atmosphere compared to the ratio found on Earth. Eight times as much deuterium was found at Mars than exists on Earth, suggesting that ancient Mars had significantly higher levels of water. Results from theCuriosityrover had previously found a high ratio of deuterium inGale Crater, though not significantly high enough to suggest the presence of an ocean. Other scientists caution that this new study has not been confirmed, and point out that Martian climate models have not yet shown that the planet was warm enough in the past to support bodies of liquid water.\n\nAdditional evidence for a northern ocean was published in May 2016, describing how some of the surface in Ismenius Lacus quadrangle was altered by twotsunamis. The tsunamis were caused by asteroids striking the ocean. Both were thought to have been strong enough to create 30 km diameter craters. The first tsunami picked up and carried boulders the size of cars or small houses. The backwash from the wave formed channels by rearranging the boulders. The second came in when the ocean was 300 m lower. The second carried a great deal of ice which was dropped in valleys. Calculations show that the average height of the waves would have been 50 m, but the heights would vary from 10 m to 120 m. Numerical simulations show that in this particular part of the ocean two impact craters of the size of 30 km in diameter would form every 30 million years. The implication here is that a great northern ocean may have existed for millions of years. One argument against an ocean has been the lack of shoreline features. These features may have been washed away by these tsunami events. The parts of Mars studied in this research areChryse Planitiaand northwesternArabia Terra. These tsunamis affected some surfaces in the Ismenius Lacus quadrangle and in theMare Acidalium quadrangle.\n\nIn July 2019, support was reported for anancient oceanon Mars that may have been formed by a possiblemega-tsunamisource resulting from ameteorite impactcreatingLomonosov crater.\n\nIn January 2022, a study about the climate 3 Gy ago on Mars shows that an ocean is stable with a water cycle that is closed.They estimate a return water flow, in form of ice in glacier, from the icy highlands to the ocean is in magnitude less than the Earth at the last glacial maximum. This simulation includes for the first time a circulation of the ocean. They demonstrate that the ocean's circulation prevent the ocean to freeze. These also shows that simulations are in agreement with observed geomorphological features identified as ancient glacial valleys.\n\nEvidence for solid, liquid, and gaseous forms of water has been found on Mars. Water ice likely exists in the polar ice caps, glaciers, surface ice, subsurface ice, in clouds and as snow precipitation. Water vapor has been detected in small amounts in the atmosphere. Controversial evidence suggests that liquid water may exist on Mars transiently in very small amounts on the surface, and some evidence suggests that large amounts of liquid water may exist under glaciers and far beneath the surface.\n\nUnder conditions typical of the surface of Mars (water vapor pressure <1 Paand ambient atmospheric pressure ~700 Pa), warming water ice on the Martian surface wouldsublime(transform into water vapor) at rates of up to 4 meters per year.\n\nIt is widely accepted that Mars had abundant water early in its history. A fraction of this water is retained on modern Mars as both ice and locked into the structure of abundant water-rich materials, includingclay minerals(phyllosilicates) andsulfates.Studies of hydrogen isotopic ratios indicate that when Mars was forming billions of years ago, asteroids and comets from beyond 2.5astronomical units(AU) provided water to Mars.The volume of water provided in this way is thought to be equivalent to 6% to 27% of the Earth's present ocean.\n\nA significant amount of surfacehydrogenhas been observed globally by theMars Odysseyneutron spectrometer andgamma ray spectrometerand theMars ExpressHigh Resolution Stereo Camera (HRSC).This hydrogen is thought to be incorporated into the molecular structure of ice, and throughstoichiometriccalculations the observed fluxes have been converted into concentrations of water ice in the upper meter of the Martian surface. This process has revealed that ice is both widespread and abundant on the present surface. Below 60 degrees of latitude, ice is concentrated in several regions, particularly around theElysiumvolcanoes,Terra Sabaea, and northwest ofTerra Sirenum, and exists in concentrations up to 18% ice in the subsurface. Above 60 degrees latitude, ice is highly abundant. Polewards on 70 degrees of latitude, ice concentrations exceed 25% almost everywhere, and approach 100% at the poles.TheSHARADandMARSISradar sounding instruments have also confirmed that individual surface features are ice rich. Due to the known instability of ice at current Martian surface conditions, it is thought that almost all of this ice is covered by a thin layer of rocky or dusty material.\n\nThe Mars Odyssey neutron spectrometer observations indicate that if all the ice in the top meter of the Martian surface were spread evenly, it would give a Water Equivalent Global layer (WEG) of at least ≈14 centimetres (5.5 in)—in other words, the globally averaged Martian surface is approximately 14% water.The water ice currently locked in both Martian poles corresponds to a WEG of 30 metres (98 ft), and geomorphic evidence favors significantly larger quantities ofsurface waterover geologic history, with WEG as deep as 500 metres (1,600 ft).It is thought that part of this past water has been lost to the deep subsurface, and part to space, although the detailed mass balance of these processes remains poorly understood.The current atmospheric reservoir of water is important as a conduit allowing gradual migration of ice from one part of the surface to another on both seasonal and longer timescales, but it is insignificant in volume, with a WEG of no more than 10 micrometres (0.00039 in).\n\nIt is possible that liquid water could also exist on the surface of Mars through the formation ofbrinessuggested by the abundance of hydrated salts.Brines are significant on Mars because they can stabilize liquid water at lower temperatures than pure water on its own.Pure liquid water is unstable on the surface of the planet, as it is subjected to freezing, evaporation, and boiling.Similar to how salt is applied to roads on Earth to prevent them from icing over, briny mixtures of water and salt on Mars may have low enough freezing points to lead to stable liquid at the surface. Given the complex nature of the Martianregolith, mixtures of salts are known to change the stability of brines.Modeling thedeliquescenceof salt mixtures can be used to test for brine stability and can help us determine if liquid brines are present on the surface of Mars. The composition of the Martian regolith, determined by thePhoenixlander, can be used to constrain these models and give an accurate representation of how brines may actually form on the planet.Results of these models givewater activityvalues for various salts at different temperatures, where the lower the water activity, the more stable the brine. At temperatures between 208 K and 253 K,chloratesalts exhibit the lowest water activity values, and below 208 Kchloridesalts exhibit the lowest values. Results of modeling show that the aforementioned complex mixtures of salts do not significantly increase the stability of brines, indicating that brines may not be a significant source of liquid water at the surface of Mars.\n\nIn September 2019, researchers reported that theInSightlander uncovered unexplainedmagnetic pulses, andmagnetic oscillationsconsistent with liquid water deep underground.\n\nBased on data from the InSight lander, scientists suggested liquid water exists deep underground. According to a paper published April 25, 2025 in the journal National Science Review, recordings of seismic waves from deep within the Red Planet indicate that a layer of liquid water may be in the Martian rocks between 3.4 and 5 miles [5.4 to 8 kilometers] below the surface.\n\nThe total volume of hidden water could flood the whole of Mars' surface with an ocean 1,700 to 2,560 feet [520 to 780 metres] deep, around the same volume of liquid that is contained within Antarctica's ice sheet, the study authors estimate.Another group earilier found similar results and suggested the water would be in fractures in igneous rocks.Liquid water in the Martian mid-crust\n\nThe existence of ice in the Martian northern (Planum Boreum) and southern (Planum Australe) polar caps has been known since the time ofMariner 9orbiter.However, the amount and purity of this ice were not known until the early 2000s. In 2004, theMARSISradar sounder on the EuropeanMars Expresssatellite confirmed the existence of relatively clean ice in the south polar ice cap that extends to a depth of 3.7 kilometres (2.3 mi) below the surface.Similarly, the SHARAD radar sounder on board theMars Reconnaissance Orbiterobserved the base of the north polar cap 1.5 – 2 km beneath the surface. Together, the volume of ice present in the Martian north and south polar ice caps is similar to that of theGreenland ice sheet.\n\nAn even larger ice sheet on south polar region sheet is suspected to have retreated in ancient times (Hesperian period), that may have contained 20 million km3of water ice, which is equivalent to a layer 137 m deep over the entire planet.\n\nBoth polar caps reveal abundant internal layers of ice and dust when examined with images of the spiral-shaped troughs that cut through their volume, and the subsurface radar measurements showed that these layers extend continuously across the ice sheets. This layering contains a record of past climates on Mars, just how Earth's ice sheets have a record for Earth's climate. Reading this record is not straightforward however,so, many researchers have studied this layering not only to understand the structure, history, and flow properties of the caps,but also to understand the evolution of climate on Mars.\n\nSurrounding the polar caps are many smaller ice sheets inside craters, some of which lie under thick deposits of sand or martian dust.Particularly, the 81.4 kilometres (50.6 mi) wideKorolev Crater, is estimated to contain approximately 2,200 cubic kilometres (530 cu mi) of water ice exposed to the surface.Korolev's floor lies about 2 kilometres (1.2 mi) below the rim, and is covered by a 1.8 kilometres (1.1 mi) deep central mound of permanent water ice, up to 60 kilometres (37 mi) in diameter.\n\nThe potential existence of subglacial lakes on Mars was hypothesised when modelling ofLake VostokinAntarcticashowed that this lake could have existed before the Antarctic glaciation, and that a similar scenario could potentially have occurred on Mars.In July 2018, scientists from theItalian Space Agencyreported the detection of such a potentialsubglacial lakeon Mars, 1.5 kilometres (1 mi) below thesouthern polar ice cap, and spanning 20 kilometres (10 mi) horizontally, the first evidence for a potential stable body of liquid water on the planet.The evidence for this potentialMartian lakewas deduced from a bright spot in the radar echo sounding data of theMARSISradar on board the EuropeanMars Expressorbiter,collected between May 2012 and December 2015. The potential lake is centred at 193°E, 81°S, a flat area that does not exhibit any peculiar topographic characteristics but is surrounded by higher ground, except on its eastern side where there is a depression.TheSHARADradar on board NASA'sMars Reconnaissance Orbiterhas seen no sign of the lake.\n\nOn 28 September 2020, the MARSIS discovery was supported, using new data, and reanalysing all the data with a new technique. These new radar studies report three more potential subglacial lakes on Mars. All are 1.5 km (0.93 mi) below thesouthern polar ice cap. The size of the first potential lake found, and the largest, has been corrected to 30 km (19 mi) wide. It is surrounded by 3 smaller potential lakes, each a few kilometres wide.\n\nBecause the temperature at the base of the polar cap is estimated to be 205 K (−68 °C; −91 °F), scientists assume that water could remain liquid through the antifreeze effect of magnesium and calciumperchlorates.The 1.5-kilometre (0.93 mi) ice layer covering the potential lake is composed of water ice with 10 to 20% admixed dust, and seasonally covered by a 1-metre-thick (3 ft 3 in) layer of CO2ice.Since the raw-data coverage of the south polar ice cap is limited, the discoverers stated that \"there is no reason to conclude that the presence of subsurface water on Mars is limited to a single location.\"\n\nIn 2019, a study was published that explored the physical conditions necessary for such a lake to exist.The study calculated the amount of geothermal heat necessary to reach temperatures under which a liquid water and perchlorate mix would be stable under the ice. The authors concluded that \"even if there are local concentrations of large amounts of perchlorate salts at the base of the south polar ice, typical Martian conditions are too cold to melt the ice ... a local heat source within the crust is needed to increase the temperatures, and a magma chamber within 10 km of the ice could provide such a heat source. This result suggests that if the liquid water interpretation of the observations is correct, magmatism on Mars may have been active extremely recently.\"\n\nChina's Zhurong rover that studied Utopia Planitia region of Mars found a shift in sand dunes at around the same time as layers in the North polar region changed. Researchers believe that the tilt of Mars changed at that time and produced changes in the winds at Zhurong's landing site and in the layers in the ice cap.\n\nIf a liquid lake does indeed exist, its salty water may also be mixed with soil to form a sludge.The lake's high levels of salt would present difficulties for most lifeforms. On Earth, organisms calledhalophilesexist that thrive in extremely salty conditions, though not in dark, cold, concentrated perchlorate solutions.Nevertheless, halotolerant organisms might be able to cope with enhanced perchlorate concentrations by drawing on physiological adaptations similar to those observed in the yeastDebaryomyces hanseniiexposed in lab experiments to increasingNaClO4concentrations.\n\nFor many years, various scientists have suggested that some Martian surfaces look likeperiglacialregions on Earth.By analogy with these terrestrial features, it has been argued for many years that these may be regions ofpermafrost. This would suggest that frozen water lies right beneath the surface.A common feature in the higher latitudes,patterned ground, can occur in a number of shapes, including stripes and polygons. On the Earth, these shapes are caused by the freezing and thawing of soil.There are other types of evidence for large amounts of frozen water under the surface of Mars, such asterrain softening, which rounds sharp topographical features.Evidence from Mars Odyssey'sgamma ray spectrometerand direct measurements with thePhoenixlander have corroborated that many of these features are intimately associated with the presence of ground ice.\n\nIn 2018, using the HiRISE camera on board theMars Reconnaissance Orbiter(MRO), researchers found at least eight eroding slopes showing exposed water ice sheets as thick as 100 meters, covered by a layer of about 1 or 2 meters thick ofsoil.The sites are at latitudes from about 55 to 58 degrees, suggesting that there is shallow ground ice under roughly a third of the Martian surface.This image confirms what was previously detected with the spectrometer on2001 Mars Odyssey, the ground-penetrating radars on MRO and onMars Express, and by thePhoenixlanderin situexcavation.These ice layers hold easily accessible clues about Mars' climate history and make frozen water accessible to future robotic or human explorers.Some researchers suggested these deposits could be the remnants of glaciers that existed millions of years ago when the planet's spin axis and orbit were different. (See sectionMars' Ice agesbelow.) A more detailed study published in 2019 discovered that water ice exists at latitudes north of 35°N and south of 45°S, with some ice patches only a few centimeters from the surface covered by dust. Extraction of water ice at these conditions would not require complex equipment.\n\nCertain regions of Mars displayscalloped-shaped depressions. The depressions are suspected to be the remains of a degrading ice-rich mantle deposit. Scallops are caused by icesublimatingfrom frozen soil. The landforms of scalloped topography can be formed by the subsurface loss of water ice by sublimation under current Martian climate conditions. A model predicts similar shapes when the ground has large amounts of pure ice, up to many tens of meters in depth.This mantle material was probably deposited from the atmosphere as ice formed on dust when the climate was different due to changes in the tilt of the Mars pole (see§ Ice ages, below).The scallops are typically tens of meters deep and from a few hundred to a few thousand meters across. They can be almost circular or elongated. Some appear to have coalesced causing a large heavily pitted terrain to form. The process of forming the terrain may begin with sublimation from a crack. There are often polygonal cracks where scallops form, and the presence of scalloped topography seems to be an indication of frozen ground.\n\nOn November 22, 2016, NASA reported finding a large amount of underground ice in the Utopia Planitia region of Mars.The volume of water detected has been estimated to be equivalent to the volume of water inLake Superior.\n\nThe volume of water ice in the region were based on measurements from the ground-penetrating radar instrument onMars Reconnaissance Orbiter, calledSHARAD. From the data obtained from SHARAD, “dielectric permittivity”, or the dielectric constant was determined. The dielectric constant value was consistent with a large concentration of water ice.\n\nThese scalloped features are superficially similar toSwiss cheese features, found around the south polar cap. Swiss cheese features are thought to be due to cavities forming in a surface layer of solidcarbon dioxide, rather than water ice—although the floors of these holes are probably H2O-rich.\n\nOn July 28, 2005, theEuropean Space Agencyannounced the existence of a crater partially filled with frozen water;some then interpreted the discovery as an \"ice lake\".Images of the crater, taken by theHigh Resolution Stereo Cameraon board theEuropean Space Agency'sMars Expressorbiter, clearly show a broad sheet of ice in the bottom of an unnamed crater located onVastitas Borealis, a broad plain that covers much of Mars' far northern latitudes, at approximately 70.5° North and 103° East. The crater is 35 kilometres (22 mi) wide and about 2 kilometres (1.2 mi) deep. The height difference between the crater floor and the surface of the water ice is about 200 metres (660 ft). ESA scientists have attributed most of this height difference to sand dunes beneath the water ice, which are partially visible. While scientists do not refer to the patch as a \"lake\", the water ice patch is remarkable for its size and for being present throughout the year. Deposits of water ice and layers of frost have been found in many different locations on the planet.\n\nAs more and more of the surface of Mars has been imaged by the modern generation of orbiters, it has become gradually more apparent that there are probably many more patches of ice scattered across the Martian surface. Many of these putative patches of ice are concentrated in the Martian mid-latitudes (≈30–60° N/S of the equator). For example, many scientists think that the widespread features in those latitude bands variously described as \"latitude dependent mantle\" or \"pasted-on terrain\" consist of dust- or debris-covered ice patches, which are slowly degrading.A cover of debris is required both to explain the dull surfaces seen in the images that do not reflect like ice, and also to allow the patches to exist for an extended period of time without subliming away completely. These patches have been suggested as possible water sources for some of the enigmatic channelized flow features likegulliesalso seen in those latitudes.\n\nSurface features consistent with existingpack icehave been discovered in the southernElysium Planitia.What appear to be plates, ranging in size from 30 metres (98 ft) to 30 kilometres (19 mi), are found in channels leading to a large flooded area. The plates show signs of break up and rotation that clearly distinguish them from lava plates elsewhere on the surface of Mars. The source for the flood is thought to be the nearby geological faultCerberus Fossaethat spewed water as well as lava aged some 2 to 10 million years. It was suggested that the water exited theCerberus Fossaethen pooled and froze in the low, level plains and that such frozen lakes may still exist.\n\nMany large areas of Mars either appear to host glaciers, or carry evidence that they used to be present. Much of the areas in high latitudes, especially theIsmenius Lacus quadrangle, are suspected to still contain enormous amounts of water ice.Recent evidence has led many planetary scientists to conclude that water ice still exists as glaciers across much of the Martian mid- and high latitudes, protected from sublimation by thin coverings of insulating rock and/or dust.An example of this are the glacier-like features calledlobate debris apronsin an area calledDeuteronilus Mensae, which display widespread evidence of ice lying beneath a few meters of rock debris.Glaciers are associated withfretted terrain, and many volcanoes. Researchers have described glacial deposits onHecates Tholus,Arsia Mons,Pavonis Mons,andOlympus Mons.Glaciers have also been reported in a number of larger Martian craters in the mid-latitudes and above.\n\nGlacier-like features on Mars are known variously as viscous flow features,Martian flow features, lobate debris aprons,or lineated valley fill,depending on the form of the feature, its location, the landforms it is associated with, and the author describing it. Many, but not all, small glaciers seem to be associated with gullies on the walls of craters and mantling material.The lineated deposits known as lineated valley fill are probably rock-covered glaciers that are found on the floors of most channels within thefretted terrainfound aroundArabia Terrain the northern hemisphere. Their surfaces have ridged and grooved materials that deflect around obstacles. Lineated floor deposits may be related tolobate debris aprons, which have been proven to contain large amounts of ice by orbiting radar.For many years, researchers interpreted that features called 'lobate debris aprons' were glacial flows and it was thought that ice existed under a layer of insulating rocks.With new instrument readings, it has been confirmed that lobate debris aprons contain almost pure ice that is covered with a layer of rocks.\n\nAnalysis ofSHARADdata led researchers to conclude that lobate debris aprons (LDAs) are over 80% pure ice. The paper's authors examined five different sites from around the planet, and all showed high levels of pure water ice.Because of the high purity of the ice content, the authors argued that the formation of glaciers happened by atmospheric precipitation or direct condensation. After glaciers were formed, there was a time when enhanced sublimation formed a lag layer or promoted the accumulation of dry debris atop the water ice glacier. That dry debris would then insulate the underlying ice from going away.\n\nMoving ice carries rock material, then drops it as the ice disappears. This typically happens at the snout or edges of the glacier. On Earth, such features would be calledmoraines, but on Mars they are typically known asmoraine-like ridges,concentric ridges, orarcuate ridges.Since ice tends to sublime rather than melt on Mars, and because Mars's low temperatures tend to make glaciers \"cold based\" (frozen down to their beds, and unable to slide), the remains of these glaciers and the ridges they leave do not appear the exactly same as normal glaciers on Earth. In particular, Martian moraines tend to be deposited without being deflected by the underlying topography, which is thought to reflect the fact that the ice in Martian glaciers is normally frozen down and cannot slide.Ridges of debris on the surface of the glaciers indicate the direction of ice movement. The surface of some glaciers have rough textures due tosublimationof buried ice. The ice evaporates without melting and leaves behind an empty space. Overlying material then collapses into the void.Sometimes chunks of ice fall from the glacier and get buried in the land surface. When they melt, a more or less round hole remains. Many of these \"kettle holes\" have been identified on Mars.\n\nDespite strong evidence for glacial flow on Mars, there is little convincing evidence forlandformscarved by glacialerosion, e.g.,U-shaped valleys,crag and tailhills,arêtes,drumlins. Such features are abundant in glaciated regions on Earth, so their absence on Mars has proven puzzling. The lack of these landforms is thought to be related to the cold-based nature of the ice in most recent glaciers on Mars. Because thesolar insolationreaching the planet, the temperature and density of the atmosphere, and thegeothermal heat fluxare all lower on Mars than they are on Earth, modelling suggests the temperature of the interface between a glacier and its bed stays below freezing and the ice is literally frozen down to the ground. This prevents it from sliding across the bed, which is thought to inhibit the ice's ability to erode the surface.\n\nIn August 2024, researchers reported on a new analysis of seismometer readings suggesting the presence of liquid water, trapped in tiny cracks and pores of rock, deep in the rocky outer crust, at a depth of six to 12 miles (10 to 20km) below the surface. The data came from NASA's Mars Insight Lander, which recorded four years' of vibrations - Mars quakes - from deep inside the planet.\n\nThe research only analyzed the portion of Mars directly below the InSight lander. However, the researchers speculated that if their findings are representative of the rest of Mars, there would be enough water to fill oceans on the planet’s surface, covering the entirety of Mars to a depth of 1 mile (1.6 kilometers).\n\nA study from 2021 estimated that, based on observed ratios of deuterium to hydrogen on Mars, between 30 and 99 percent of the water on Mars may be trapped in the crust, mostly in the form of hydrated minerals.\n\nPure liquid water cannot exist in a stable form on the surface of Mars with its present low atmospheric pressure and low temperature because it would boil, except at the lowest elevations for a few hours.So, a geological mystery commenced in 2006 when observations from NASA'sMars Reconnaissance Orbiterrevealedgullydeposits that were not there ten years prior, possibly caused by flowing liquidbrineduring the warmest months on Mars.The images were of two craters inTerra SirenumandCentauri Montesthat appear to show the presence of flows (wet or dry) on Mars at some point between 1999 and 2001.\n\nThere is disagreement in the scientific community as to whether or not gullies are formed by liquid water. While some scientists believe that most gullies are formed by liquid water formed from snow or ice melting,other scientists believe that gullies are formed by dry flows possibly lubricated by sublimating carbon dioxide that forms from freezing of the martian atmosphere.\n\nSome studies attest that gullies forming in the southern highlands could not be formed by water due to improper conditions. The low pressure, non-geothermal, colder regions would not give way to liquid water at any point in the year but would be ideal for solid carbon dioxide. The carbon dioxide melting in the warmer summer would yield liquid carbon dioxide which would then form the gullies.Even if gullies are carved by flowing water at the surface, the exact source of the water and the mechanisms behind its motion are not understood.\n\nIn August 2011, NASA announced the discovery of current seasonal changes on steep slopes below rocky outcrops near crater rims in the Southern hemisphere. These dark streaks, now calledrecurrent slope lineae(RSL), were seen to grow downslope during the warmest part of the Martian Summer, then to gradually fade through the rest of the year, recurring cyclically between years.The researchers suggested these marks were consistent with salty water (brines) flowing downslope and then evaporating, possibly leaving some sort of residue.The CRISM spectroscopic instrument has since made direct observations of hydrous salts appearing at the same time that these recurrent slope lineae form, confirming in 2015 that these lineae are produced by the flow of liquid brines through shallow soils. The lineae contain hydrated chlorate andperchloratesalts (ClO4−), which contain liquid water molecules.The lineae flow downhill in Martian summer, when the temperature is above −23 °C (−9 °F; 250 K).However, the source of the water remains unknown.However, neutron spectrometer data by theMars Odysseyorbiter obtained over one decade, was published in December 2017, and shows no evidence of water (hydrogenated regolith) at the active sites, so its authors also support the hypotheses of either short-lived atmospheric water vapour deliquescence, or dry granular flows.They conclude that liquid water on today's Mars may be limited to traces of dissolved moisture from the atmosphere and thin films, which are challenging environments for life as it is currently known.\n\nAn alternative scenario is a Knudsen pump effect, from photophoretic when shadows occurs in a granular material.The authors demonstrated that the RSLs stopped at an angle of 28° in Garni crater, in agreement with dry granular avalanche. In addition, the authors pointed out several limitations of the wet hypothesis, such as the fact that the detection of water was only indirect (salt detection but not water).\n\nPrecipitation, most likely consisting of snow made of water ice, was observed to fall from cirrus clouds by the Phoenix lander.\n\nThe variation in Mars's surface water content is strongly coupled to the evolution of its atmosphere and may have been marked by several key stages. Head and others put together a detailed history of water on Mars and presented it in March, 2023.In March 2021, researchers reported findings, based on ratios ofdeuteriumto hydrogen, suggesting that a considerable amount of water has likely been sequestered into the rocks and crust of the planet over the years instead of being lost to space.\n\nThe early Noachian era was characterized by atmospheric loss to space from heavy meteoritic bombardment and hydrodynamic escape.Ejection by meteorites may have removed ~60% of theearly atmosphere.Significant quantities ofphyllosilicatesmay have formed during this period requiring a sufficiently dense atmosphere to sustain surface water, as the spectrally dominant phyllosilicate group, smectite, suggests moderate water-to-rock ratios.However, the pH-pCO2between smectite and carbonate show that the precipitation of smectite would constrain pCO2to a value not more than 1×10−2atm (1.0 kPa).As a result, the dominant component of a dense atmosphere on early Mars becomes uncertain, if the clays formed in contact with the Martian atmosphere,particularly given the lack of evidence forcarbonate deposits. An additional complication is that the ~25% lower brightness of the young Sun would have required an ancient atmosphere with a significantgreenhouse effectto raise surface temperatures to sustain liquid water.Higher CO2content alone would have been insufficient, as CO2precipitates atpartial pressuresexceeding 1.5 atm (1,500 hPa), reducing its effectiveness as agreenhouse gas.\n\nDuring the middle to late Noachian era, Mars underwent potential formation of asecondary atmosphereby outgassing dominated by the Tharsis volcanoes, including significant quantities of H2O, CO2, and SO2.Martian valley networks date to this period, indicating globally widespread and temporally sustained surface water as opposed to catastrophic floods.The end of this period coincides with the termination of the internalmagnetic fieldand a spike in meteoritic bombardment.The cessation of the internal magnetic field and subsequent weakening of any localmagnetic fieldsallowed unimpededatmospheric strippingby the solar wind. For example, when compared with their terrestrial counterparts,38Ar/36Ar,15N/14N, and13C/12C ratios of the Martian atmosphere are consistent with ~60% loss of Ar, N2, and CO2by solar wind stripping of an upper atmosphere enriched in the lighter isotopes viaRayleigh fractionation.Supplementing the solar wind activity, impacts would have ejected atmospheric components in bulk without isotopic fractionation. Nevertheless, cometary impacts in particular may have contributed volatiles to the planet.\n\nAtmospheric enhancement by sporadic outgassing events were countered by solar wind stripping of the atmosphere, albeit less intensely than by the young Sun.Catastrophic floods date to this period, favoring sudden subterranean release of volatiles, as opposed to sustained surface flows.While the earlier portion of this era may have been marked by aqueous acidic environments and Tharsis-centric groundwater dischargedating to the late Noachian, much of the surface alteration processes during the latter portion is marked by oxidative processes including the formation of Fe3+oxides that impart a reddish hue to the Martian surface.Such oxidation of primary mineral phases can be achieved by low-pH (and possibly high temperature) processes related to the formation of palagonitic tephra,by the action of H2O2that forms photochemically in the Martian atmosphere,and by the action of water,none of which require free O2. The action of H2O2may have dominated temporally given the drastic reduction in aqueous and igneous activity in this recent era, making the observed Fe3+oxides volumetrically small, though pervasive and spectrally dominant.Nevertheless, aquifers may have driven sustained, but highly localized surface water in recent geologic history, as evident in the geomorphology of craters such as Mojave.Furthermore, theLafayetteMartian meteoriteshows evidence of aqueous alteration as recently as 650 Ma.\n\nIn 2020 scientists reported that Mars' current loss of atomic hydrogen from water is largely driven by seasonal processes anddust stormsthat transport water directly to the upper atmosphere and that this has influenced the planet's climate likely during the last 1 Ga.More recent studies have suggested that upward propagating atmospheric gravity waves can play an important role during global dust storms in modulating water escape.\n\nMars has experienced about 40 large scale changes in the amount and distribution of ice on its surface over the past five million years,with the most recent happening about 2.1 to 0.4 Myr ago, during the LateAmazonianglaciation at thedichotomyboundary.These changes are known as ice ages.Ice ages on Mars are very different from the ones that the Earth experiences. Ice ages are driven by changes in Mars's orbit andtilt—also known as obliquity. Orbital calculations show that Mars wobbles on its axis far more than Earth does. The Earth is stabilized by its proportionally large moon, so it only wobbles a few degrees. Mars may change its tilt by many tens of degrees.When this obliquity is high, its poles get much more direct sunlight and heat; this causes the ice caps to warm and become smaller as ice sublimes. Adding to the variability of the climate, theeccentricityof the orbit of Mars changes twice as much as Earth's eccentricity. As the poles sublime, the ice is redeposited closer to the equator, which receive somewhat lesssolar insolationat these high obliquities.Computer simulations have shown that a 45° tilt of the Martian axis would result in ice accumulation in areas that display glacial landforms.\n\nThe moisture from the ice caps travels to lower latitudes in the form of deposits of frost or snow mixed with dust. The atmosphere of Mars contains a great deal of fine dust particles, the water vapor condenses on these particles that then fall down to the ground due to the additional weight of the water coating. When ice at the top of the mantling layer returns to the atmosphere, it leaves behind dust that serves to insulate the remaining ice.The total volume of water removed is a few percent of the ice caps, or enough to cover the entire surface of the planet under one meter of water. Much of this moisture from the ice caps results in a thick smooth mantle with a mixture of ice and dust.This ice-rich mantle, that can be 100 meters thick at mid-latitudes,smoothes the land at lower latitudes, but in places it displays a bumpy texture or patterns that give away the presence of former water ice underneath.\n\nSince theVikinglandersthat searched for current microbial life in 1976, NASA has pursued a \"follow the water\" strategy on Mars. However, liquid water is a necessary but not sufficient condition for life as we know it becausehabitabilityis a function of a multitude of environmental parameters.\n\nHabitable environments need not be inhabited, and for purposes ofplanetary protection, scientists are trying to identify potential habitats where stowaway bacteria from Earth on spacecraft could contaminate Mars.If life exists—or existed—on Mars, evidence orbiosignaturescould be found in the subsurface, away from present-day harsh surface conditions such asperchlorates,ionizing radiation, desiccation and freezing.Habitable locations could occur kilometers below the surface in a hypothetical hydrosphere, or it could occur near the sub-surface in contact with permafrost.\n\nTheCuriosityrover is assessing Mars' past and present habitability potential. The European-RussianExoMarsprogramme is an astrobiology project dedicated to the search for and identification of biosignatures on Mars. It includes theExoMars Trace Gas Orbiterthat started mapping theatmospheric methanein April 2018, and the plannedExoMars roverthat will drill and analyze subsurface samples 2 meters deep. NASA'sPerseverance roverhas cached samples for their potential transport to Earth laboratories in the late 2020s or 2030s.\n\nThe images acquired by theMariner 9Mars orbiter, launched in 1971, revealed the first evidence of past liquid water in the form of dry river beds,canyons(including theValles Marineris, a system of canyons over about 4,020 kilometres (2,500 mi) long), evidence of watererosionand deposition.The findings from the Mariner 9 missions underpinned the laterViking program. The enormousValles Marineriscanyon system is named after Mariner 9 in honor of its achievements.\n\nBy discovering many geological forms that are typically formed from large amounts of water, the twoVikingorbiters and the two landers (1976-1982) caused a revolution in our knowledge about water on Mars. Hugeoutflow channelswere found in many areas. They showed that floods of water broke through dams, carved deep valleys, eroded grooves into bedrock, and traveled thousands of kilometers.Large areas in the southern hemisphere contained branchedvalley networks, suggesting that rain once fell.Many craters look as if the impactor fell into mud. When they were formed, ice in the soil may have melted, turned the ground into mud, then the mud flowed across the surface.Regions, called \"Chaotic Terrain,\" seemed to have quickly lost great volumes of water that caused large channels to form downstream. Estimates for some channel flows run to ten thousand times the flow of theMississippi River.Underground volcanism may have melted frozen ice; the water then flowed away and the ground collapsed to leave chaotic terrain. Also, general chemical analysis by the two Viking landers suggested the surface has been either exposed to or submerged in water in the past.\n\nIn 1998, data from the Mars Orbiter Laser Altimeter of the Mars Global Surveyor orbiter showed that the topography of the northern polar ice cap was consistent with a composition of primarily water ice.\n\nTheMars Global Surveyor's (1996-2006)Thermal Emission Spectrometer(TES) was an instrument able to determine the mineral composition on the surface of Mars. Mineral composition gives information on the presence or absence of water in ancient times. TES identified a large (30,000 square kilometres (12,000 sq mi)) area in theNili Fossaeformation that contains the mineralolivine.It is thought that the ancient asteroid impact that created theIsidis basinresulted in faults that exposed the olivine. The discovery of olivine is strong evidence that parts of Mars have been extremely dry for a long time. Olivine was also discovered in many other small outcrops within 60 degrees north and south of the equator.The probe imaged several channels that suggest past sustained liquid flows, two of them are found inNanedi Vallesand inNirgal Vallis.\n\nThePathfinderlander (1997-1998) recorded the variation of diurnal temperature cycle. It was coldest just before sunrise, about −78 °C (−108 °F; 195 K), and warmest just after Mars noon, about −8 °C (18 °F; 265 K). At this location, the highest temperature never reached the freezing point of water (0 °C (32 °F; 273 K)), too cold for pure liquid water to exist on the surface.\n\nThe atmospheric pressure measured by the Pathfinder on Mars is very low —about 0.6% of Earth's, and it would not permit pure liquid water to exist on the surface.\n\nOther observations were consistent with water being present in the past. Some of the rocks at the Mars Pathfinder site leaned against each other in a manner geologists term imbricated. It is suspected that strong flood waters in the past pushed the rocks around until they faced away from the flow. Some pebbles were rounded, perhaps from being tumbled in a stream. Parts of the ground are crusty, maybe due to cementing by a fluid containing minerals.There was evidence of clouds and maybe fog.\n\nThe2001 Mars Odysseyorbiter (2001-present) found much evidence for water on Mars in the form of images, and with itsneutron spectrometer, it proved that much of the ground is loaded with water ice. Mars has enough ice just beneath the surface to fillLake Michigantwice.In both hemispheres, from 55° latitude to the poles, Mars has a high density of ice just under the surface; one kilogram of soil contains about 500 grams (18 oz) of water ice. But close to the equator, there is only 2% to 10% of water in the soil.Scientists think that much of this water is also locked up in the chemical structure of minerals, such asclayandsulfates.Although the upper surface contains a few percent of chemically-bound water, ice lies just a few meters deeper, as it has been shown inArabia Terra,Amazonis quadrangle, andElysium quadranglethat contain large amounts of water ice.The orbiter also discovered vast deposits of bulk water ice near the surface of equatorial regions.Evidence for equatorial hydration is both morphological and compositional and is seen at both theMedusae Fossaeformation and theTharsis Montes.Analysis of the data suggests that the southern hemisphere may have a layered structure, suggestive of stratified deposits beneath a now extinct large water mass.\n\nThe instruments aboard theMars Odysseyare able to study the top meter of soil. In 2002, available data were used to calculate that if all soil surfaces were covered by an even layer of water, this would correspond to a global layer of water (GLW) 0.5–1.5 kilometres (0.31–0.93 mi).\n\nThousands of images returned fromOdysseyorbiter also support the idea that Mars once had great amounts of water flowing across its surface. Some images show patterns of branching valleys; others show layers that may have been formed under lakes; even river and lakedeltashave been identified.For many years researchers suspected that glaciers exist under a layer of insulating rocks.Lineated valley fillis one example of these rock-covered glaciers. They are found on the floors of some channels. Their surfaces have ridged and grooved materials that deflect around obstacles. Lineated floor deposits may be related tolobate debris aprons, which have been shown by orbiting radar to contain large amounts of ice.\n\nThePhoenixlander (2008) also confirmed the existence of large amounts of water ice in the northern region of Mars.This finding was predicted by previous orbital data and theory,and was measured from orbit by the Mars Odyssey instruments.On June 19, 2008, NASA announced that dice-sized clumps of bright material in the \"Dodo-Goldilocks\" trench, dug by the robotic arm, had vaporized over the course of four days, strongly indicating that the bright clumps were composed of water ice thatsublimesfollowing exposure. Recent radiative transfer modeling has shown that this water ice was snow with a grain size of ~350 μm with 0.015% dust.Even though CO2(dry ice) also sublimes under the conditions present, it would do so at a rate much faster than observed.On July 31, 2008, NASA announced thatPhoenixfurther confirmed the presence of water ice at its landing site. During the initial heating cycle of a sample, the mass spectrometer detected water vapor when the sample temperature reached 0 °C (32 °F; 273 K).Stable liquid water cannot exist on the surface of Mars with its present low atmospheric pressure and temperature (it would boil), except at the lowest elevations for short periods.\n\nThe presence of theperchlorate(ClO4–) anion, a strongoxidizer, in the martian soil was confirmed. This salt can considerably lower the waterfreezing point.\n\nWhenPhoenixlanded, theretrorocketssplashed soil and melted ice onto the vehicle.Photographs showed the landing had left blobs of material stuck to the landing struts.The blobs expanded at a rate consistent withdeliquescence, darkened before disappearing (consistent withliquefactionfollowed by dripping), and appeared to merge. These observations, combined withthermodynamicevidence, indicated that the blobs were likely liquidbrinedroplets.Other researchers suggested the blobs could be \"clumps of frost.\"In 2015 it was confirmed that perchlorate plays a role in formingrecurring slope lineaeon steepgullies.\n\nFor about as far as the camera can see, the landing site is flat, but shaped into polygons between 2–3 metres (6 ft 7 in – 9 ft 10 in) in diameter which are bounded by troughs that are 20–50 centimetres (7.9–19.7 in) deep. These shapes are due to ice in the soil expanding and contracting due to major temperature changes. The microscope showed that the soil on top of the polygons is composed of rounded particles and flat particles, probably a type of clay.Ice is present a few inches below the surface in the middle of the polygons, and along its edges, the ice is at least 8 inches (200 mm) deep.\n\nSnow was observed to fall from cirrus clouds. The clouds formed at a level in the atmosphere that was around −65 °C (−85 °F; 208 K), so the clouds would have to be composed of water-ice, rather than carbon dioxide-ice (CO2or dry ice), because the temperature for forming carbon dioxide ice is much lower than −120 °C (−184 °F; 153 K). As a result of mission observations, it is now suspected that water ice (snow) would have accumulated later in the year at this location.The highest temperature measured during the mission, which took place during the Martian summer, was −19.6 °C (−3.3 °F; 253.6 K), while the coldest was −97.7 °C (−143.9 °F; 175.5 K). So, in this region the temperature remained far below the freezing point (0 °C (32 °F; 273 K)) of water.\n\nTheMars Exploration Rovers,Spirit(2004-2010) andOpportunity(2004-2018) found a great deal of evidence for past water on Mars. TheSpirit roverlanded in what was thought to be a large lake bed. The lake bed had been covered over with lava flows, so evidence of past water was initially hard to detect. On March 5, 2004, NASA announced thatSpirithad found hints of water history on Mars in a rock dubbed \"Humphrey\".\n\nAsSpirittraveled in reverse in December 2007, pulling a seized wheel behind, the wheel scraped off the upper layer of soil, uncovering a patch of white ground rich insilica. Scientists think that it must have been produced in one of two ways.One:hot springdeposits produced when water dissolved silica at one location and then carried it to another (i.e. ageyser). Two: acidic steam rising through cracks in rocks stripped them of their mineral components, leaving silica behind.TheSpiritrover also found evidence for water in the Columbia Hills of Gusev crater. In the Clovis group of rocks theMössbauer spectrometer(MB) detectedgoethite,that forms only in the presence of water,iron in the oxidized form Fe3+,carbonate-rich rocks, which means that regions of the planet once harbored water.\n\nTheOpportunityroverwas directed to a site that had displayed large amounts ofhematitefrom orbit. Hematite often forms from water. The rover indeed found layered rocks and marble- or blueberry-like hematiteconcretions. Elsewhere on its traverse,Opportunityinvestigated aeolian dunestratigraphyin Burns Cliff inEndurance Crater. Its operators concluded that the preservation and cementation of these outcrops had been controlled by flow of shallow groundwater.In its years of continuous operation,Opportunitysent back evidence that this area on Mars was soaked in liquid water in the past.\n\nThe MER rovers found evidence for ancient wet environments that were very acidic. In fact, whatOpportunityfound evidence ofsulfuric acid, a harsh chemical for life.But on May 17, 2013, NASA announced thatOpportunityfoundclaydeposits that typically form in wet environments that are near neutralacidity. This find provides additional evidence about a wet ancient environment possibly favorable forlife.\n\nOn January 24, 2014, researchers reported thatcurrent studieson Mars by theCuriosityandOpportunityroversfound evidence of ancient environments which could have beenhabitablebychemo-litho-autotrophicmicroorganisms, as well as ancient water, includingfluvio-lacustrine environments(plainsrelated to ancient rivers or lakes).\n\nTheMars Reconnaissance Orbiter'sHiRISEinstrument (2006-present) has taken many images that strongly suggest that Mars has had a rich history of water-related processes. A major discovery was finding evidence of ancienthot springs. If they have hosted microbial life, they may containbiosignatures.Research published in January 2010, described strong evidence for sustained precipitation in the area aroundValles Marineris.The types of minerals there are associated with water. Also, the high density of small branching channels indicates a great deal of precipitation.\n\nRocks on Mars have been found to frequently occur as layers, called strata, in many different places.Layers form by various ways, including volcanoes, wind, or water.Light-toned rocks on Mars have been associated withhydrated mineralslike sulfates and clay.\n\nThe orbiter helped scientists determine that much of the surface of Mars is covered by a thick smooth mantle that is thought to be a mixture of ice and dust.\n\nThe ice mantle under the shallow subsurface is thought to result from frequent, major climate changes. Changes in Mars' orbit and tilt cause significant changes in the distribution of water ice from polar regions down to latitudes equivalent to Texas. During certain climate periods water vapor leaves polar ice and enters the atmosphere. The water returns to the ground at lower latitudes as deposits of frost or snow mixed generously with dust. The atmosphere of Mars contains a great deal of fine dust particles.Water vapor condenses on the particles, then they fall down to the ground due to the additional weight of the water coating. When ice at the top of the mantling layer goes back into the atmosphere, it leaves behind dust, which insulates the remaining ice.\n\nIn 2008, research with the Shallow Radar on the Mars Reconnaissance Orbiter provided strong evidence that thelobate debris aprons(LDA) inHellas Planitiaand in mid northern latitudes areglaciersthat are covered with a thin layer of rocks. Its radar also detected a strong reflection from the top and base of LDAs, meaning that pure water ice made up the bulk of the formation.The discovery of water ice in LDAs demonstrates that water is found at even lower latitudes.\n\nResearch published in September 2009, demonstrated that some new craters on Mars show exposed, pure water ice.After a time, the ice disappears, evaporating into the atmosphere. The ice is only a few feet deep. The ice was confirmed with the Compact Imaging Spectrometer (CRISM) on board the Mars Reconnaissance Orbiter.Similar exposures of ice have been detected within the mid-latitude mantle (originally proposed to contain buried dusty snow covered with dust and regolith;) that drapes most pole-facing slopes in the mid-latitudes using spectral analysis of HiRISE images.\n\nAdditional collaborating reports published in 2019 evaluated the amount of water ice located at the northern pole. One report used data from the MRO'sSHARAD(SHAllow RADar sounder) probes. SHARAD has the capability scanning up to about 2 kilometres (1.2 miles) below the surface at 15 metres (49 ft) intervals. The analysis of past SHARAD runs showed evidence of strata of water ice and sand below thePlanum Boreum, with as much as 60% to 88% of the volume being water ice. This supports the theory of the long-term global weather of Mars consisting of cycles of global warming and cooling; during cooling periods, water gathered at the poles to form the ice layers, and then as global warming occurred, the unthawed water ice was covered by dust and dirt from Mars' frequent dust storms. The total ice volume determine by this study indicated that there was approximately 2.2×105cubic kilometres (5.3×104cu mi), or enough water, if melted, to fully cover the Mars surface with a 1.5 metres (4.9 ft) layer of water.The work was corroborated by a separate study that used recorded gravity data to estimate the density of the Planum Boreum, indicating that on average, it contained up to 55% by volume of water ice.\n\nMany features that look like thepingoson the Earth were found in Utopia Planitia (~35-50° N; ~80-115° E) by examining photos from HiRISE. Pingos contain a core of ice.\n\nEarly in its mission,NASA'sCuriosityrover (2012-present) discovered unambiguousfluvialsediments on Mars. The properties of the pebbles in these outcrops suggested former vigorous flow on a streambed, with flow between ankle- and waist-deep. These rocks were found at the foot of analluvial fansystem descending from the crater wall, which had previously been identified from orbit.\n\nIn October 2012, the firstX-ray diffraction analysisof aMartian soilwas performed byCuriosity. The results revealed the presence of several minerals, includingfeldspar,pyroxenesandolivine, and suggested that the Martian soil in the sample was similar to the weathered basaltic soils ofHawaiian volcanoes. The sample used is composed of dust distributed fromglobal dust stormsand local fine sand. So far, the materialsCuriosityhas analyzed are consistent with the initial ideas of deposits inGale Craterrecording a transition through time from a wet to dry environment.\n\nIn December 2012, NASA reported thatCuriosityperformed its first extensivesoil analysis, revealing the presence of water molecules,sulfurandchlorinein theMartian soil.And in March 2013, NASA reported evidence ofmineral hydration, likely hydratedcalcium sulfate, in severalrock samplesincluding the broken fragments of\"Tintina\" rockand\"Sutton Inlier\" rockas well as inveinsandnodulesin other rocks like\"Knorr\" rockand\"Wernicke\" rock.Analysis using the rover'sDAN instrumentprovided evidence of subsurface water, amounting to as much as 4% water content, down to a depth of 60 cm (2.0 ft), in the rover's traverse from theBradbury Landingsite to theYellowknife Bayarea in theGlenelgterrain.\n\nOn September 26, 2013, NASA scientists reported theMarsCuriosityrover detected abundant chemically-bound water (1.5 to 3 weight percent) insoil samplesat theRocknest regionofAeolis PalusinGale Craterusing mass spectrometry.One of the study's authors stated that this was equivalent to about 2 pints (1.1 liters) of water per cubic foot (28.3 liters) of soil.In addition, NASA reported the rover found two principal soil types: a fine-grainedmafic typeand a locally derived, coarse-grainedfelsic type.The mafic type, similar to othermartian soilsandmartian dust, was associated with hydration of the amorphous phases of the soil.Also,perchlorates, the presence of which may make detection of life-relatedorganic moleculesdifficult, were found at theCuriosityrover landing site (and earlier at the more polar site of thePhoenix lander) suggesting a \"global distribution of these salts\".NASA also reported thatJake M rock, a rock encountered byCuriosityon the way toGlenelg, was amugeariteand very similar to terrestrial mugearite rocks.\n\nOn December 9, 2013, NASA reported that Mars once had a largefreshwater lakeinsideGale Crater,that could have been a hospitable environment formicrobial life.\n\nOn January 24, 2014, researchers reported thatcurrent studieson Mars by theCuriosityandOpportunityroversfound evidence of ancient environments which could have beenhabitablebychemo-litho-autotrophicmicroorganisms, as well as ancient water, includingfluvio-lacustrine environments(plainsrelated to ancient rivers or lakes).\n\nOn December 16, 2014, NASA reported detecting an unusual increase, then decrease, in the amounts ofmethanein theatmospherenear the rover; also, based ondeuteriumtohydrogenratio studies, in 3 billion year old clays, much of the water atGale Crateron Mars was found to have been lost during ancient times, before the lake bed in the crater was formed; afterwards, large amounts of water continued to be lost.\n\nOn April 13, 2015,Naturepublished an analysis of humidity and ground temperature data collected byCuriosity, showing that ambient conditions could allow transient films of liquid brine water to form in the upper 5 cm of Mars's subsurface at night. Such a brine would not allow for reproduction ormetabolismof known terrestrial microorganisms.\n\nOn October 8, 2015, NASA confirmed that lakes and streams existed inGale crater3.3 – 3.8 billion years ago delivering sediments to build up the lower layers ofMount Sharp.\n\nOn November 4, 2018, geologists presented evidence, based on studies inGale Craterby theCuriosityrover, that there was abundant water on early Marsincluding large floods at Gale Crater.\n\nTheMars ExpressOrbiter(2004-present), launched by theEuropean Space Agency, has been mapping the surface of Mars and investigating the subsurface. Between 2012 and 2015, theOrbiterscanned the area beneath the ice caps on thePlanum Australeusing radar, finding a possiblesubglacial lakeabout 20 kilometres (12 mi) wide. The top of the potential lake would be located 1.5 kilometres (0.93 mi) under the glacier; however, this interpretation iscontroversial.\n\nChina'sZhurong rover(2021-2022) touched down on Mars in Utopia Planitia on May 14, 2021. Its six scientific instruments included two panoramic cameras, a ground-penetrating radar and a magnetic field detector. Zhurong used a laser to zap rocks to study their compositions.\n\nZhurong found evidence of water when it examined the crust at the surface, called \"duricrust.\" The crust\ncontained hydrated sulfate/silica materials in the Amazonian-age terrain of the landing site. The duricrust may have been produced either by subsurface ice melting or groundwater rising.\n\nLooking at the dunes at Zhurong's landing site, researchers found a large shift in wind direction (as evidenced in the dune directions) that occurred about the same time that layers in the Martian northern ice caps changed. It was suggested that these events happened when the rotational tilt of the planet changed.\n\nIn 2024, researchers published data recorded by NASA'sInSightlander (2018-2022) which suggested the presence ofgroundwater on Mars. The data consisted of measurements ofseismic wavesfromMarsquakesmade by InSight'sseismometer. At the area it was measuring, it is estimated that there is water 7 to 13 miles beneath thesurface of Mars. It is estimated that if the small area observed by InSight is representative of all other areas of Mars, the volume of groundwater on Mars would be enough to cover all of Mars' surface with a layer of water between 0.62 and 1.24 miles deep.",
      "sections": [
        {
          "level": 2,
          "heading": "Historical background"
        },
        {
          "level": 2,
          "heading": "Aqueous and hydrated minerals"
        },
        {
          "level": 3,
          "heading": "Water in weathering products (aqueous minerals)"
        },
        {
          "level": 3,
          "heading": "Hydrothermal alteration"
        },
        {
          "level": 3,
          "heading": "Weathering rates"
        },
        {
          "level": 3,
          "heading": "Martian meteorites"
        },
        {
          "level": 2,
          "heading": "Geomorphic evidence for ancient liquid water"
        },
        {
          "level": 3,
          "heading": "Lakes and river valleys"
        },
        {
          "level": 3,
          "heading": "Lake deltas"
        },
        {
          "level": 3,
          "heading": "Groundwater"
        },
        {
          "level": 3,
          "heading": "Mars ocean hypothesis"
        },
        {
          "level": 2,
          "heading": "Present water"
        },
        {
          "level": 3,
          "heading": "Polar ice caps"
        },
        {
          "level": 3,
          "heading": "Ground ice and subsurface ice"
        },
        {
          "level": 3,
          "heading": "Glaciers"
        },
        {
          "level": 3,
          "heading": "Groundwater and crust"
        },
        {
          "level": 3,
          "heading": "Evidence for recent flows"
        },
        {
          "level": 3,
          "heading": "Precipitation"
        },
        {
          "level": 2,
          "heading": "Development of Mars' water inventory"
        },
        {
          "level": 3,
          "heading": "Early Noachian era (4.6 Ga to 4.1 Ga)"
        },
        {
          "level": 3,
          "heading": "Middle to late Noachian era (4.1 Ga to 3.8 Ga)"
        },
        {
          "level": 3,
          "heading": "Hesperian to Amazonian era (present) (~3.8 Ga to present)"
        },
        {
          "level": 2,
          "heading": "Habitability assessments"
        },
        {
          "level": 2,
          "heading": "Findings by probes"
        },
        {
          "level": 3,
          "heading": "Mariner 9"
        },
        {
          "level": 3,
          "heading": "Viking program"
        },
        {
          "level": 3,
          "heading": "Mars Global Surveyor"
        },
        {
          "level": 3,
          "heading": "Mars Pathfinder"
        },
        {
          "level": 3,
          "heading": "Mars Odyssey"
        },
        {
          "level": 3,
          "heading": "Phoenix"
        },
        {
          "level": 3,
          "heading": "SpiritandOpportunityRovers"
        },
        {
          "level": 3,
          "heading": "Mars Reconnaissance Orbiter"
        },
        {
          "level": 3,
          "heading": "Curiosityrover"
        },
        {
          "level": 3,
          "heading": "Mars Express"
        },
        {
          "level": 3,
          "heading": "Zhurong Rover"
        },
        {
          "level": 3,
          "heading": "InSight"
        }
      ],
      "raw_content_length": 1600366,
      "cleaned_content_length": 87053,
      "scraped_at": "2025-09-02 15:30:48",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Water on Mars",
      "discoverers": [
        "William Herschel",
        "Gerard Kuiper",
        "Robert Leighton",
        "Bruce Murray",
        "Walter Adams",
        "Theodore Dunham"
      ],
      "discovery_years": [
        "1963"
      ],
      "discovery_timeline": [
        "1784: William Herschel suggests Mars has water based on observations.",
        "1963: Water vapor on Mars is first unequivocally detected by spectroscopy.",
        "2001: Mars Odyssey launched to study Martian water.",
        "2008: Ice detected in soil samples by Phoenix lander.",
        "2013: Further ice detection by Curiosity rover.",
        "2018: Radar findings suggest liquid water in subglacial lakes.",
        "2024: Seismometer data suggests liquid water deep under the surface."
      ],
      "mechanism": "Detection of water on Mars is achieved through spectroscopy, remote sensing techniques, and surface investigations by landers and rovers.",
      "key_features": [
        "Presence of large quantities of ice on and under the surface of Mars.",
        "Detection of liquid water in subglacial lakes and beneath the surface.",
        "Evidence of past water flow and potential for microbial life."
      ],
      "applications": [
        "Astrobiology",
        "Planetary exploration",
        "Resource utilization for future human missions to Mars."
      ],
      "significance": "Understanding the extent and situation of water on Mars is crucial for assessing the planet's potential for harboring life and for future human exploration.",
      "institutions": [
        "NASA",
        "ESA"
      ],
      "awards": null,
      "extracted_at": "2023-10-01"
    }
  },
  {
    "article_info": {
      "url": "https://en.wikipedia.org/wiki/Penicillin",
      "title": "Penicillin",
      "main_content": "Penicillins(P,PCNorPEN) are a group ofβ-lactam antibioticsoriginally obtained fromPenicilliummoulds, principallyP. chrysogenumandP. rubens. Most penicillins in clinical use are synthesised byP. chrysogenumusingdeep tank fermentationand then purified.A number of natural penicillins have been discovered, but only two purified compounds are in clinical use:penicillin G(intramuscularorintravenous use) andpenicillin V(given by mouth). Penicillins were among the first medications to be effective against manybacterial infectionscaused bystaphylococciandstreptococci. They are still widely used today for various bacterial infections, though many types ofbacteriahave developedresistancefollowing extensive use.\n\nIn the United States, 10% of the population claimspenicillin allergies, but because the frequency of positive skin test results decreases by 10% with each year of avoidance, 90% of these patients can eventually tolerate penicillin. Additionally, those with penicillin allergies can usually toleratecephalosporins(another group of β-lactam) because theimmunoglobulin E (IgE)cross-reactivity is only 3%.\n\nPenicillin was discovered in 1928 by the Scottish physicianAlexander Flemingas a crude extract ofP. rubens.Fleming's student Cecil George Paine was the first to successfully use penicillin to treat eye infection (neonatal conjunctivitis) in 1930. The purified compound (penicillin F) was isolated in 1940 by a research team led byHoward FloreyandErnst Boris Chainat theUniversity of Oxford. Fleming first used the purified penicillin to treat streptococcalmeningitisin 1942.The 1945Nobel Prize in Physiology or Medicinewas shared by Chain, Fleming and Florey.\n\nSeveral semisynthetic penicillins are effective against a broader spectrum of bacteria: these include theantistaphylococcal penicillins,aminopenicillins, andantipseudomonal penicillins.\n\nThe term \"penicillin\" is defined as the natural product ofPenicilliummould with antimicrobial activity.It was coined byAlexander Flemingon 7 March 1929 when he discovered the antibacterial property ofPenicillium rubens.Fleming explained in his 1929 paper in theBritish Journal of Experimental Pathologythat \"to avoid the repetition of the rather cumbersome phrase 'Mould broth filtrate', the name 'penicillin' will be used.\"The name thus refers to the scientific name of the mould, as described by Fleming in his Nobel lecture in 1945:\n\nI have been frequently asked why I invented the name \"Penicillin\". I simply followed perfectly orthodox lines and coined a word which explained that the substance penicillin was derived from a plant of the genus Penicillium just as many years ago the word \"Digitalin\" was invented for a substance derived from the plantDigitalis.\n\nIn modern usagepenicillinis used more broadly to refer to anyβ-lactamantimicrobial that contains athiazolidinering fused to the β-lactam core and may or may not be a natural product.Like most natural products, penicillin is present inPenicilliummoulds as a mixture of active constituents (gentamicinis another example of a natural product that is an ill-defined mixture of active components).The principal active components ofPenicilliumare listed in the following table:\n\nOther minor active components ofPenicilliumincludepenicillin O,penicillin U1, and penicillin U6. Other named constituents of naturalPenicillium, such as penicillin A, were subsequently found not to have antibiotic activity and are not chemically related to antibiotic penicillins.\n\nThe precise constitution of the penicillin extracted depends on the species ofPenicilliummould used and on the nutrient media used to culture the mould.Fleming's original strain ofPenicillium rubensproduces principally penicillin F, named after Fleming. But penicillin F is unstable, difficult to isolate, and produced by the mould in small quantities.\n\nThe principal commercial strain ofPenicillium chrysogenum(the Peoria strain) producespenicillin Gas the principal component when corn steep liquor is used as the culture medium.Whenphenoxyethanolor phenoxyacetic acid are added to the culture medium, the mould producespenicillin Vas the main penicillin instead.\n\n6-Aminopenicillanic acid(6-APA) is a compound derived from penicillin G. 6-APA contains the beta-lactam core of penicillin G, but with the side chains stripped off; 6-APA is a useful precursor for manufacturing other penicillins. There are many semi-synthetic penicillins derived from 6-APA and these are in three groups: antistaphylococcal penicillins, broad-spectrum penicillins, and antipseudomonal penicillins. The semi-synthetic penicillins are all referred to as penicillins because they are all derived ultimately from penicillin G.\n\nThe use of units to prescribe penicillin is largely obsolete outside of the US. Since the original penicillin was an ill-defined mixture of active compounds (an amorphous yellow powder), the potency of penicillin varied from batch to batch. It was therefore impractical to prescribe 1 g of penicillin because the activity of 1 g of penicillin from one batch would be different from the activity from another batch. To address this problem, after manufacture, each batch of penicillin was standardised against a known unit of penicillin: each glass vial was then filled with the number of units required. In the 1940s, a vial of 5,000 Oxford units was standard,but the depending on the batch, could contain anything from 15 mg to 20 mg of penicillin. Later, a vial of 1,000,000 international units became standard, and this could contain 2.5 g to 3 g of natural penicillin (a mixture of penicillin I, II, III, and IV and natural impurities). With the advent of pure penicillin G preparations (a white crystalline powder), there is little reason to prescribe penicillin in units, although units are still used forbenzathine benzylpenicillinin the United States.\n\nThe \"unit\" of penicillin has had three previous definitions, and each definition was chosen as being roughly equivalent to the previous one.\n\nThere is an older unit for penicillin V that is not equivalent to the current penicillin V unit. The reason is that the US FDA incorrectly assumed that the potency of penicillin V is the same mole-for-mole as penicillin G. In fact, penicillin V is less potent than penicillin G, and the current penicillin V unit reflects that fact.\n\nA similar standard was also established for penicillin K.\n\nPenicillins consist of a distinct 4-memberedbeta-lactamring, in addition to a thiazolide ring and an R side chain. The main distinguishing feature between variants within this family is theR substituent.\n\nThis side chain is connected to the 6-aminopenicillanic acid residue and results invariationsin the antimicrobial spectrum, stability and susceptibility tobeta-lactamasesof each type.\n\nPenicillin G (benzylpenicillin) was first produced from apenicilliumfungus that occurs in nature. The strain of fungus used today for the manufacture of penicillin G was created bygenetic engineeringto improve the yield in the manufacturing process. None of the other natural penicillins (F, K, N, X, O, U1 or U6) are currently in clinical use.\n\nPenicillin V (phenoxymethylpenicillin) is produced by adding theprecursorphenoxyacetic acid to the medium in which a genetically modified strain[dubious–discuss]of thepenicilliumfungus is being cultured.\n\nThere are three major groups of othersemi-syntheticantibioticsrelated to the penicillins. They are synthesised by adding various side-chains to theprecursor6-APA, which is isolated from penicillin G. These are the antistaphylococcal antibiotics, broad-spectrum antibiotics and antipseudomonal antibiotics.\n\nAntistaphylococcal antibiotics are so-called because they are resistant to being broken down by staphylococcalpenicillinase. They are also, therefore, referred to as being penicillinase-resistant.\n\nThis group of antibiotics is called \"broad-spectrum\" because they are active against a wide range of Gram-negative bacteria such asEscherichia coliandSalmonella typhi, for which penicillin is not suitable. However, resistance in these organisms is now common.\n\nThere are many ampicillin precursors in existence. These are inactive compounds that are broken down in the gut to release ampicillin. None of these pro-drugs of ampicillin is in current use:\n\nEpicillinis an aminopenicillin that has never seen widespread clinical use.\n\nThe Gram-negative species,Pseudomonas aeruginosa,is naturally resistant to many antibiotic classes. There were many efforts in the 1960s and 1970s to develop antibiotics that are active againstPseudomonasspecies. There are two chemical classes within the group: carboxypenicillins and ureidopenicillins. All are given by injection: none can be given by mouth.\n\nThe term \"penicillin\", when used by itself, may refer to either of twochemical compounds, penicillin G or penicillin V.\n\nPenicillin G is destroyed by stomach acid, so it cannot be taken by mouth, but doses as high as 2.4 g can be given (much higher than penicillin V). It is given by intravenous or intramuscular injection. It can be formulated as an insoluble salt, and there are two such formulations in current use:procaine penicillinandbenzathine benzylpenicillin. When a high concentration in the blood must be maintained, penicillin G must be administered at relatively frequent intervals, because it is eliminated quite rapidly from the bloodstream by the kidney.\n\nPenicillin G is licensed for use to treatsepticaemia,empyema,pneumonia,pericarditis,endocarditisandmeningitiscaused by susceptible strains of staphylococci and streptococci. It is also licensed for the treatment ofanthrax,actinomycosis, cervicofacial disease, thoracic and abdominal disease,clostridial infections,botulism,gas gangrene(with accompanying debridement and/or surgery as indicated),tetanus(as an adjunctive therapy to human tetanus immune globulin),diphtheria(as an adjunctive therapy to antitoxin and for the prevention of the carrier state),erysipelothrixendocarditis,fusospirochetosis(severe infections of the oropharynx, lower respiratory tract and genital area),Listeriainfections, meningitis, endocarditis,Pasteurellainfections including bacteraemia and meningitis,Haverhill fever;rat-bite feveranddisseminated gonococcal infections,meningococcalmeningitis and/or septicaemia caused by penicillin-susceptible organisms and syphilis.\n\nPenicillin V can be taken by mouth because it is relatively resistant to stomach acid. Doses higher than 500 mg are not fully effective because of poor absorption. It is used for the same bacterial infections as those of penicillin G and is the most widely used form of penicillin.However, it is not used for diseases, such asendocarditis, where high blood levels of penicillin are required.\n\nBecause penicillin resistance is now so common, other antibiotics are now the preferred choice for treatments. For example, penicillin used to be the first-line treatment for infections withNeisseria gonorrhoeaeandNeisseria meningitidis, but it is no longer recommended for treatment of these infections. Penicillin resistance is now very common inStaphylococcus aureus, which means penicillin should not be used to treat infections caused byS. aureusinfection unless the infecting strain is known to be susceptible.\n\nCommon (≥ 1% of people)adverse drug reactionsassociated with use of the penicillins includediarrhoea,hypersensitivity,nausea,rash,neurotoxicity,urticaria, andsuperinfection(includingcandidiasis). Infrequent adverse effects (0.1–1% of people) includefever,vomiting,erythema,dermatitis,angioedema,seizures(especially in people withepilepsy), andpseudomembranous colitis.Penicillin can also induceserum sicknessor aserum sickness-like reactionin some individuals. Serum sickness is atype III hypersensitivityreaction that occurs one to three weeks after exposure to drugs including penicillin. It is not a true drug allergy, because allergies aretype I hypersensitivityreactions, but repeated exposure to the offending agent can result in an anaphylactic reaction.Allergy will occur in 1–10% of people, presenting as a skin rash after exposure. IgE-mediatedanaphylaxiswill occur in approximately 0.01% of patients.\n\nPain and inflammation at the injection site are also common forparenterallyadministered benzathine benzylpenicillin, benzylpenicillin, and, to a lesser extent, procaine benzylpenicillin. The condition is known aslivedoid dermatitisor Nicolau syndrome.\n\nThe term \"penam\" is used to describe the common core skeleton of a member of the penicillins. This core has the molecular formula R-C9H11N2O4S, where R is the variable side chain that differentiates the penicillins from one another. The penam core has amolar massof 243 g/mol, with larger penicillins having molar mass near 450—for example, cloxacillin has a molar mass of 436 g/mol. 6-APA (C8H12N2O3S) forms the basic structure of penicillins. It is made up of an enclosed dipeptide formed by the condensation ofL-cysteineandD-valine. This results in the formations of β-lactam and thiazolidinic rings.\n\nThe key structural feature of the penicillins is the four-membered β-lactam ring; this structuralmoietyis essential for penicillin's antibacterial activity. The β-lactam ring is itself fused to a five-memberedthiazolidinering. The fusion of these two rings causes the β-lactam ring to be more reactive than monocyclic β-lactams because the two fused rings distort the β-lactamamide bondand therefore remove theresonance stabilisationnormally found in these chemical bonds.An acyl side side chain attached to the β-lactam ring.\n\nA variety of β-lactam antibiotics have been produced following chemical modification from the 6-APA structure during synthesis, specifically by making chemical substitutions in the acyl side chain. For example, the first chemically altered penicillin, methicillin, had substitutions by methoxy groups at positions 2’ and 6’ of the 6-APA benzene ring from penicillin G.This difference makes methicillin resistant to the activity ofβ-lactamase, an enzyme by which many bacteria are naturally unsusceptible to penicillins.\n\nPenicillin can easily enter bacterial cells in the case ofGram-positive species. This is because Gram-positive bacteria do not have an outer cell membrane and are simply enclosed in a thickcell wall.Penicillin molecules are small enough to pass through the spaces ofglycoproteinsin the cell wall. For this reason Gram-positive bacteria are very susceptible to penicillin (as first evidenced by the discovery of penicillin in 1928).\n\nPenicillin, or any other molecule, entersGram-negative bacteriain a different manner. The bacteria have thinner cell walls but the external surface is coated with an additional cell membrane, called the outer membrane. The outer membrane is a lipid layer (lipopolysaccharidechain) that blocks passage of water-soluble (hydrophilic) molecules like penicillin. It thus acts as the first line of defence against any toxic substance, which is the reason for relative resistance to antibiotics compared to Gram-positive species.But penicillin can still enter Gram-negative species by diffusing through aqueous channels calledporins(outer membrane proteins), which are dispersed among the fatty molecules and can transport nutrients and antibiotics into the bacteria.Porins are large enough to allow diffusion of most penicillins, but the rate of diffusion through them is determined by the specific size of the drug molecules. For instance, penicillin G is large and enters through porins slowly; while smaller ampicillin and amoxicillin diffuse much faster.In contrast, large vancomycin can not pass through porins and is thus ineffective for Gram-negative bacteria.The size and number of porins are different in different bacteria. As a result of the two factors—size of penicillin and porin—Gram-negative bacteria can be unsusceptible or have varying degree of susceptibility to specific penicillin.\n\nPenicillin kills bacteria by inhibiting the completion of the synthesis ofpeptidoglycans, the structural component of thebacterial cell wall. It specifically inhibits the activity of enzymes that are needed for the cross-linking of peptidoglycans during the final step in cell wall biosynthesis. It does this by binding topenicillin binding proteinswith the β-lactam ring, a structure found on penicillin molecules.This causes the cell wall to weaken due to fewer cross-links and means water uncontrollably flows into the cell because it cannot maintain the correct osmotic gradient. This results in celllysisand death.\n\nBacteria constantly remodel their peptidoglycan cell walls, simultaneously building and breaking down portions of the cell wall as they grow and divide. During the last stages of peptidoglycan biosynthesis, uridine diphosphate-N-acetylmuramic acid pentapeptide (UDP-MurNAc) is formed in which the fourth and fifth amino acids are bothD-alanyl-D-alanine. The transfer ofD-alanine is done (catalysed) by theenzymeDD-transpeptidase(penicillin-binding proteinsare such type).The structural integrity of bacterial cell wall depends on thecross linkingof UDP-MurNAc andN-acetyl glucosamine.Penicillin and other β-lactam antibiotics act as an analogue ofD-alanine-D-alanine (the dipeptide) in UDP-MurNAc owing to conformational similarities. TheDD-transpeptidase then binds the four-membered β-lactamringof penicillin instead of UDP-MurNAc.As a consequence,DD-transpeptidase is inactivated, the formation of cross-links between UDP-MurNAc andN-acetyl glucosamine is blocked so that an imbalance between cell wall production and degradation develops, causing the cell to rapidly die.\n\nThe enzymes thathydrolysethe peptidoglycan cross-links continue to function, even while those that form such cross-links do not. This weakens the cell wall of the bacterium, and osmotic pressure becomes increasingly uncompensated—eventually causing cell death (cytolysis). In addition, the build-up of peptidoglycan precursors triggers the activation of bacterial cell wall hydrolases and autolysins, which further digest the cell wall's peptidoglycans. The small size of the penicillins increases their potency, by allowing them to penetrate the entire depth of the cell wall. This is in contrast to theglycopeptide antibioticsvancomycinandteicoplanin, which are both much larger than the penicillins.\n\nGram-positive bacteria are calledprotoplastswhen they lose their cell walls.Gram-negativebacteria do not lose their cell walls completely and are calledspheroplastsafter treatment with penicillin.\n\nPenicillin shows a synergistic effect withaminoglycosides, since the inhibition of peptidoglycan synthesis allows aminoglycosides to penetrate the bacterial cell wall more easily, allowing their disruption of bacterial protein synthesis within the cell. This results in a loweredMBCfor susceptible organisms.\n\nPenicillins, like otherβ-lactam antibiotics, block not only the division of bacteria, includingcyanobacteria, but also the division of cyanelles, thephotosyntheticorganellesof theglaucophytes, and the division ofchloroplastsofbryophytes. In contrast, they have no effect on theplastidsof the highly developedvascular plants. This supports theendosymbiotic theoryof theevolutionof plastid division in land plants.\n\nSome bacteria produce enzymes that break down the β-lactam ring, calledβ-lactamases, which make the bacteria resistant to penicillin. Therefore, some penicillins are modified or given with other drugs for use against antibiotic-resistant bacteria or in immunocompromised patients. The use of clavulanic acid or tazobactam, β-lactamase inhibitors, alongside penicillin gives penicillin activity against β-lactamase-producing bacteria. β-Lactamase inhibitors irreversibly bind to β-lactamase preventing it from breaking down the beta-lactam rings on the antibiotic molecule. Alternatively, flucloxacillin is a modified penicillin that has activity against β-lactamase-producing bacteria due to an acyl side chain that protects the beta-lactam ring from β-lactamase.\n\nPenicillin has low protein binding in plasma. Thebioavailabilityof penicillin depends on the type: penicillin G has low bioavailability, below 30%, whereas penicillin V has higher bioavailability, between 60 and 70%.\n\nPenicillin has a short half-life and is excreted via the kidneys.This means it must be dosed at least four times a day to maintain adequate levels of penicillin in the blood. Early manuals on the use of penicillin, therefore, recommended injections of penicillin as frequently as every three hours, and dosing penicillin has been described as being similar to trying to fill a bath with the plug out.This is no longer required since much larger doses of penicillin are cheaply and easily available; however, some authorities recommend the use of continuous penicillin infusions for this reason.\n\nWhen Alexander Fleming discovered the crude penicillin in 1928, one important observation he made was that many bacteria were not affected by penicillin.This phenomenon was realised byErnst ChainandEdward Abrahamwhile trying to identify the exact of penicillin. In 1940 they discovered that unsusceptible bacteria likeEscherichia coliproduced specific enzymes that can break down penicillin molecules, thus making them resistant to the antibiotic. They named the enzymepenicillinase.Penicillinase is now classified as member of enzymes called β-lactamases. These β-lactamases are naturally present in many other bacteria, and many bacteria produce them upon constant exposure to antibiotics. In most bacteria, resistance can be through three different mechanisms – reduced permeability in bacteria, reduced binding affinity of the penicillin-binding proteins (PBPs) or destruction of the antibiotic through the expression of β-lactamase.Using any of these, bacteria commonly develop resistance to different antibiotics, a phenomenon calledmulti-drug resistance.\n\nThe actual process of resistance mechanism can be very complex. In case of reduced permeability in bacteria, the mechanisms are different between Gram-positive and Gram-negative bacteria. In Gram-positive bacteria, blockage of penicillin is due to changes in the cell wall. For example, resistance to vancomycin inS. aureusis due to additional peptidoglycan synthesis that makes the cell wall much thicker preventing effective penicillin entry.Resistance in Gram-negative bacteria is due to mutational variations in the structure and number of porins.In bacteria likePseudomonas aeruginosa, there is reduced number of porins; whereas in bacteria likeEnterobacterspecies,EscherichiacoliandKlebsiella pneumoniae, there are modified porins such as non-specific porins (such as OmpC and OmpF groups) that cannot transport penicillin.\n\nResistance due to PBP alterations is highly varied. A common case is found inStreptococcus pneumoniaewhere there is mutation in the gene for PBP, and the mutant PBPs have decreased binding affinity for penicillins.There are six mutant PBPs inS. pneumoniae, of which PBP1a, PBP2b, PBP2x and sometimes PBP2a are responsible for reduced binding affinity.S. aureuscan activate a hidden gene that produces a different PBP, PBD2, which has low binding affinity for penicillins.There is a different strain ofS. aureusnamedmethicillin-resistantS. aureus(MRSA) which is resistant not only to penicillin and other β-lactams, but also to most antibiotics. The bacterial strain developed after introduction of methicillin in 1959.In MRSA, mutations in the genes (mecsystem) for PBP produce a variant protein called PBP2a (also termed PBP2'),while making four normal PBPs. PBP2a has poor binding affinity for penicillin and also lacks glycosyltransferase activity required for complete peptidoglycan synthesis (which is carried out by the four normal PBPs).InHelicobacter cinaedi, there are multiple mutations in different genes that make PBP variants.\n\nEnzymatic destruction by β-lactamases is the most important mechanism of penicillin resistance,and is described as \"the greatest threat to the usage [of penicillins]\".It was the first discovered mechanism of penicillin resistance. During the experiments when purification and biological activity tests of penicillin were performed in 1940, it was found thatE. coliwas unsusceptible.The reason was discovered as production of an enzyme penicillinase (hence, the first β-lactamase known) inE. colithat easily degraded penicillin.There are over 2,000 types of β-lactamases each of which has unique amino acid sequence, and thus, enzymatic activity.All of them are able to hydrolyse β-lactam rings but their exact target sites are different.They are secreted on the bacterial surface in large quantities in Gram-positive bacteria but less so in Gram-negative species. Therefore, in a mixed bacterial infection, the Gram-positive bacteria can protect the otherwise penicillin-susceptible Gram-negative cells.\n\nThere are unusual mechanisms inP. aeruginosa, in which there can be biofilm-mediated resistance and formation of multidrug-tolerantpersister cells.\n\nStarting in the late-19th century there had been reports of the antibacterial properties ofPenicilliummould, but scientists were unable to discern what process was causing the effect.The Scottish physicianAlexander FlemingatSt. Mary's Hospitalin London (now part ofImperial College) was the first to show thatPenicillium rubenshad antibacterial properties.On 3 September 1928 he observed by chance that fungal contamination of a bacterial culture (Staphylococcus aureus) appeared to kill the bacteria. He confirmed this observation with a new experiment on 28 September 1928.He published his experiment in 1929, and called the antibacterial substance (the fungal extract) penicillin.\n\nC. J. La Touche identified the fungus asPenicillium rubrum(later reclassified byCharles ThomasP. notatumandP. chrysogenum, but later corrected asP. rubens).Fleming expressed initial optimism that penicillin would be a useful antiseptic, because of its high potency and minimal toxicity in comparison to other antiseptics of the day, and noted its laboratory value in the isolation ofBacillus influenzae(now calledHaemophilus influenzae).\n\nFleming did not convince anyone that his discovery was important.This was largely because penicillin was so difficult to isolate that its development as a drug seemed impossible. It is speculated that had Fleming been more successful at making other scientists interested in his work, penicillin would possibly have been developed years earlier.\n\nThe importance of his work has been recognised by the placement of anInternational Historic Chemical Landmarkat the Alexander Fleming Laboratory Museum in London on 19 November 1999.\n\nIn 1930 Cecil George Paine, apathologistat theRoyal InfirmaryinSheffield, successfully treatedophthalmia neonatorum, a gonococcal infection in infants, with penicillin (fungal extract) on 25 November 1930.\n\nIn 1940 the Australian scientistHoward Florey(later Baron Florey) and a team of researchers (Ernst Chain,Edward Abraham,Arthur Duncan Gardner,Norman Heatley,Margaret Jennings,Jean Orr-Ewingand Arthur Gordon Sanders) at theSir William Dunn School of Pathologyof theUniversity of Oxfordmade progress in making concentrated penicillin from fungal culture broth that showed bothin vitroandin vivobactericidal action.In 1941 they treated a policeman,Albert Alexander, with a severe face infection; his condition improved, but then supplies of penicillin ran out and he died. Subsequently, several other patients were treated successfully.In December 1942, survivors of theCocoanut Grove fireinBoston, United States, were the first burn patients to be successfully treated with penicillin.\n\nThe first successful use of pure penicillin was in 1942 when Fleming cured Harry Lambert of an infection of the nervous system (streptococcalmeningitis) which would otherwise have been fatal. By that time the Oxford team could produce only a small amount. Florey willingly gave the only available sample to Fleming. Lambert showed improvement from the very next day of the treatment, and was completely cured within a week.Fleming published his clinical trial inThe Lancetin 1943.Following the medical breakthrough, the BritishWar Cabinetset up the Penicillin Committee on 5 April 1943 that led to projects formass production.\n\nAs the medical application was established, the Oxford team found that it was impossible to produce usable amounts in their laboratory.Failing to persuadeHis Majesty's Government, Florey and Heatley travelled to the US in June 1941 with their mould samples in order to interest theUS federal governmentfor large-scale production.They approached the Northern Regional Research Laboratory (NRRL, now theNational Center for Agricultural Utilization Research) of theUS Department of AgricultureatPeoria, Illinois, where facilities for large-scale fermentations were established.Mass culture of the mould and search for better moulds immediately followed.\n\nOn 14 March 1942 the first patient was treated for streptococcal sepsis with US-made penicillin produced byMerck & Co.Half of the total supply produced at the time was used on that one patient, Anne Miller.By June 1942, just enough US penicillin was available to treat ten patients.In July 1943 theWar Production Boarddrew up a plan for the mass distribution of penicillin stocks toAlliedtroops fighting in Europe.The results of fermentation research oncorn steep liquorat the NRRL allowed the United States to produce 2.3 million doses in time for theinvasion of Normandyin the spring of 1944. After a worldwide search in 1943, a mouldycantaloupein aPeoria, Illinoismarket was found to contain the best strain of mould for production using the corn steep liquor process.Six times as much penicillin could be produced compared to using Fleming's mould.Jasper H. Kane, a scientist atPfizer, suggested using a deep-tank fermentation method for producing large quantities of pharmaceutical-grade penicillin.Large-scale production resulted from the development of a deep-tank fermentation plant by the chemical engineerMargaret Hutchinson Rousseau.As a direct result of the war and the War Production Board, by June 1945 over 646 billion units per year were being produced.\n\nG. Raymond Rettewmade a significant contribution to the American war effort by his techniques to produce commercial quantities of penicillin, wherein he combined his knowledge of mushroom spawn with the function of the Sharples Cream Separator.By 1943 Rettew's lab was producing most of the world's penicillin. During theSecond World Warpenicillin made a major difference in the number of deaths and amputations caused by infected wounds amongst Allied forces, saving an estimated 12–15% of lives.Availability was severely limited, however, by the difficulty of manufacturing large quantities of penicillin and by the rapidrenal clearanceof the drug, necessitating frequent dosing. Methods for mass production of penicillin were patented byAndrew Jackson Moyerin 1945.Florey had not patented penicillin, having been advised by SirHenry Dalethat doing so would be unethical.\n\nPenicillin is actively excreted, and about 80% of a penicillin dose is cleared from the body within three to four hours of administration. Indeed, during the early penicillin era, the drug was so scarce and so highly valued that it became common to collect the urine from patients being treated, so that the penicillin in the urine could be isolated and reused.This was not a satisfactory solution, so researchers looked for a way to slow penicillin excretion. They hoped to find a molecule that could compete with penicillin for the organic acid transporter responsible for excretion, such that the transporter would preferentially excrete the competing molecule and the penicillin would be retained. Theuricosuricagentprobenecidproved to be suitable. When probenecid and penicillin are administered together, probenecid competitively inhibits the excretion of penicillin, increasing penicillin's concentration and prolonging its activity. Eventually, the advent of mass-production techniques and semi-synthetic penicillins resolved the supply issues, so this use of probenecid declined.Probenecid is still useful, however, for certain infections requiring particularly high concentrations of penicillins.\n\nAfter the Second World War Australia was the first country to make the drug available for civilian use. In the United States penicillin was made available to the general public on 15 March 1945.\n\nFleming, Florey and Chain shared the 1945 Nobel Prize in Physiology or Medicine for the development of penicillin.\n\nThechemical structureof penicillin was first proposed byEdward Abrahamin 1942and was later confirmed in 1945 usingX-ray crystallographybyDorothy Crowfoot Hodgkin, who was also working at Oxford.She later in 1964 received the Nobel Prize in Chemistry for this and other structure determinations.\n\nThe chemistJohn C. Sheehanat theMassachusetts Institute of Technology(MIT) completed the first chemicalsynthesisof penicillin in 1957.Sheehan had started his studies into penicillin synthesis in 1948, and during these investigations developed new methods for the synthesis ofpeptides, as well as newprotecting groups—groups that mask the reactivity of certain functional groups.Although the initial synthesis developed by Sheehan was not appropriate for mass production of penicillins, one of the intermediate compounds in Sheehan's synthesis was 6-aminopenicillanic acid (6-APA), the nucleus of penicillin.\n\n6-APA was discovered by researchers at the Beecham Research Laboratories (later theBeecham Group) in Surrey in 1957 (published in 1959).Attaching different groups to the 6-APA 'nucleus' of penicillin allowed the creation of new forms of penicillins which are more versatile and better in activity.\n\nThe narrow range of treatable diseases or \"spectrum of activity\" of the penicillins, along with the poor activity of the orally active phenoxymethylpenicillin, led to the search for derivatives of penicillin that could treat a wider range of infections. The isolation of 6-APA, the nucleus of penicillin, allowed for the preparation of semisynthetic penicillins, with various improvements over benzylpenicillin (bioavailability, spectrum, stability, tolerance).\n\nThe first major development was ampicillin in 1961. It offered a broader spectrum of activity than either of the original penicillins. Further development yielded β-lactamase-resistant penicillins, including flucloxacillin, dicloxacillin and methicillin. These were significant for their activity against β-lactamase-producing bacterial species, but were ineffective against themethicillin-resistantStaphylococcus aureus(MRSA) strains that subsequently emerged.\n\nAnother development of the line of true penicillins was the antipseudomonal penicillins, such as carbenicillin, ticarcillin, and piperacillin, useful for their activity against Gram-negative bacteria. However, the usefulness of the β-lactam ring was such that related antibiotics, including the mecillinams, the carbapenems, and, most importantly, the cephalosporins, still retain it at the center of their structures.\n\nPenicillin is produced by the fermentation of various types of sugar by the fungusPenicillium rubens.The fermentation process produces penicillin as asecondary metabolitewhen the growth of the fungus is inhibited by stress.The biosynthetic pathway outlined below experiencesfeedback inhibitioninvolving the by-productl-lysine inhibiting the enzymehomocitrate synthase.\n\nThePenicilliumcells are grown using a technique calledfed-batchculture, in which the cells are constantly subjected to stress, which is required for induction of penicillin production. While the usage ofglucoseas a carbon source represses penicillin biosynthesis enzymes,lactosedoes not exert any effect and alkalinepHlevels override this regulation. Excessphosphate, availableoxygen, and usage ofammoniumas anitrogensource repress penicillin production, whilemethioninecan act as a sole nitrogen/sulfur source with stimulating effects.\n\nThebiotechnologicalmethod ofdirected evolutionhas been applied to produce by mutation a large number ofPenicilliumstrains. These techniques includeerror-prone PCR,DNA shuffling,ITCHYandstrand-overlap PCR.\n\nThe biosynthetic gene cluster for penicillin was first cloned and sequenced in 1990.Overall, there are three main and important steps to the biosynthesis ofpenicillin G(benzylpenicillin).",
      "sections": [
        {
          "level": 2,
          "heading": "Nomenclature"
        },
        {
          "level": 3,
          "heading": "Penicillin units"
        },
        {
          "level": 2,
          "heading": "Types"
        },
        {
          "level": 3,
          "heading": "Natural penicillins"
        },
        {
          "level": 3,
          "heading": "Semi-synthetic penicillin"
        },
        {
          "level": 3,
          "heading": "Antibiotics created from 6-APA"
        },
        {
          "level": 3,
          "heading": "β-lactamase inhibitors"
        },
        {
          "level": 2,
          "heading": "Medical usage"
        },
        {
          "level": 3,
          "heading": "Penicillin G"
        },
        {
          "level": 3,
          "heading": "Penicillin V"
        },
        {
          "level": 3,
          "heading": "Bacterial susceptibility"
        },
        {
          "level": 2,
          "heading": "Side effects"
        },
        {
          "level": 2,
          "heading": "Structure"
        },
        {
          "level": 2,
          "heading": "Pharmacology"
        },
        {
          "level": 3,
          "heading": "Entry into bacteria"
        },
        {
          "level": 3,
          "heading": "Mechanism of action"
        },
        {
          "level": 3,
          "heading": "Pharmacokinetics"
        },
        {
          "level": 2,
          "heading": "Resistance"
        },
        {
          "level": 2,
          "heading": "History"
        },
        {
          "level": 3,
          "heading": "Discovery"
        },
        {
          "level": 3,
          "heading": "Development and medical application"
        },
        {
          "level": 3,
          "heading": "Mass production"
        },
        {
          "level": 3,
          "heading": "Structure determination and total synthesis"
        },
        {
          "level": 3,
          "heading": "Developments from penicillin"
        },
        {
          "level": 2,
          "heading": "Production"
        },
        {
          "level": 3,
          "heading": "Biosynthesis"
        }
      ],
      "raw_content_length": 899183,
      "cleaned_content_length": 36451,
      "scraped_at": "2025-09-02 15:30:57",
      "status": "success"
    },
    "extraction_status": "success",
    "extraction_error": null,
    "structured_data": {
      "primary_name": "Penicillin",
      "discoverers": [
        "Alexander Fleming",
        "Cecil George Paine",
        "Howard Florey",
        "Ernst Boris Chain"
      ],
      "discovery_years": [
        "1928"
      ],
      "discovery_timeline": [
        "1928: Discovery of penicillin by Alexander Fleming",
        "1930: First use of penicillin to treat eye infection by Cecil George Paine",
        "1940: Purification of penicillin F by Howard Florey and Ernst Boris Chain",
        "1942: First use of purified penicillin to treat streptococcal meningitis by Fleming",
        "1945: Nobel Prize awarded to Chain, Fleming, and Florey"
      ],
      "mechanism": "Penicillin works by inhibiting bacterial cell wall synthesis, specifically targeting the transpeptidation enzyme involved in cross-linking peptidoglycan layers in bacterial cell walls.",
      "key_features": [
        "Effective against a wide range of bacterial infections",
        "First antibiotic discovered",
        "Natural product derived from Penicillium moulds",
        "Various semi-synthetic derivatives available"
      ],
      "applications": [
        "Treatment of bacterial infections",
        "Used in clinical settings for various infections caused by staphylococci and streptococci",
        "Used in the treatment of neonatal conjunctivitis"
      ],
      "significance": "Penicillin was one of the first antibiotics used to effectively treat bacterial infections, revolutionizing medicine and leading to the development of other antibiotics.",
      "institutions": [
        "University of Oxford"
      ],
      "awards": [
        "1945 Nobel Prize in Physiology or Medicine"
      ],
      "extracted_at": "2023-10-01"
    }
  }
]