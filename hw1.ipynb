{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG4aG6PBi5JK"
   },
   "source": [
    "# IS5126 Individual Assignment 1 - HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18110,
     "status": "ok",
     "timestamp": 1756842125228,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "9aw-dl5LPMa3",
    "outputId": "00000e1f-f4c5-4bea-d71c-9365682f09a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawl4ai in e:\\anaconda\\envs\\is5126\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (24.1.0)\n",
      "Requirement already satisfied: aiohttp>=3.11.11 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite~=0.20 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: anyio>=4.0.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (4.10.0)\n",
      "Requirement already satisfied: lxml~=5.3 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (5.4.0)\n",
      "Requirement already satisfied: litellm>=1.53.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.74.9)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.2.6)\n",
      "Requirement already satisfied: pillow>=10.4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (11.3.0)\n",
      "Requirement already satisfied: playwright>=1.49.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.54.0)\n",
      "Requirement already satisfied: patchright>=1.49.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.52.5)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.1.1)\n",
      "Requirement already satisfied: requests~=2.26 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (4.13.5)\n",
      "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.2.0)\n",
      "Requirement already satisfied: xxhash~=3.4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (3.5.0)\n",
      "Requirement already satisfied: rank-bm25~=0.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (0.2.2)\n",
      "Requirement already satisfied: snowballstemmer~=2.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.11.7)\n",
      "Requirement already satisfied: pyOpenSSL>=24.3.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (25.1.0)\n",
      "Requirement already satisfied: psutil>=6.1.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (6.0.2)\n",
      "Requirement already satisfied: nltk>=3.9.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (3.9.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (14.1.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (0.28.1)\n",
      "Requirement already satisfied: fake-useragent>=2.0.3 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.7 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (8.2.1)\n",
      "Requirement already satisfied: chardet>=5.2.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (5.2.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: humanize>=4.10.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (4.13.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.2.2)\n",
      "Requirement already satisfied: alphashape>=1.3.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from crawl4ai) (2.1.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiosqlite~=0.20->crawl4ai) (4.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests~=2.26->crawl4ai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests~=2.26->crawl4ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests~=2.26->crawl4ai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests~=2.26->crawl4ai) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.20.1)\n",
      "Requirement already satisfied: click-log>=0.3.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (0.4.0)\n",
      "Requirement already satisfied: trimesh>=3.9.8 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (4.7.4)\n",
      "Requirement already satisfied: networkx>=2.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (3.4.2)\n",
      "Requirement already satisfied: rtree>=0.9.7 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (1.4.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from alphashape>=1.3.1->crawl4ai) (1.15.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from anyio>=4.0.0->crawl4ai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from anyio>=4.0.0->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from click>=8.1.7->crawl4ai) (0.4.6)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from httpx>=0.27.2->crawl4ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from httpx[http2]>=0.27.2->crawl4ai) (4.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->crawl4ai) (4.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.68.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (1.102.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.11.0)\n",
      "Requirement already satisfied: tokenizers in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.4.1)\n",
      "Requirement already satisfied: zipp>=3.20 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.23.0)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (0.10.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from patchright>=1.49.0->crawl4ai) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from patchright>=1.49.0->crawl4ai) (3.2.4)\n",
      "Requirement already satisfied: cryptography<46,>=41.0.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from pyOpenSSL>=24.3.0->crawl4ai) (45.0.6)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from cffi>=1.14->cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.34.4)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in e:\\anaconda\\envs\\is5126\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\envs\\is5126\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\envs\\is5126\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install crawl4ai\n",
    "%pip install beautifulsoup4 lxml requests\n",
    "!playwright install\n",
    "print(\"✅ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1756842125294,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "s3GgZE_LPMa4",
    "outputId": "230ce8f4-fa2a-4649-f683-0d7fc7716bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "from getpass import getpass\n",
    "import traceback\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import nest_asyncio, asyncio\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set font for visualization\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Enable nest_asyncio to support async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkYZwNMmi5JM"
   },
   "source": [
    "## Part 1: Wikipedia Scraper Implementation:\n",
    "– WikipediaScraper class with all required methods\n",
    "– Demonstration of scraping your chosen 5+ Wikipedia articles\n",
    "– Explanation of how your articles relate to your research domain\n",
    "– Error handling examples with sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1756842125297,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "QGJG0bMPi5JO",
    "outputId": "90362581-cd0a-40e6-eb9c-33a1edc8cb6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WikipediaScraper class definition completed\n"
     ]
    }
   ],
   "source": [
    "class WikipediaScraper:\n",
    "    def __init__(self, base_urls: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize Wikipedia scraper\n",
    "\n",
    "        Args:\n",
    "            base_urls: List of Wikipedia article URLs to scrape\n",
    "        \"\"\"\n",
    "        self.base_urls = base_urls\n",
    "        self.scraped_data = []\n",
    "        self.rate_limit_delay = 2  # Request interval (seconds) to avoid overloading Wikipedia servers\n",
    "\n",
    "        # Validate URL format\n",
    "        self._validate_urls()\n",
    "\n",
    "    def _validate_urls(self):\n",
    "        \"\"\"Validate if URLs are valid Wikipedia links\"\"\"\n",
    "        valid_urls = []\n",
    "        for url in self.base_urls:\n",
    "            if self._is_valid_wikipedia_url(url):\n",
    "                valid_urls.append(url)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid Wikipedia URL: {url}\")\n",
    "\n",
    "        if not valid_urls:\n",
    "            raise ValueError(\"No valid Wikipedia URLs provided\")\n",
    "\n",
    "        self.base_urls = valid_urls\n",
    "        print(f\"Validation passed, {len(valid_urls)} valid URLs found\")\n",
    "\n",
    "    def _is_valid_wikipedia_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is a valid Wikipedia URL\"\"\"\n",
    "        try:\n",
    "            return (\n",
    "                'wikipedia.org' in url and\n",
    "                '/wiki/' in url\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _looks_like_html(self, content: str) -> bool:\n",
    "        \"\"\"Determine whether a string looks like HTML\"\"\"\n",
    "        try:\n",
    "            return bool(content and content.lstrip().startswith(\"<\"))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _html_to_text(self, html: str) -> str:\n",
    "        \"\"\"Convert Wikipedia HTML to cleaner plain text (keep title and paragraphs)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "            # Title\n",
    "            title_node = soup.select_one(\"#firstHeading\")\n",
    "            title_text = title_node.get_text(\" \", strip=True) if title_node else \"\"\n",
    "\n",
    "            # Main content container\n",
    "            content = soup.select_one(\"#mw-content-text\") or soup\n",
    "\n",
    "            # Noise selectors to remove\n",
    "            noise_selectors = [\n",
    "                \"#toc\", \".mw-references-wrap\", \"table.infobox\",\n",
    "                \"div.navbox\", \"table.metadata\", \"div.hatnote\",\n",
    "                \"script\", \"style\", \"noscript\",\n",
    "            ]\n",
    "            for sel in noise_selectors:\n",
    "                for node in content.select(sel):\n",
    "                    node.decompose()\n",
    "\n",
    "            # Paragraph text\n",
    "            paragraphs = []\n",
    "            for p in content.select(\"p\"):\n",
    "                text = p.get_text(\" \", strip=True)\n",
    "                text = re.sub(r\"\\s*\\[\\d+\\]\", \"\", text)  # remove footnote like [1]\n",
    "                if text:\n",
    "                    paragraphs.append(text)\n",
    "\n",
    "            combined = ((title_text + \"\\n\\n\") if title_text else \"\") + \"\\n\\n\".join(paragraphs)\n",
    "            combined = re.sub(r\"\\n{3,}\", \"\\n\\n\", combined).strip()\n",
    "            return combined\n",
    "        except Exception:\n",
    "            return html\n",
    "\n",
    "    async def scrape_article(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Scrape a single Wikipedia article\n",
    "\n",
    "        Args:\n",
    "            url: Article URL\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing article information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Starting to scrape article: {url}\")\n",
    "\n",
    "            async with AsyncWebCrawler(verbose=False) as crawler:\n",
    "                # Scrape article content\n",
    "                result = await crawler.arun(url)\n",
    "\n",
    "                if result.success:\n",
    "                    # Extract article information\n",
    "                    article_data = self._extract_article_info(result, url)\n",
    "                    print(f\"Successfully scraped article: {article_data.get('title', 'Unknown')}\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    error_msg = result.error_message if hasattr(result, 'error_message') else 'Unknown error'\n",
    "                    print(f\"Scraping failed: {error_msg}\")\n",
    "                    return self._create_error_response(url, error_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping article {url}: {str(e)}\")\n",
    "            return self._create_error_response(url, str(e))\n",
    "\n",
    "    def _extract_article_info(self, result, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract article info from scraping result\"\"\"\n",
    "        try:\n",
    "            content = result.html\n",
    "\n",
    "            # Extract title\n",
    "            title = self._extract_title(content, url)\n",
    "\n",
    "            # Extract main content\n",
    "            main_content = self._extract_main_content(content)\n",
    "\n",
    "            # Extract key sections\n",
    "            sections = self._extract_sections(content)\n",
    "\n",
    "            # Clean content\n",
    "            cleaned_content = self.clean_content(main_content)\n",
    "\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'main_content': cleaned_content,\n",
    "                'sections': sections,\n",
    "                'raw_content_length': len(content),\n",
    "                'cleaned_content_length': len(cleaned_content),\n",
    "                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'status': 'success'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article info: {str(e)}\")\n",
    "            return self._create_error_response(url, f\"Extraction failed: {str(e)}\")\n",
    "\n",
    "    def _extract_title(self, content: str, url: str) -> str:\n",
    "        \"\"\"Extract article title from HTML content\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Prefer title from h1\n",
    "            h1_title = soup.find('h1')\n",
    "            if h1_title:\n",
    "                title_text = h1_title.get_text(strip=True)\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            # Fallback: page <title>\n",
    "            title_tag = soup.find('title')\n",
    "            if title_tag:\n",
    "                title_text = title_tag.get_text(strip=True)\n",
    "                # Remove suffix like \" - Wikipedia\"\n",
    "                if ' - Wikipedia' in title_text:\n",
    "                    title_text = title_text.split(' - Wikipedia')[0]\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting title: {str(e)}\")\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "    def _extract_main_content(self, content: str) -> str:\n",
    "        \"\"\"Extract main article content from HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            # Remove noisy elements\n",
    "            noise_selectors = [\n",
    "                '#toc', '.mw-references-wrap', 'table.infobox',\n",
    "                'div.navbox', 'table.metadata', 'div.hatnote',\n",
    "                'script', 'style', 'noscript', 'table', 'nav',\n",
    "                '.mw-editsection', '.mw-cite-backlink', '.thumb',\n",
    "                '.image', 'sup.reference', 'span.mw-ref'\n",
    "            ]\n",
    "\n",
    "            for selector in noise_selectors:\n",
    "                for element in main_content.select(selector):\n",
    "                    element.decompose()\n",
    "\n",
    "            # Collect all paragraph text\n",
    "            paragraphs = main_content.find_all('p')\n",
    "            content_list = []\n",
    "\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text) > 10:  # filter out very short paragraphs\n",
    "                    # remove footnote numbers and artifacts\n",
    "                    text = re.sub(r'\\s*\\[\\d+\\]', '', text)\n",
    "                    text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "                    content_list.append(text)\n",
    "\n",
    "            if content_list:\n",
    "                return '\\n\\n'.join(content_list)\n",
    "            else:\n",
    "                return \"Unable to extract valid content\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract content from HTML: {str(e)}\")\n",
    "            return content[:1000]  # return first 1000 chars as fallback\n",
    "\n",
    "    def _extract_sections(self, content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract heading structure from HTML (without content)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            sections = []\n",
    "\n",
    "            # Find all heading tags\n",
    "            headings = main_content.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "            for heading in headings:\n",
    "                level = int(heading.name[1])  # h1->1, h2->2, h3->3\n",
    "                heading_text = heading.get_text(strip=True)\n",
    "\n",
    "                # Skip some headings\n",
    "                if self._should_skip_heading(heading_text):\n",
    "                    continue\n",
    "\n",
    "                sections.append({\n",
    "                    'level': level,\n",
    "                    'heading': heading_text\n",
    "                })\n",
    "\n",
    "            return sections\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract sections from HTML: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _should_skip_heading(self, heading_text: str) -> bool:\n",
    "        \"\"\"Decide whether a heading should be skipped\"\"\"\n",
    "        skip_patterns = [\n",
    "            'Contents', 'References', 'External links', 'Further reading',\n",
    "            'See also', 'Notes', 'Bibliography', 'Sources', 'Citations', 'Footnotes'\n",
    "        ]\n",
    "\n",
    "        for pattern in skip_patterns:\n",
    "            if pattern.lower() in heading_text.lower():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def clean_content(self, raw_content: str) -> str:\n",
    "        \"\"\"Clean and pre-process scraped content\"\"\"\n",
    "        if not raw_content:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            # Remove excessive whitespace and newlines\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', raw_content)\n",
    "            cleaned = re.sub(r'[ \\t]+', ' ', cleaned)\n",
    "\n",
    "            # Remove footnote references\n",
    "            cleaned = re.sub(r'\\s*\\[\\d+\\]', '', cleaned)\n",
    "\n",
    "            # Ensure proper blank lines between paragraphs\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "\n",
    "            return cleaned.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to clean content: {e}\")\n",
    "            return raw_content\n",
    "\n",
    "    async def scrape_all_articles(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Scrape all articles\"\"\"\n",
    "        print(f\"Starting batch scrape for {len(urls)} articles\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                print(f\"Progress: {i+1}/{len(urls)}\")\n",
    "\n",
    "                # Scrape single article\n",
    "                article_data = await self.scrape_article(url)\n",
    "                all_results.append(article_data)\n",
    "\n",
    "                # Rate limit: add delay between requests\n",
    "                if i < len(urls) - 1:  # not the last one\n",
    "                    print(f\"Waiting {self.rate_limit_delay} seconds...\")\n",
    "                    await asyncio.sleep(self.rate_limit_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping article {url}: {str(e)}\")\n",
    "                all_results.append(self._create_error_response(url, str(e)))\n",
    "\n",
    "        self.scraped_data = all_results\n",
    "        print(f\"Batch scrape completed, successfully scraped {len([r for r in all_results if r.get('status') == 'success'])} articles\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def _create_error_response(self, url: str, error_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create an error response\"\"\"\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': 'Error',\n",
    "            'main_content': '',\n",
    "            'sections': [],\n",
    "            'error': error_message,\n",
    "            'status': 'error',\n",
    "            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "    def save_to_json(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save scraped data to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Full data saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "    def save_content_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save main contents of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                main = item.get('main_content', '').strip()\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if main:\n",
    "                    lines.append(main)\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Main content saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving main content Markdown file: {str(e)}\")\n",
    "\n",
    "    def save_sections_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save section information of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                sections = item.get('sections', [])\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if isinstance(sections, list) and sections:\n",
    "                    for s in sections:\n",
    "                        if isinstance(s, dict):\n",
    "                            heading = s.get('heading') or s.get('title') or 'Section'\n",
    "                            content = s.get('content') or s.get('text') or ''\n",
    "                            lines.append(f\"## {heading}\")\n",
    "                            if content:\n",
    "                                lines.append(content)\n",
    "                            lines.append(\"\")\n",
    "                        else:\n",
    "                            lines.append(str(s))\n",
    "                            lines.append(\"\")\n",
    "                else:\n",
    "                    lines.append(\"No sections available\")\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Section information saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving section information Markdown file: {str(e)}\")\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get scraping summary\"\"\"\n",
    "        if not self.scraped_data:\n",
    "            return {\"message\": \"No scraped data\"}\n",
    "\n",
    "        successful = [r for r in self.scraped_data if r.get('status') == 'success']\n",
    "        failed = [r for r in self.scraped_data if r.get('status') == 'error']\n",
    "\n",
    "        total_content_length = sum(r.get('cleaned_content_length', 0) for r in successful)\n",
    "\n",
    "        return {\n",
    "            'total_articles': len(self.scraped_data),\n",
    "            'successful_scrapes': len(successful),\n",
    "            'failed_scrapes': len(failed),\n",
    "            'total_content_length': total_content_length,\n",
    "            'average_content_length': total_content_length // len(successful) if successful else 0,\n",
    "            'success_rate': len(successful) / len(self.scraped_data) * 100\n",
    "        }\n",
    "\n",
    "print(\"✅ WikipediaScraper class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1756842125353,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "Z1Gt1AeCi5JP"
   },
   "outputs": [],
   "source": [
    "# Utility function to convert search keywords to Wikipedia URLs\n",
    "def build_wikipedia_urls(terms):\n",
    "    \"\"\"Generate Wikipedia article URL list from keywords (English articles).\n",
    "    - Rules: Replace spaces with underscores; trim whitespace; deduplicate.\n",
    "    - Note: If keywords are not English or non-standard titles, additional processing/search may be needed.\n",
    "    \"\"\"\n",
    "    if not terms:\n",
    "        return []\n",
    "    base = \"https://en.wikipedia.org/wiki/\"\n",
    "    urls = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        title = t.strip().replace(\" \", \"_\")\n",
    "        if not title:\n",
    "            continue\n",
    "        url = base + title\n",
    "        if url not in seen:\n",
    "            urls.append(url)\n",
    "            seen.add(url)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1756842125355,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "Gff1GRT3i5JP",
    "outputId": "65b5f16e-03df-4c2a-9a27-f10a179a712f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data scraping function definition completed\n"
     ]
    }
   ],
   "source": [
    "async def scrape_scientific_discoveries(search_terms_list: Optional[List[str]] = None):\n",
    "    \"\"\"Scrape scientific discovery related Wikipedia articles\"\"\"\n",
    "    print(\"🚀 Starting to scrape scientific discovery related Wikipedia articles...\")\n",
    "    print(\"=\" * 60)\n",
    "    if search_terms_list != None:\n",
    "        scientific_urls = build_wikipedia_urls(search_terms_list)\n",
    "        print(f\"✅ Generated {len(scientific_urls)} Wikipedia URLs\")\n",
    "    else:\n",
    "        search_terms = [\n",
    "        \"CRISPR\",\n",
    "        \"RNA vaccine\",\n",
    "        \"Gravitational wave\",\n",
    "        \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        \"Ancient DNA\",\n",
    "        \"Water on Mars\",\n",
    "        \"Penicillin\"\n",
    "        ]\n",
    "        scientific_urls = build_wikipedia_urls(search_terms)\n",
    "\n",
    "    print(scientific_urls)\n",
    "    scraper = WikipediaScraper(scientific_urls)\n",
    "    results = await scraper.scrape_all_articles(scientific_urls)\n",
    "\n",
    "    # Save data\n",
    "    scraper.save_to_json(results,\"./scientific_discoveries.json\")\n",
    "    scraper.save_content_to_markdown(results,\"./content.md\")\n",
    "    scraper.save_sections_to_markdown(results,\"./section.md\")\n",
    "\n",
    "    # Display summary\n",
    "    summary = scraper.get_summary()\n",
    "    print(\"\\n📊 Scraping Results Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\n🎉 Scraping completed! Retrieved data for {len(results)} articles\")\n",
    "    return results\n",
    "\n",
    "print(\"✅ Data scraping function definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 58861,
     "status": "ok",
     "timestamp": 1756842184212,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "QA1LO9gQi5JQ",
    "outputId": "0eabec5a-16a7-4e87-a1e1-301cc16a9514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting to scrape scientific discovery related Wikipedia articles...\n",
      "============================================================\n",
      "['https://en.wikipedia.org/wiki/CRISPR', 'https://en.wikipedia.org/wiki/RNA_vaccine', 'https://en.wikipedia.org/wiki/Gravitational_wave', 'https://en.wikipedia.org/wiki/Higgs_boson', 'https://en.wikipedia.org/wiki/Quantum_computing', 'https://en.wikipedia.org/wiki/Ancient_DNA', 'https://en.wikipedia.org/wiki/Water_on_Mars', 'https://en.wikipedia.org/wiki/Penicillin']\n",
      "Validation passed, 8 valid URLs found\n",
      "Starting batch scrape for 8 articles\n",
      "Progress: 1/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/CRISPR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">81s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m81s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">09s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m09s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">92s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m92s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: CRISPR\n",
      "Waiting 2 seconds...\n",
      "Progress: 2/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/RNA_vaccine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">47s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m47s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">56s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m56s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4.</span><span style=\"color: #008000; text-decoration-color: #008000\">04s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m04s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: mRNA vaccine\n",
      "Waiting 2 seconds...\n",
      "Progress: 3/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Gravitational_wave\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">06s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m06s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">78s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m78s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">85s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m85s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Gravitational wave\n",
      "Waiting 2 seconds...\n",
      "Progress: 4/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Higgs_boson\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">94s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m94s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">87s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m87s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">82s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m82s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Higgs boson\n",
      "Waiting 2 seconds...\n",
      "Progress: 5/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Quantum_computing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">62s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m62s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">78s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m78s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">41s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m41s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Quantum computing\n",
      "Waiting 2 seconds...\n",
      "Progress: 6/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Ancient_DNA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">05s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m05s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">27s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m27s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">33s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m33s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Ancient DNA\n",
      "Waiting 2 seconds...\n",
      "Progress: 7/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Water_on_Mars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">66s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m66s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">68s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m68s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">35s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m35s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Water on Mars\n",
      "Waiting 2 seconds...\n",
      "Progress: 8/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Penicillin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">20s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m20s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">49s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m49s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">70s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m70s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Penicillin\n",
      "Batch scrape completed, successfully scraped 8 articles\n",
      "Full data saved to: ./scientific_discoveries.json\n",
      "Main content saved to: ./content.md\n",
      "Section information saved to: ./section.md\n",
      "\n",
      "📊 Scraping Results Summary:\n",
      "========================================\n",
      "total_articles: 8\n",
      "successful_scrapes: 8\n",
      "failed_scrapes: 0\n",
      "total_content_length: 367048\n",
      "average_content_length: 45881\n",
      "success_rate: 100.0\n",
      "\n",
      "🎉 Scraping completed! Retrieved data for 8 articles\n"
     ]
    }
   ],
   "source": [
    "# Execute data scraping - can directly use await in Jupyter\n",
    "scraped_result = await scrape_scientific_discoveries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56kuX5znmOfO"
   },
   "source": [
    "## Part2 Structured Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756842184234,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "xaiOwWS0PMa9",
    "outputId": "b301a6fa-e0d0-476f-d32b-10e86eb5303f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scientific discovery domain Pydantic model definition completed\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model designed for scientific discovery domain\n",
    "class ScientificDiscoveryExtraction(BaseModel):\n",
    "    \"\"\"Structured data model for scientific discovery information\"\"\"\n",
    "\n",
    "    # Basic information\n",
    "    primary_name: str = Field(description=\"Primary name of the scientific discovery or technology\")\n",
    "\n",
    "    # Discoverers and time\n",
    "    discoverers: List[str] = Field(description=\"List of names of main discoverers or researchers\")\n",
    "    discovery_years: List[str] = Field(description=\"Most important discovery year, strongly recommend returning only one year. Must select the single year that best represents the core breakthrough of the technology/discovery from the article. Priority: key technical implementation > important paper publication > initial concept proposal. For example, for CRISPR, choose 2012 (Doudna and Charpentier's key paper) rather than 1987 (first sequence discovery). Avoid returning multiple years.\")\n",
    "    discovery_timeline: List[str] = Field(description=\"Timeline from initial to final discovery\")\n",
    "\n",
    "    # Technical details\n",
    "    mechanism: str = Field(description=\"Basic working principle or mechanism of the technology\")\n",
    "    key_features: List[str] = Field(description=\"Main features or advantages of the technology\")\n",
    "\n",
    "    # Applications and impact\n",
    "    applications: List[str] = Field(description=\"Main application fields or uses\")\n",
    "    significance: str = Field(description=\"Scientific or social significance of the discovery\")\n",
    "\n",
    "    # Optional additional information\n",
    "    institutions: Optional[List[str]] = Field(default=None, description=\"Related research institutions or universities\")\n",
    "    awards: Optional[List[str]] = Field(default=None, description=\"Important awards or honors received\")\n",
    "\n",
    "    # Metadata\n",
    "    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat(), description=\"Data extraction time\")\n",
    "\n",
    "print(\"✅ Scientific discovery domain Pydantic model definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1756842184270,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "9FdWkCKFPMa9",
    "outputId": "0913bd11-9bbd-4c39-8bd6-d20170b02060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Structured data extractor class definition completed\n"
     ]
    }
   ],
   "source": [
    "class StructuredDataExtractor:\n",
    "    \"\"\"A class for structured data extraction using the OpenAI API\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None, model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the extractor\n",
    "\n",
    "        Args:\n",
    "            api_key: OpenAI API key (optional, will use environment variable OPENAI_API_KEY first)\n",
    "            model: Model name to use\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.client = None\n",
    "\n",
    "        # Prefer the provided API key, then the environment variable; if not found, ask interactively\n",
    "        final_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not final_api_key:\n",
    "            try:\n",
    "                final_api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "            except Exception:\n",
    "                final_api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "\n",
    "        if final_api_key:\n",
    "            self.client = openai.OpenAI(api_key=final_api_key)\n",
    "            print(\"✅ OpenAI client initialized successfully\")\n",
    "        else:\n",
    "            print(\"⚠️ No OpenAI API key found, related features will be limited\")\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Retry configuration\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2  # seconds\n",
    "\n",
    "    def extract_structured_data(self, content: str, model: str = None) -> Optional[ScientificDiscoveryExtraction]:\n",
    "        \"\"\"\n",
    "        Extract structured data using OpenAI structured output\n",
    "\n",
    "        Args:\n",
    "            content: Text content to extract\n",
    "            model: Model to use (optional)\n",
    "\n",
    "        Returns:\n",
    "            ScientificDiscoveryExtraction: Extracted structured data\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            self.logger.error(\"OpenAI client not initialized, please provide an API key\")\n",
    "            return None\n",
    "\n",
    "        model_to_use = model or self.model\n",
    "\n",
    "        # Build system prompt\n",
    "        system_prompt = \"\"\"\n",
    "        You are a professional scientific literature analyst. Please extract structured information\n",
    "        from the given scientific article content.\n",
    "\n",
    "        Extract the following information:\n",
    "        1. Main name of the scientific discovery or technology\n",
    "        2. Main discoverer(s) or researcher(s)\n",
    "        3. Important discovery time points\n",
    "        4. Timeline of the discovery\n",
    "        5. Basic working principle of the technology\n",
    "        6. Main features and advantages\n",
    "        7. Application areas\n",
    "        8. Scientific significance\n",
    "        9. Related institutions (if any)\n",
    "        10. Awards received (if any)\n",
    "\n",
    "        **Important extraction rules**:\n",
    "        - For discovery_years, follow these rules:\n",
    "          * If a range of years is mentioned (e.g., \"2010-2012\", \"from 2008 to 2011\"),\n",
    "            determine which year is most relevant to the key discovery in the title\n",
    "          * Prefer the year associated with the key breakthrough, first publication, or major experiment success\n",
    "          * If multiple years are mentioned and it's unclear which is more important, return the earliest one\n",
    "          * Example: If the title is \"CRISPR gene-editing technology\" and the text says\n",
    "            \"Research started in 2008, breakthrough in 2012\", choose 2012\n",
    "          * If completely uncertain, return the earliest year in the range\n",
    "\n",
    "        If some information is not explicitly mentioned, try to infer from the context,\n",
    "        or state \"unclear\" in the corresponding field.\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Please extract structured information from the following scientific article content:\n",
    "\n",
    "        {content}  # Limit length to avoid token issues\n",
    "\n",
    "        Please extract information according to the ScientificDiscoveryExtraction model.\n",
    "\n",
    "        **Special note on discovery_years**:\n",
    "        - Carefully read the article title and content to understand the core of the scientific discovery\n",
    "        - If a year range is present, decide which year best represents the key breakthrough\n",
    "        - Priority: first major publication, critical technical success, or key experimental result\n",
    "        - If unclear, choose the earliest year in the range\n",
    "        \"\"\"\n",
    "\n",
    "        # Try extraction with retries\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self.logger.info(f\"Starting extraction attempt {attempt + 1}...\")\n",
    "\n",
    "                response = self.client.beta.chat.completions.parse(\n",
    "                    model=model_to_use,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    response_format=ScientificDiscoveryExtraction,\n",
    "                    temperature=0.1  # Low temperature for consistency\n",
    "                )\n",
    "\n",
    "                # Extract structured data\n",
    "                extracted_data = response.choices[0].message.parsed\n",
    "\n",
    "                if extracted_data:\n",
    "                    self.logger.info(\"✅ Structured data extraction succeeded\")\n",
    "                    return extracted_data\n",
    "                else:\n",
    "                    self.logger.warning(\"⚠️ API returned empty data\")\n",
    "\n",
    "            except openai.RateLimitError as e:\n",
    "                self.logger.warning(f\"Rate limit error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (2 ** attempt))  # exponential backoff\n",
    "\n",
    "            except openai.APIError as e:\n",
    "                self.logger.error(f\"OpenAI API error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unknown error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "        self.logger.error(\"❌ All extraction attempts failed\")\n",
    "        return None\n",
    "\n",
    "    def batch_extract(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process multiple articles in batch\n",
    "\n",
    "        Args:\n",
    "            articles: List of article data\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of extraction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        self.logger.info(f\"Starting batch processing of {len(articles)} articles...\")\n",
    "\n",
    "        for i, article in enumerate(articles):\n",
    "            try:\n",
    "                self.logger.info(f\"Processing article {i+1}/{len(articles)}: {article.get('title', 'Unknown')}\")\n",
    "\n",
    "                # Check article status\n",
    "                if article.get('status') != 'success':\n",
    "                    self.logger.warning(f\"Skipping failed article: {article.get('error', 'Unknown error')}\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'skipped',\n",
    "                        'extraction_error': article.get('error', 'Article scraping failed'),\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Extract main content\n",
    "                content = article.get('main_content', '')\n",
    "                if not content:\n",
    "                    self.logger.warning(\"Article content is empty, skipping\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Empty content',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Run structured extraction\n",
    "                extracted_data = self.extract_structured_data(content)\n",
    "\n",
    "                if extracted_data:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'success',\n",
    "                        'extraction_error': None,\n",
    "                        'structured_data': extracted_data.dict()\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Extraction failed after retries',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "\n",
    "                # Delay to avoid rate limit\n",
    "                if i < len(articles) - 1:\n",
    "                    time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing article: {e}\")\n",
    "                results.append({\n",
    "                    'article_info': article,\n",
    "                    'extraction_status': 'error',\n",
    "                    'extraction_error': str(e),\n",
    "                    'structured_data': None\n",
    "                })\n",
    "\n",
    "        # Summary statistics\n",
    "        successful = len([r for r in results if r['extraction_status'] == 'success'])\n",
    "        failed = len([r for r in results if r['extraction_status'] in ['failed', 'error']])\n",
    "        skipped = len([r for r in results if r['extraction_status'] == 'skipped'])\n",
    "\n",
    "        self.logger.info(f\"Batch processing finished: Success {successful}, Failed {failed}, Skipped {skipped}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_extraction_results(self, results: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"\n",
    "        Save extraction results to a file\n",
    "\n",
    "        Args:\n",
    "            results: List of extraction results\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n",
    "            self.logger.info(f\"✅ Extraction results saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error saving file: {e}\")\n",
    "\n",
    "print(\"✅ Structured data extractor class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1756842184285,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "S3QYtLVgPMa-"
   },
   "outputs": [],
   "source": [
    "def perform_structured_extraction(scraper_result):\n",
    "    # Perform structured data extraction - this was the missing call code!\n",
    "    print(\"🚀 Starting structured data extraction...\")\n",
    "    print(\"=\" * 60)\n",
    "    # Prefer the provided API key, then the environment variable; if not found, ask interactively\n",
    "    final_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not final_api_key:\n",
    "        try:\n",
    "            final_api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "        except Exception:\n",
    "            final_api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "    else:\n",
    "        try:\n",
    "            # 1. Initialize the extractor (using the previously defined class)\n",
    "            extractor = StructuredDataExtractor()\n",
    "\n",
    "            if scraper_result is not None:\n",
    "                scraped_articles = scraper_result\n",
    "                print(f\"📖 Using the provided scraping results\")\n",
    "            else:\n",
    "                print(f\"📖 No scraping results provided, trying to load from file\")\n",
    "                # 2. Read previously scraped data\n",
    "                with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "                    scraped_articles = json.load(f)\n",
    "\n",
    "            print(f\"📖 Successfully loaded {len(scraped_articles)} articles\")\n",
    "\n",
    "            # 3. Perform batch structured extraction (call the core method!)\n",
    "            print(\"🔄 Starting batch structured extraction...\")\n",
    "            extraction_results = extractor.batch_extract(scraped_articles)\n",
    "\n",
    "            # 4. Save extraction results (call the save method!)\n",
    "            extractor.save_extraction_results(extraction_results, './structured_extractions.json')\n",
    "\n",
    "            # 5. Show extraction statistics\n",
    "            successful = [r for r in extraction_results if r['extraction_status'] == 'success']\n",
    "            failed = [r for r in extraction_results if r['extraction_status'] != 'success']\n",
    "\n",
    "            print(f\"\\n📊 Extraction statistics:\")\n",
    "            print(f\"✅ Successfully extracted: {len(successful)} articles\")\n",
    "            print(f\"❌ Failed to extract: {len(failed)} articles\")\n",
    "\n",
    "            # 6. Display the first successful extraction result\n",
    "            if successful:\n",
    "                first_result = successful[0]\n",
    "                print(f\"\\n📄 Sample extraction result - {first_result['article_info']['title']}:\")\n",
    "                print(\"-\" * 50)\n",
    "                structured = first_result['structured_data']\n",
    "                print(f\"📝 Primary name: {structured['primary_name']}\")\n",
    "                print(f\"👥 Discoverers: {', '.join(structured['discoverers'])}\")\n",
    "                print(f\"📅 Discovery years: {', '.join(structured['discovery_years'])}\")\n",
    "                print(f\"🔬 Applications: {len(structured['applications'])} items\")\n",
    "                print(f\"⭐ Key features: {len(structured['key_features'])} items\")\n",
    "\n",
    "            print(f\"\\n✅ Structured extraction completed! Results saved to './structured_extractions.json'\")\n",
    "\n",
    "            return extraction_results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Execution error: {str(e)}\")\n",
    "            print(\"Please check the API key settings and ensure the data file exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUgMw9L6PMa-"
   },
   "source": [
    "## Part 3: Function Calling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756842184294,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "s4rlA9QF1oSm",
    "outputId": "cb2ac6a7-1a99-4688-e1d4-337fa60ae2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAIAnalyzer class definition completed\n"
     ]
    }
   ],
   "source": [
    "class OpenAIAnalyzer:\n",
    "    \"\"\"OpenAI analyzer class for intelligent matching and data analysis\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"Initialize OpenAI client\n",
    "\n",
    "        Args:\n",
    "            api_key: OpenAI API key, if None then get from environment variables\n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            try:\n",
    "                api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "            except Exception:\n",
    "                api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        print(\"✅ OpenAI client initialized successfully\")\n",
    "\n",
    "    def analyze_query_match(self, user_query: str, available_names: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the match between user query and available names\n",
    "\n",
    "        Args:\n",
    "            user_query: User input query\n",
    "            available_names: List of available names\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing best match and confidence\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build prompt\n",
    "            prompt = f\"\"\"\n",
    "            User query: \"{user_query}\"\n",
    "\n",
    "            Available scientific discovery names:\n",
    "            {json.dumps(available_names, ensure_ascii=False, indent=2)}\n",
    "\n",
    "            Please analyze the match between user query and available names, return JSON format result:\n",
    "            {{\n",
    "                \"best_match\": \"most matching name\",\n",
    "                \"confidence\": confidence from 0.0-1.0,\n",
    "                \"reasoning\": \"matching reason\",\n",
    "                \"alternative_matches\": [\"other possible matches\"]\n",
    "            }}\n",
    "\n",
    "            Note:\n",
    "            1. Consider synonyms, abbreviations, different expressions\n",
    "            2. If no suitable match found, set confidence to 0.0\n",
    "            3. Only return JSON, no other text\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a professional scientific data matching assistant, skilled at analyzing query and data matching.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ OpenAI analysis error: {e}\")\n",
    "            return {\n",
    "                \"best_match\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"reasoning\": f\"Analysis failed: {e}\",\n",
    "                \"alternative_matches\": []\n",
    "            }\n",
    "\n",
    "    def suggest_search_strategy(self, user_query: str, data_sample: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Suggest search strategy\n",
    "\n",
    "        Args:\n",
    "            user_query: User query\n",
    "            data_sample: Data sample\n",
    "\n",
    "        Returns:\n",
    "            Search strategy suggestion\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            User query: \"{user_query}\"\n",
    "\n",
    "            Data sample structure:\n",
    "            {json.dumps(data_sample[:3], ensure_ascii=False, indent=2)}\n",
    "\n",
    "            Please analyze user query intent, suggest best search strategy, return JSON format:\n",
    "            {{\n",
    "                \"search_type\": \"name|scientist|application|general\",\n",
    "                \"search_keywords\": [\"keyword1\", \"keyword2\"],\n",
    "                \"explanation\": \"search strategy explanation\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a scientific data search strategy expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Search strategy analysis error: {e}\")\n",
    "            return {\n",
    "                \"search_type\": \"general\",\n",
    "                \"search_keywords\": [user_query],\n",
    "                \"explanation\": f\"Analysis failed, using general search: {e}\"\n",
    "            }\n",
    "\n",
    "print(\"✅ OpenAIAnalyzer class definition completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F64QtW3jNh4"
   },
   "source": [
    "***3.1 Scientific Data Query***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1756842184384,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "c_SeoN47jNh4",
    "outputId": "336a4f67-f3d6-4fba-fb1b-84e0e802bb20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced ScientificDataQuery class definition completed\n",
      "💡 Run test_data_loading() to test data loading\n"
     ]
    }
   ],
   "source": [
    "class ScientificDataQuery:\n",
    "    \"\"\"Scientific discovery data query class - enhanced with OpenAI intelligent matching\n",
    "\n",
    "    This class provides data access functions with OpenAI-powered intelligent matching.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file: str = \"./structured_extractions.json\", data: Optional[List[Dict[str, Any]]] = None, openai_api_key: Optional[str] = None):\n",
    "        \"\"\"Initialize the query class with data loading and OpenAI analyzer\n",
    "\n",
    "        Args:\n",
    "            data_file: Path to the structured data file (default: \"./structured_extractions.json\")\n",
    "            data: Optional in-memory data to use instead of loading from file\n",
    "            openai_api_key: OpenAI API key for intelligent matching\n",
    "        \"\"\"\n",
    "        # Initialize OpenAI analyzer\n",
    "        try:\n",
    "            self.openai_analyzer = OpenAIAnalyzer(api_key=openai_api_key)\n",
    "            self.has_openai = True\n",
    "            print(\"✅ OpenAI analyzer initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.openai_analyzer = None\n",
    "            self.has_openai = False\n",
    "            print(f\"⚠️ OpenAI analyzer initialization failed: {e}\")\n",
    "            print(\"   Will use basic matching functionality\")\n",
    "        try:\n",
    "            if data is not None:\n",
    "                # Use in-memory data directly\n",
    "                self.raw_data = data\n",
    "                print(f\"✅ Using provided in-memory data with {len(data)} items\")\n",
    "            else:\n",
    "                # Load from file (default behavior for function calls)\n",
    "                with open(data_file, 'r', encoding='utf-8') as f:\n",
    "                    self.raw_data = json.load(f)\n",
    "                print(f\"✅ Loaded data from {data_file}\")\n",
    "\n",
    "            # Extract successful structured data\n",
    "            self.discoveries = []\n",
    "            for item in self.raw_data:\n",
    "                if isinstance(item, dict):\n",
    "                    if item.get('extraction_status') == 'success' and item.get('structured_data'):\n",
    "                        # Merge structured data with metadata\n",
    "                        structured_data = item['structured_data']\n",
    "                        if isinstance(structured_data, dict):\n",
    "                            # Merge article_info and structured_data\n",
    "                            article_info = item.get('article_info', {})\n",
    "                            self.discoveries.append({\n",
    "                                **article_info,  # Contains url, title and other basic information\n",
    "                                **structured_data  # Contains primary_name, discoverers and other structured data\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"⚠️ Skipping item with malformed 'structured_data': {item.get('article_info', {}).get('title', 'Unknown Title')}\")\n",
    "                    elif isinstance(item, dict) and 'primary_name' in item:\n",
    "                        # Already structured data\n",
    "                        self.discoveries.append(item)\n",
    "\n",
    "            print(f\"✅ Successfully loaded {len(self.discoveries)} scientific discovery data\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Error: Data file not found at {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"❌ Error: Could not decode JSON from {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except Exception as e:\n",
    "            print(f\"❌ An unexpected error occurred during data loading: {e}\")\n",
    "            self.discoveries = []\n",
    "\n",
    "    def find_discovery_by_name(self, discovery_name: str, use_openai: bool = True) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Find a scientific discovery by name with OpenAI intelligent matching\n",
    "\n",
    "        Args:\n",
    "            discovery_name: Name of the discovery to find\n",
    "            use_openai: Whether to use OpenAI for intelligent matching if exact match fails\n",
    "\n",
    "        Returns:\n",
    "            Discovery data if found, None otherwise\n",
    "        \"\"\"\n",
    "        # First try exact matching\n",
    "        discovery_name_lower = discovery_name.lower().strip()\n",
    "        for discovery in self.discoveries:\n",
    "            if discovery.get('primary_name', '').lower() == discovery_name_lower:\n",
    "                return discovery\n",
    "\n",
    "        # If exact matching fails and OpenAI is enabled, try intelligent matching\n",
    "        if use_openai and self.has_openai:\n",
    "            print(f\"🔍 Exact matching failed, trying OpenAI intelligent matching: '{discovery_name}'\")\n",
    "\n",
    "            # Get all available names\n",
    "            available_names = [d.get('primary_name', '') for d in self.discoveries if d.get('primary_name')]\n",
    "\n",
    "            # Use OpenAI to analyze matching\n",
    "            match_result = self.openai_analyzer.analyze_query_match(discovery_name, available_names)\n",
    "\n",
    "            if match_result.get('confidence', 0) > 0.7:  # Confidence threshold\n",
    "                best_match = match_result.get('best_match')\n",
    "                print(f\"✅ OpenAI found match: '{best_match}' (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "                print(f\"   Reason: {match_result.get('reasoning', '')}\")\n",
    "\n",
    "                # Return the matched discovery\n",
    "                for discovery in self.discoveries:\n",
    "                    if discovery.get('primary_name', '').lower() == best_match.lower():\n",
    "                        return discovery\n",
    "            else:\n",
    "                print(f\"❌ OpenAI did not find high confidence match (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "                if match_result.get('alternative_matches'):\n",
    "                    print(f\"   Alternative matches: {match_result.get('alternative_matches', [])}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def find_discoveries_by_scientist(self, scientist_name: str, use_openai: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find all discoveries by a scientist with OpenAI intelligent matching\n",
    "\n",
    "        Args:\n",
    "            scientist_name: Name of the scientist\n",
    "            use_openai: Whether to use OpenAI for intelligent matching if exact match fails\n",
    "\n",
    "        Returns:\n",
    "            List of discoveries by the scientist\n",
    "        \"\"\"\n",
    "        # First try exact matching\n",
    "        scientist_name_lower = scientist_name.lower().strip()\n",
    "        matched_discoveries = []\n",
    "\n",
    "        for discovery in self.discoveries:\n",
    "            for discoverer in discovery.get('discoverers', []):\n",
    "                if discoverer.lower() == scientist_name_lower:\n",
    "                    matched_discoveries.append(discovery)\n",
    "                    break\n",
    "\n",
    "        # If exact matching fails and OpenAI is enabled, try intelligent matching\n",
    "        if not matched_discoveries and use_openai and self.has_openai:\n",
    "            print(f\"🔍 Exact matching failed, trying OpenAI intelligent matching for scientist: '{scientist_name}'\")\n",
    "\n",
    "            # Get all scientist names\n",
    "            all_scientists = set()\n",
    "            for discovery in self.discoveries:\n",
    "                all_scientists.update(discovery.get('discoverers', []))\n",
    "            available_scientists = list(all_scientists)\n",
    "\n",
    "            # Use OpenAI to analyze matching\n",
    "            match_result = self.openai_analyzer.analyze_query_match(scientist_name, available_scientists)\n",
    "\n",
    "            if match_result.get('confidence', 0) > 0.7:\n",
    "                best_match = match_result.get('best_match')\n",
    "                print(f\"✅ OpenAI found matching scientist: '{best_match}' (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "\n",
    "                # Find all discoveries by this scientist\n",
    "                for discovery in self.discoveries:\n",
    "                    for discoverer in discovery.get('discoverers', []):\n",
    "                        if discoverer.lower() == best_match.lower():\n",
    "                            matched_discoveries.append(discovery)\n",
    "                            break\n",
    "            else:\n",
    "                print(f\"❌ OpenAI did not find high confidence matching scientist (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "\n",
    "        return matched_discoveries\n",
    "\n",
    "    def get_all_discoveries(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all available scientific discoveries\"\"\"\n",
    "        return self.discoveries\n",
    "\n",
    "    def get_discovery_details(self, discovery_name: str, use_openai: bool = True) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get comprehensive details about a specific scientific discovery with OpenAI matching\"\"\"\n",
    "        discovery = self.find_discovery_by_name(discovery_name, use_openai)\n",
    "        if discovery:\n",
    "            return {\n",
    "                \"primary_name\": discovery.get('primary_name'),\n",
    "                \"discoverers\": discovery.get('discoverers', []),\n",
    "                \"discovery_years\": discovery.get('discovery_years', []),\n",
    "                \"mechanism\": discovery.get('mechanism', ''),\n",
    "                \"key_features\": discovery.get('key_features', []),\n",
    "                \"applications\": discovery.get('applications', []),\n",
    "                \"significance\": discovery.get('significance', ''),\n",
    "                \"institutions\": discovery.get('institutions', []),\n",
    "                \"awards\": discovery.get('awards', []),\n",
    "                \"url\": discovery.get('url', '')\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def search_by_application(self, application_keyword: str, use_openai: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for scientific discoveries related to a specific application keyword with OpenAI matching\"\"\"\n",
    "        # First try exact matching\n",
    "        results = []\n",
    "        keyword_lower = application_keyword.lower()\n",
    "        for discovery in self.discoveries:\n",
    "            applications = discovery.get('applications', [])\n",
    "            if any(keyword_lower in app.lower() for app in applications):\n",
    "                results.append(discovery)\n",
    "\n",
    "        # If exact matching results are few and OpenAI is enabled, try intelligent matching\n",
    "        if len(results) < 3 and use_openai and self.has_openai:\n",
    "            print(f\"🔍 Few exact matching results, trying OpenAI intelligent matching for application: '{application_keyword}'\")\n",
    "\n",
    "            # Get all application keywords\n",
    "            all_applications = set()\n",
    "            for discovery in self.discoveries:\n",
    "                all_applications.update(discovery.get('applications', []))\n",
    "            available_applications = list(all_applications)\n",
    "\n",
    "            # Use OpenAI to analyze matching\n",
    "            match_result = self.openai_analyzer.analyze_query_match(application_keyword, available_applications)\n",
    "\n",
    "            # Lower confidence threshold to 0.3\n",
    "            if match_result.get('confidence', 0) > 0.3:\n",
    "                best_match = match_result.get('best_match')\n",
    "                print(f\"✅ OpenAI found matching application: '{best_match}' (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "\n",
    "                # Find all discoveries containing this application\n",
    "                for discovery in self.discoveries:\n",
    "                    applications = discovery.get('applications', [])\n",
    "                    if any(best_match.lower() in app.lower() for app in applications):\n",
    "                        if discovery not in results:  # Avoid duplicates\n",
    "                            results.append(discovery)\n",
    "            else:\n",
    "                print(f\"❌ OpenAI did not find matching application (confidence: {match_result.get('confidence', 0):.2f})\")\n",
    "                # Even with low confidence, try to return alternative matches\n",
    "                if match_result.get('alternative_matches'):\n",
    "                    print(f\"   Trying alternative matches: {match_result.get('alternative_matches', [])}\")\n",
    "                    for alt_match in match_result.get('alternative_matches', []):\n",
    "                        for discovery in self.discoveries:\n",
    "                            applications = discovery.get('applications', [])\n",
    "                            if any(alt_match.lower() in app.lower() for app in applications):\n",
    "                                if discovery not in results:\n",
    "                                    results.append(discovery)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def smart_search(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Smart search using OpenAI to analyze query intent and select best search strategy\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "\n",
    "        Returns:\n",
    "            Search results and suggestions\n",
    "        \"\"\"\n",
    "        if not self.has_openai:\n",
    "            return {\n",
    "                \"error\": \"OpenAI analyzer not initialized\",\n",
    "                \"suggestion\": \"Please check API key settings\"\n",
    "            }\n",
    "\n",
    "        print(f\"🤖 Starting smart search: '{query}'\")\n",
    "\n",
    "        # Get search strategy suggestion\n",
    "        strategy = self.openai_analyzer.suggest_search_strategy(query, self.discoveries[:5])\n",
    "        print(f\"📋 Search strategy: {strategy.get('explanation', '')}\")\n",
    "\n",
    "        search_type = strategy.get('search_type', 'general')\n",
    "        results = []\n",
    "\n",
    "        if search_type == 'name':\n",
    "            # Search by name\n",
    "            discovery = self.find_discovery_by_name(query, use_openai=True)\n",
    "            if discovery:\n",
    "                results = [discovery]\n",
    "        elif search_type == 'scientist':\n",
    "            # Search by scientist\n",
    "            results = self.find_discoveries_by_scientist(query, use_openai=True)\n",
    "        elif search_type == 'application':\n",
    "            # Search by application\n",
    "            results = self.search_by_application(query, use_openai=True)\n",
    "        else:\n",
    "            # General search - try all methods\n",
    "            print(\"🔍 Executing general search...\")\n",
    "\n",
    "            # Try searching by name\n",
    "            discovery = self.find_discovery_by_name(query, use_openai=True)\n",
    "            if discovery:\n",
    "                results.append(discovery)\n",
    "\n",
    "            # Try searching by scientist\n",
    "            scientist_results = self.find_discoveries_by_scientist(query, use_openai=True)\n",
    "            results.extend([r for r in scientist_results if r not in results])\n",
    "\n",
    "            # Try searching by application\n",
    "            app_results = self.search_by_application(query, use_openai=True)\n",
    "            results.extend([r for r in app_results if r not in results])\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"strategy\": strategy,\n",
    "            \"results\": results,\n",
    "            \"result_count\": len(results),\n",
    "            \"search_type_used\": search_type\n",
    "        }\n",
    "\n",
    "# Test function: verify data structure is correct\n",
    "def test_data_loading():\n",
    "    \"\"\"Test if data loading is correct\"\"\"\n",
    "    try:\n",
    "        query_engine = ScientificDataQuery()\n",
    "        discoveries = query_engine.get_all_discoveries()\n",
    "\n",
    "        print(f\"📊 Successfully loaded {len(discoveries)} scientific discoveries\")\n",
    "\n",
    "        if discoveries:\n",
    "            print(\"\\n📋 Data structure verification:\")\n",
    "            first_discovery = discoveries[0]\n",
    "            print(f\"✅ Primary name: {first_discovery.get('primary_name', 'N/A')}\")\n",
    "            print(f\"✅ Discoverers: {first_discovery.get('discoverers', [])}\")\n",
    "            print(f\"✅ Discovery years: {first_discovery.get('discovery_years', [])}\")\n",
    "            print(f\"✅ Applications: {first_discovery.get('applications', [])}\")\n",
    "            print(f\"✅ URL: {first_discovery.get('url', 'N/A')}\")\n",
    "\n",
    "            # Test exact matching\n",
    "            result = query_engine.find_discovery_by_name(\"CRISPR-Cas9 genome editing technology\")\n",
    "            if result:\n",
    "                print(f\"\\n✅ Exact matching test successful: {result.get('primary_name')}\")\n",
    "            else:\n",
    "                print(\"\\n❌ Exact matching test failed\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"✅ Enhanced ScientificDataQuery class definition completed\")\n",
    "print(\"💡 Run test_data_loading() to test data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756842184389,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "LHCUb_Jt1oSm"
   },
   "outputs": [],
   "source": [
    "# Usage example: Demonstrate enhanced search functionality\n",
    "def demo_enhanced_search():\n",
    "    \"\"\"Demonstrate enhanced search functionality\"\"\"\n",
    "\n",
    "    # Initialize query class (requires OPENAI_API_KEY environment variable)\n",
    "    try:\n",
    "        query_engine = ScientificDataQuery()\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🧪 Demonstrating Enhanced Scientific Discovery Search Functionality\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Example 1: Exact matching\n",
    "        print(\"\\n1️⃣ Exact Matching Example:\")\n",
    "        result = query_engine.find_discovery_by_name(\"CRISPR\")\n",
    "        if result:\n",
    "            print(f\"✅ Found: {result.get('primary_name')}\")\n",
    "        else:\n",
    "            print(\"❌ Not found\")\n",
    "\n",
    "        # Example 2: Fuzzy matching (requires OpenAI)\n",
    "        print(\"\\n2️⃣ Fuzzy Matching Example:\")\n",
    "        result = query_engine.find_discovery_by_name(\"gene editing\")  # English query\n",
    "        if result:\n",
    "            print(f\"✅ Found: {result.get('primary_name')}\")\n",
    "        else:\n",
    "            print(\"❌ Not found\")\n",
    "\n",
    "        # Example 3: Smart search\n",
    "        print(\"\\n3️⃣ Smart Search Example:\")\n",
    "        smart_result = query_engine.smart_search(\"Einstein\")\n",
    "        print(f\"Search result count: {smart_result.get('result_count', 0)}\")\n",
    "        if smart_result.get('results'):\n",
    "            for i, discovery in enumerate(smart_result['results'][:2], 1):\n",
    "                print(f\"  {i}. {discovery.get('primary_name', 'Unknown')}\")\n",
    "\n",
    "        # Example 4: Search by application\n",
    "        print(\"\\n4️⃣ Application Search Example:\")\n",
    "        app_results = query_engine.search_by_application(\"medicine\")\n",
    "        print(f\"Found {len(app_results)} related discoveries\")\n",
    "        for i, discovery in enumerate(app_results[:2], 1):\n",
    "            print(f\"  {i}. {discovery.get('primary_name', 'Unknown')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Demo failed: {e}\")\n",
    "        print(\"💡 Please ensure OPENAI_API_KEY environment variable is set\")\n",
    "\n",
    "# Run demo (uncomment to run)\n",
    "# demo_enhanced_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8auVGAjNh4"
   },
   "source": [
    "***3.2 Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756842184418,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "D5EpLGIBjNh4",
    "outputId": "0f8875e8-c851-4baf-b9bd-24ae42796bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Part 3 function definitions completed\n"
     ]
    }
   ],
   "source": [
    "def compare_discoveries(discovery1: str, discovery2: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two scientific discoveries\n",
    "\n",
    "    Args:\n",
    "        discovery1: Name of the first discovery\n",
    "        discovery2: Name of the second discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed comparison analysis\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "\n",
    "    # Find the two discoveries\n",
    "    disc1 = query.find_discovery_by_name(discovery1)\n",
    "    disc2 = query.find_discovery_by_name(discovery2)\n",
    "\n",
    "    if not disc1:\n",
    "        return f\"❌ Discovery not found: {discovery1}\"\n",
    "    if not disc2:\n",
    "        return f\"❌ Discovery not found: {discovery2}\"\n",
    "\n",
    "    # Build comparison analysis\n",
    "    comparison = f\"\"\"\n",
    "    🔬 Scientific Discovery Comparison\n",
    "    {'='*50}\n",
    "\n",
    "    📍 Discovery 1: {disc1['primary_name']}\n",
    "    📍 Discovery 2: {disc2['primary_name']}\n",
    "\n",
    "    👥 Discoverers:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['discoverers'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['discoverers'])}\n",
    "\n",
    "    📅 Discovery Years:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['discovery_years'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['discovery_years'])}\n",
    "\n",
    "    🔧 Mechanisms:\n",
    "    • {disc1['primary_name']}: {disc1['mechanism']}\n",
    "    • {disc2['primary_name']}: {disc2['mechanism']}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['key_features'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['key_features'])}\n",
    "\n",
    "    🎯 Applications:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['applications'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['applications'])}\n",
    "\n",
    "    🏆 Awards:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1.get('awards', ['No award information']))}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2.get('awards', ['No award information']))}\n",
    "\n",
    "    💡 Significance:\n",
    "    • {disc1['primary_name']}: {disc1['significance']}\n",
    "    • {disc2['primary_name']}: {disc2['significance']}\n",
    "\n",
    "    🏛️ Institutions:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1.get('institutions', ['No institution info']))}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2.get('institutions', ['No institution info']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "def get_research_timeline(scientist: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the research timeline of a scientist\n",
    "\n",
    "    Args:\n",
    "        scientist: Scientist name\n",
    "\n",
    "    Returns:\n",
    "        Timeline of the scientist's discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discoveries = query.find_discoveries_by_scientist(scientist)\n",
    "\n",
    "    if not discoveries:\n",
    "        return f\"❌ No discoveries found for scientist '{scientist}'\"\n",
    "\n",
    "    # Sort by discovery year\n",
    "    sorted_discoveries = sorted(discoveries,\n",
    "                                key=lambda x: int(x['discovery_years'][0]) if x['discovery_years'] else 0)\n",
    "\n",
    "    timeline = f\"\"\"\n",
    "    👨‍🔬 Research Timeline of {scientist}\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(sorted_discoveries, 1):\n",
    "        timeline += f\"\"\"\n",
    "    🔬 Discovery {i}: {discovery['primary_name']}\n",
    "    📅 Year: {', '.join(discovery['discovery_years'])}\n",
    "    🎯 Significance: {discovery['significance']}\n",
    "    🏛️ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "    🏆 Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def search_by_application(application: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for discoveries by application area\n",
    "\n",
    "    Args:\n",
    "        application: Application keyword\n",
    "\n",
    "    Returns:\n",
    "        List of relevant discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    all_discoveries = query.get_all_discoveries()\n",
    "\n",
    "    application_lower = application.lower()\n",
    "    relevant_discoveries = []\n",
    "\n",
    "    for discovery in all_discoveries:\n",
    "        # Search within applications\n",
    "        for app in discovery.get('applications', []):\n",
    "            if application_lower in app.lower():\n",
    "                relevant_discoveries.append(discovery)\n",
    "                break\n",
    "\n",
    "    if not relevant_discoveries:\n",
    "        return f\"❌ No discoveries found related to '{application}'\"\n",
    "\n",
    "    result = f\"\"\"\n",
    "    🎯 Discoveries related to '{application}'\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(relevant_discoveries, 1):\n",
    "        result += f\"\"\"\n",
    "        🔬 Discovery {i}: {discovery['primary_name']}\n",
    "        👥 Discoverers: {', '.join(discovery['discoverers'])}\n",
    "        📅 Year: {', '.join(discovery['discovery_years'])}\n",
    "        🎯 Applications: {', '.join(discovery['applications'])}\n",
    "        💡 Significance: {discovery['significance']}\n",
    "        \"\"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_discovery_details(discovery_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information of a specific discovery\n",
    "\n",
    "    Args:\n",
    "        discovery_name: Name of the scientific discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discovery = query.find_discovery_by_name(discovery_name)\n",
    "\n",
    "    if not discovery:\n",
    "        return f\"❌ Discovery not found: {discovery_name}\"\n",
    "\n",
    "    details = f\"\"\"\n",
    "    🔬 {discovery['primary_name']} - Details\n",
    "    {'='*60}\n",
    "\n",
    "    👥 Discoverers: {', '.join(discovery['discoverers'])}\n",
    "\n",
    "    📅 Discovery Years: {', '.join(discovery['discovery_years'])}\n",
    "\n",
    "    🔧 Mechanism:\n",
    "    {discovery['mechanism']}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    {chr(10).join([f\"  • {feature}\" for feature in discovery['key_features']])}\n",
    "\n",
    "    🎯 Applications:\n",
    "    {chr(10).join([f\"  • {app}\" for app in discovery['applications']])}\n",
    "\n",
    "    💡 Significance:\n",
    "    {discovery['significance']}\n",
    "\n",
    "    🏛️ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "\n",
    "    🏆 Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "\n",
    "    🔗 URL: {discovery.get('url', 'No URL available')}\n",
    "    \"\"\"\n",
    "\n",
    "    return details\n",
    "\n",
    "print(\"✅ Part 3 function definitions completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2IBCB_ojNh5"
   },
   "source": [
    "***3.3 Function Scheme***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756842184423,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "tVCndl8K1oSn",
    "outputId": "ba099d6a-50e0-496b-d2d1-9b0bc9908377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed compare_discoveries function\n"
     ]
    }
   ],
   "source": [
    "# Replace the original compare_discoveries function\n",
    "def compare_discoveries(discovery1: str, discovery2: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two scientific discoveries (Fixed version)\n",
    "\n",
    "    Args:\n",
    "        discovery1: Name of the first discovery\n",
    "        discovery2: Name of the second discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed comparison analysis\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "\n",
    "    # Find the two discoveries\n",
    "    disc1 = query.find_discovery_by_name(discovery1)\n",
    "    disc2 = query.find_discovery_by_name(discovery2)\n",
    "\n",
    "    if not disc1:\n",
    "        return f\"❌ Discovery not found: {discovery1}\"\n",
    "    if not disc2:\n",
    "        return f\"❌ Discovery not found: {discovery2}\"\n",
    "\n",
    "    # Safely join list items, handling None and non-list cases\n",
    "    def safe_join(items, default=\"N/A\"):\n",
    "        if items is None:\n",
    "            return default\n",
    "        if not isinstance(items, list):\n",
    "            return str(items) if items else default\n",
    "        if not items:\n",
    "            return default\n",
    "        return ', '.join(str(item) for item in items)\n",
    "\n",
    "    # Build comparison analysis\n",
    "    comparison = f\"\"\"\n",
    "    🔬 Scientific Discovery Comparison\n",
    "    {'='*50}\n",
    "\n",
    "    📍 Discovery 1: {disc1.get('primary_name', 'Unknown')}\n",
    "    📍 Discovery 2: {disc2.get('primary_name', 'Unknown')}\n",
    "\n",
    "    👥 Discoverers:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('discoverers'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('discoverers'))}\n",
    "\n",
    "    📅 Discovery Years:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('discovery_years'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('discovery_years'))}\n",
    "\n",
    "    🔧 Mechanisms:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {disc1.get('mechanism', 'N/A')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {disc2.get('mechanism', 'N/A')}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('key_features'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('key_features'))}\n",
    "\n",
    "    🎯 Applications:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('applications'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('applications'))}\n",
    "\n",
    "    🏆 Awards:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('awards'), 'No award information')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('awards'), 'No award information')}\n",
    "\n",
    "    💡 Significance:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {disc1.get('significance', 'N/A')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {disc2.get('significance', 'N/A')}\n",
    "\n",
    "    🏛️ Institutions:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('institutions'), 'No institution info')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('institutions'), 'No institution info')}\n",
    "    \"\"\"\n",
    "\n",
    "    return comparison\n",
    "\n",
    "print(\"✅ Fixed compare_discoveries function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756842184425,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "LysKzxaVjNh5"
   },
   "outputs": [],
   "source": [
    "FUNCTION_SCHEMAS = [\n",
    "    {\n",
    "        \"name\": \"compare_discoveries\",\n",
    "        \"description\": \"Compare two scientific breakthroughs in detail. Accepts discovery names, keywords, or partial names (e.g., 'CRISPR', 'gene editing', 'mRNA', 'vaccine', 'gravitational waves'). Supports fuzzy matching in English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery1\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"First scientific discovery name or keyword (supports full name, abbreviation, partial match, or English/Chinese keyword)\"\n",
    "                },\n",
    "                \"discovery2\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Second scientific discovery name or keyword (supports full name, abbreviation, partial match, or English/Chinese keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery1\", \"discovery2\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_research_timeline\",\n",
    "        \"description\": \"Retrieve the chronological timeline of a scientist's major discoveries. Accepts names in full, partial, or last name only (e.g., 'Jennifer Doudna', 'Doudna', 'Einstein'). Supports fuzzy matching in English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"scientist\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientist's name or keyword (supports full name, partial name, or last name in English/Chinese)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"scientist\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_by_application\",\n",
    "        \"description\": \"Search for scientific discoveries by their application domain or field of use. Accepts keywords in English or Chinese (e.g., 'medicine', 'agriculture', 'biotechnology').\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"application\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Application domain keyword (e.g., 'medicine', 'agriculture', 'biotechnology', 'materials science')\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"application\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_discovery_details\",\n",
    "        \"description\": \"Retrieve comprehensive details about a specific scientific discovery. Accepts discovery names, abbreviations, or keywords (e.g., 'CRISPR', 'gene editing', 'mRNA', 'vaccine'). Supports fuzzy matching in English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientific discovery name or keyword (supports partial name, abbreviation, or English/Chinese keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery_name\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJWvrdXdjNh5"
   },
   "source": [
    "***3.4 Assistant***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1756842184446,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "W4I_kVHOjNh5"
   },
   "outputs": [],
   "source": [
    "class ScientificResearchAssistant:\n",
    "    \"\"\"Interactive scientific discovery research assistant\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize assistant\"\"\"\n",
    "        self.client = None\n",
    "\n",
    "        # Check API key (supports interactive input)\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            try:\n",
    "                api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "            except Exception:\n",
    "                api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "\n",
    "        if api_key:\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "            print(\"✅ Scientific research assistant initialized successfully\")\n",
    "        else:\n",
    "            print(\"⚠️ OpenAI API key not set, functionality will be limited\")\n",
    "\n",
    "        # Available function mapping\n",
    "        self.available_functions = {\n",
    "            \"compare_discoveries\": compare_discoveries,\n",
    "            \"get_research_timeline\": get_research_timeline,\n",
    "            \"search_by_application\": search_by_application,\n",
    "            \"get_discovery_details\": get_discovery_details\n",
    "        }\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Chat with assistant\"\"\"\n",
    "\n",
    "        if not self.client:\n",
    "            return \"❌ OpenAI API not configured, unable to perform intelligent conversation\"\n",
    "\n",
    "        try:\n",
    "            # Call OpenAI API with Function Calling\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\":\"\"\"\n",
    "                        You are a professional scientific discovery research assistant. You can help users:\n",
    "                        1. Compare different scientific discoveries\n",
    "                        2. View scientists' research timelines\n",
    "                        3. Search discoveries by application field\n",
    "                        4. Get detailed information about specific discoveries\n",
    "                        Please select the appropriate function to answer based on the user's question.\"\"\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                functions=FUNCTION_SCHEMAS,\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "\n",
    "            # Check if function call is needed\n",
    "            if message.function_call:\n",
    "                function_name = message.function_call.name\n",
    "                function_args = json.loads(message.function_call.arguments)\n",
    "\n",
    "                print(f\"🔧 Calling function: {function_name}\")\n",
    "                print(f\"📝 Parameters: {function_args}\")\n",
    "\n",
    "                # Execute function\n",
    "                if function_name in self.available_functions:\n",
    "                    result = self.available_functions[function_name](**function_args)\n",
    "                    return result\n",
    "                else:\n",
    "                    return f\"❌ Unknown function: {function_name}\"\n",
    "\n",
    "            else:\n",
    "                return message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error occurred while processing request: {str(e)}\"\n",
    "\n",
    "    def show_available_data(self) -> str:\n",
    "        \"\"\"Show available data\"\"\"\n",
    "        query = ScientificDataQuery()\n",
    "        discoveries = query.get_all_discoveries()\n",
    "\n",
    "        if not discoveries:\n",
    "            return \"❌ No available data\"\n",
    "\n",
    "        result = \"📚 Available Scientific Discovery Data:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "\n",
    "        for i, discovery in enumerate(discoveries, 1):\n",
    "            result += f\"{i}. {discovery['primary_name']}\\n\"\n",
    "            result += f\"   Discoverers: {', '.join(discovery['discoverers'])}\\n\"\n",
    "            result += f\"   Year: {', '.join(discovery['discovery_years'])}\\n\\n\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T6ETPfTjNh5"
   },
   "source": [
    "***Instance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756842184448,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "y_-Fs_Y4jNh5"
   },
   "outputs": [],
   "source": [
    "def assistant_chat(example_queries: List[str]):\n",
    "    \"\"\"Main function demonstration\"\"\"\n",
    "    print(\"🔬 Interactive Scientific Discovery Research Assistant\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Initialize assistant\n",
    "    assistant = ScientificResearchAssistant()\n",
    "\n",
    "    # Show available data\n",
    "    print(assistant.show_available_data())\n",
    "\n",
    "    if example_queries !=[]:\n",
    "        print(\"✅ Using provided example queries\")\n",
    "    else:\n",
    "        print(\"✅ No example queries provided, using default examples\")\n",
    "        example_queries = [\n",
    "            \"Please compare the scientific discoveries of CRISPR and quantum computing\",\n",
    "            \"What are the major research discoveries of Jennifer Doudna?\",\n",
    "            \"Search for scientific discoveries applied in the field of medicine\",\n",
    "            \"Give me a detailed introduction to CRISPR technology\"\n",
    "        ]\n",
    "        for q in example_queries:\n",
    "            print(\"Example question:\", q)\n",
    "\n",
    "    # Conduct conversation\n",
    "    for q in example_queries:\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer:\", assistant.chat(q))\n",
    "\n",
    "    return assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y63DDOU4yRoa"
   },
   "source": [
    "# Part 4: Integration and Demonstration\n",
    "\n",
    "### 🎯 Objective\n",
    "Create a comprehensive demo that shows how all components work together, including:\n",
    "- Full end-to-end workflow\n",
    "- Demonstration of all main features\n",
    "- Error handling example\n",
    "- Data visualization\n",
    "- Executable complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3fp0MUC1oSo"
   },
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756842184456,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "k_U5Cfxu1oSo",
    "outputId": "d1394e2e-da29-42c9-ce19-5dcda11e9678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ compare_discoveries function has been fixed and translated to English\n"
     ]
    }
   ],
   "source": [
    "# Replace the original compare_discoveries function with English version\n",
    "def compare_discoveries(discovery1: str, discovery2: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two scientific discoveries (Fixed version)\n",
    "\n",
    "    Args:\n",
    "        discovery1: Name of the first discovery\n",
    "        discovery2: Name of the second discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed comparison analysis\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "\n",
    "    # Find the two discoveries\n",
    "    disc1 = query.find_discovery_by_name(discovery1)\n",
    "    disc2 = query.find_discovery_by_name(discovery2)\n",
    "\n",
    "    if not disc1:\n",
    "        return f\"❌ Discovery not found: {discovery1}\"\n",
    "    if not disc2:\n",
    "        return f\"❌ Discovery not found: {discovery2}\"\n",
    "\n",
    "    # Safely join list items, handling None and non-list cases\n",
    "    def safe_join(items, default=\"N/A\"):\n",
    "        if items is None:\n",
    "            return default\n",
    "        if not isinstance(items, list):\n",
    "            return str(items) if items else default\n",
    "        if not items:\n",
    "            return default\n",
    "        return ', '.join(str(item) for item in items)\n",
    "\n",
    "    # Build comparison analysis\n",
    "    comparison = f\"\"\"\n",
    "    🔬 Scientific Discovery Comparison\n",
    "    {'='*50}\n",
    "\n",
    "    📍 Discovery 1: {disc1.get('primary_name', 'Unknown')}\n",
    "    📍 Discovery 2: {disc2.get('primary_name', 'Unknown')}\n",
    "\n",
    "    👥 Discoverers:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('discoverers'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('discoverers'))}\n",
    "\n",
    "    📅 Discovery Years:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('discovery_years'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('discovery_years'))}\n",
    "\n",
    "    🔧 Mechanisms:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {disc1.get('mechanism', 'N/A')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {disc2.get('mechanism', 'N/A')}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('key_features'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('key_features'))}\n",
    "\n",
    "    🎯 Applications:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('applications'))}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('applications'))}\n",
    "\n",
    "    🏆 Awards:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('awards'), 'No award information')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('awards'), 'No award information')}\n",
    "\n",
    "    💡 Significance:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {disc1.get('significance', 'N/A')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {disc2.get('significance', 'N/A')}\n",
    "\n",
    "    🏛️ Institutions:\n",
    "    • {disc1.get('primary_name', 'Unknown')}: {safe_join(disc1.get('institutions'), 'No institution info')}\n",
    "    • {disc2.get('primary_name', 'Unknown')}: {safe_join(disc2.get('institutions'), 'No institution info')}\n",
    "    \"\"\"\n",
    "\n",
    "    return comparison\n",
    "\n",
    "print(\"✅ compare_discoveries function has been fixed and translated to English\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1756842184468,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "v6roeW_X1oSo"
   },
   "outputs": [],
   "source": [
    "# Enhanced timeline visualization function\n",
    "def create_discovery_timeline_enhanced(query_engine):\n",
    "    \"\"\"Create discovery timeline chart - X-axis: Time, Y-axis: Number of participants, Labels: Scientific discovery titles\"\"\"\n",
    "    print(\"📅 Creating scientific discovery timeline chart...\")\n",
    "\n",
    "    try:\n",
    "        # Prepare data (from structured results)\n",
    "        discoveries = query_engine.discoveries\n",
    "        years = []\n",
    "        participant_counts = []\n",
    "        titles = []\n",
    "\n",
    "\n",
    "        def parse_year(value: str):\n",
    "            if not value:\n",
    "                return None\n",
    "            m = re.search(r\"\\b(1|2)\\d{3}\\b\", str(value))\n",
    "            return int(m.group(0)) if m else None\n",
    "\n",
    "        for discovery in discoveries:\n",
    "            # Parse year\n",
    "            years_raw = discovery.get('discovery_years') or []\n",
    "            y = parse_year(years_raw[0] if years_raw else \"\")\n",
    "            if y is None:\n",
    "                continue\n",
    "\n",
    "            # Get number of participants\n",
    "            discoverers = discovery.get('discoverers', [])\n",
    "            participant_count = len(discoverers) if discoverers else 0\n",
    "\n",
    "            # Get scientific discovery title\n",
    "            title = discovery.get('primary_name', 'Unknown')\n",
    "\n",
    "            years.append(y)\n",
    "            participant_counts.append(participant_count)\n",
    "            titles.append(title)\n",
    "\n",
    "        if not years:\n",
    "            print(\"❌ No valid time data found\")\n",
    "            return\n",
    "\n",
    "        # Create timeline chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "\n",
    "        # Create scatter plot, X-axis: years, Y-axis: number of participants\n",
    "        scatter = plt.scatter(years, participant_counts,\n",
    "                            s=120, alpha=0.7, c='skyblue',\n",
    "                            edgecolors='navy', linewidth=2)\n",
    "\n",
    "        # Add annotations\n",
    "        for i, (year, count, title) in enumerate(zip(years, participant_counts, titles)):\n",
    "            # Adjust annotation position to avoid overlap\n",
    "            offset_x = 2 if i % 2 == 0 else -2\n",
    "            offset_y = 0.1 if i % 2 == 0 else -0.1\n",
    "\n",
    "            plt.annotate(f\"{title[:25]}{'...' if len(title) > 25 else ''}\",\n",
    "                       (year, count),\n",
    "                       xytext=(offset_x, offset_y),\n",
    "                       textcoords='offset points',\n",
    "                       fontsize=9,\n",
    "                       ha='left' if offset_x > 0 else 'right',\n",
    "                       va='bottom' if offset_y > 0 else 'top',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3',\n",
    "                               facecolor='white',\n",
    "                               alpha=0.8,\n",
    "                               edgecolor='gray'))\n",
    "\n",
    "        plt.xlabel('Discovery Year', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Number of Participants', fontsize=12, fontweight='bold')\n",
    "        plt.title('Scientific Discovery Timeline - Participant Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "        # Set grid\n",
    "        plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "        # Set axis range\n",
    "        if years:\n",
    "            plt.xlim(min(years) - 5, max(years) + 5)\n",
    "        if participant_counts:\n",
    "            plt.ylim(0, max(participant_counts) + 1)\n",
    "\n",
    "        # Add statistics\n",
    "        total_discoveries = len(years)\n",
    "        avg_participants = sum(participant_counts) / len(participant_counts) if participant_counts else 0\n",
    "\n",
    "        plt.text(0.02, 0.98, f'Total Discoveries: {total_discoveries}\\nAverage Participants: {avg_participants:.1f}',\n",
    "                transform=plt.gca().transAxes,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✅ Timeline chart created successfully\")\n",
    "        print(f\"📊 Statistics: {total_discoveries} discoveries, average {avg_participants:.1f} participants\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Timeline chart creation failed: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1756842184485,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "Q5MXr9sN1oSo",
    "outputId": "da394235-1302-4bc3-fbc2-438fbdecc2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced discoverer network chart function defined\n",
      "💡 Run test_discoverer_network() to test the new discoverer network chart\n"
     ]
    }
   ],
   "source": [
    "# Enhanced discoverer network visualization function\n",
    "def create_discoverer_network_enhanced(query_engine):\n",
    "    \"\"\"Create discoverer network chart\"\"\"\n",
    "    print(\"👥 Creating discoverer network chart...\")\n",
    "\n",
    "    try:\n",
    "        # Count discoverers\n",
    "        discoverer_count = {}\n",
    "        for discovery in query_engine.discoveries:\n",
    "            for discoverer in discovery.get('discoverers', []):\n",
    "                discoverer_count[discoverer] = discoverer_count.get(discoverer, 0) + 1\n",
    "\n",
    "        # Create bar chart\n",
    "        if discoverer_count:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            names = list(discoverer_count.keys())\n",
    "            counts = list(discoverer_count.values())\n",
    "\n",
    "            # Sort by count for better visualization\n",
    "            sorted_data = sorted(zip(names, counts), key=lambda x: x[1], reverse=True)\n",
    "            names, counts = zip(*sorted_data)\n",
    "\n",
    "            bars = plt.bar(range(len(names)), counts, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "            plt.xlabel('Discoverers', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Number of Discoveries', fontsize=12, fontweight='bold')\n",
    "            plt.title('Discoverer Contribution Statistics', fontsize=14, fontweight='bold')\n",
    "            plt.xticks(range(len(names)), [name.split()[-1] for name in names], rotation=45, ha='right')\n",
    "\n",
    "            # Add value labels\n",
    "            for bar, count in zip(bars, counts):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "            # Add grid\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "            # Add statistics\n",
    "            total_discoverers = len(discoverer_count)\n",
    "            total_discoveries = sum(counts)\n",
    "            avg_discoveries = total_discoveries / total_discoverers if total_discoverers > 0 else 0\n",
    "\n",
    "            plt.text(0.02, 0.98, f'Total Discoverers: {total_discoverers}\\nTotal Discoveries: {total_discoveries}\\nAverage per Discoverer: {avg_discoveries:.1f}',\n",
    "                    transform=plt.gca().transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"✅ Discoverer network chart created successfully\")\n",
    "            print(f\"📊 Statistics: {total_discoverers} discoverers, {total_discoveries} total discoveries\")\n",
    "        else:\n",
    "            print(\"⚠️ No discoverer data available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Discoverer network chart creation failed: {str(e)}\")\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test the enhanced discoverer network chart\n",
    "def test_discoverer_network():\n",
    "    \"\"\"Test the enhanced discoverer network chart\"\"\"\n",
    "    try:\n",
    "        query_engine = ScientificDataQuery()\n",
    "        create_discoverer_network_enhanced(query_engine)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"✅ Enhanced discoverer network chart function defined\")\n",
    "print(\"💡 Run test_discoverer_network() to test the new discoverer network chart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1756842184496,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "m01zfgPy1oSo",
    "outputId": "0ac7db04-ee9a-47d9-d60c-57b440f441e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Discovery timeline chart function defined\n",
      "💡 Run test_discovery_timeline() to test the timeline chart\n",
      "💡 Or use create_discovery_timeline_chart(query_engine, 'Discovery Name') for specific discovery\n"
     ]
    }
   ],
   "source": [
    "# Discovery timeline visualization function\n",
    "def create_discovery_timeline_chart(query_engine, discovery_name: str = None):\n",
    "    \"\"\"Create discovery timeline chart for a specific discovery or all discoveries\n",
    "\n",
    "    Args:\n",
    "        query_engine: ScientificDataQuery instance\n",
    "        discovery_name: Specific discovery name to plot, if None plots all discoveries\n",
    "    \"\"\"\n",
    "    print(\"📅 Creating discovery timeline chart...\")\n",
    "\n",
    "    try:\n",
    "        discoveries_to_plot = []\n",
    "\n",
    "        if discovery_name:\n",
    "            # Find specific discovery\n",
    "            discovery = query_engine.find_discovery_by_name(discovery_name)\n",
    "            if discovery:\n",
    "                discoveries_to_plot = [discovery]\n",
    "                print(f\"📊 Plotting timeline for: {discovery_name}\")\n",
    "            else:\n",
    "                print(f\"❌ Discovery '{discovery_name}' not found\")\n",
    "                return\n",
    "        else:\n",
    "            # Plot all discoveries that have timeline data\n",
    "            discoveries_to_plot = [d for d in query_engine.discoveries if d.get('discovery_timeline')]\n",
    "            print(f\"📊 Plotting timelines for {len(discoveries_to_plot)} discoveries\")\n",
    "\n",
    "        if not discoveries_to_plot:\n",
    "            print(\"⚠️ No discoveries with timeline data found\")\n",
    "            return\n",
    "\n",
    "        # Create subplots if multiple discoveries\n",
    "        if len(discoveries_to_plot) > 1:\n",
    "            fig, axes = plt.subplots(len(discoveries_to_plot), 1, figsize=(15, 4 * len(discoveries_to_plot)))\n",
    "            if len(discoveries_to_plot) == 1:\n",
    "                axes = [axes]\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "            axes = [ax]\n",
    "\n",
    "        for idx, discovery in enumerate(discoveries_to_plot):\n",
    "            timeline = discovery.get('discovery_timeline', [])\n",
    "            if not timeline:\n",
    "                continue\n",
    "\n",
    "            # Parse timeline data\n",
    "            years = []\n",
    "            events = []\n",
    "\n",
    "            for event in timeline:\n",
    "                # Extract year from timeline entry (format: \"YYYY: Event description\")\n",
    "                if ':' in event:\n",
    "                    year_part = event.split(':')[0].strip()\n",
    "                    # Extract year (handle cases like \"1987\" or \"2000s\")\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', year_part)\n",
    "                    if year_match:\n",
    "                        year = int(year_match.group())\n",
    "                        years.append(year)\n",
    "                        events.append(event)\n",
    "\n",
    "            if not years:\n",
    "                print(f\"⚠️ No valid years found in timeline for {discovery.get('primary_name', 'Unknown')}\")\n",
    "                continue\n",
    "\n",
    "            # Sort by year\n",
    "            sorted_data = sorted(zip(years, events))\n",
    "            years, events = zip(*sorted_data)\n",
    "\n",
    "            # Create line plot\n",
    "            ax = axes[idx]\n",
    "            ax.plot(years, range(len(years)), marker='o', linewidth=2, markersize=6,\n",
    "                   color='steelblue', markerfacecolor='lightblue', markeredgecolor='darkblue')\n",
    "\n",
    "            # Customize the plot\n",
    "            ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Timeline Events', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Discovery Timeline: {discovery.get(\"primary_name\", \"Unknown\")}',\n",
    "                        fontsize=14, fontweight='bold')\n",
    "\n",
    "            # Add event labels\n",
    "            for i, (year, event) in enumerate(zip(years, events)):\n",
    "                ax.annotate(f'{year}: {event[:50]}{\"...\" if len(event) > 50 else \"\"}',\n",
    "                           (year, i), xytext=(10, 10), textcoords='offset points',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'),\n",
    "                           fontsize=9, ha='left')\n",
    "\n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "            # Set y-axis to show event numbers\n",
    "            ax.set_yticks(range(len(events)))\n",
    "            ax.set_yticklabels([f'Event {i+1}' for i in range(len(events))])\n",
    "\n",
    "            # Add statistics\n",
    "            timeline_span = max(years) - min(years) if len(years) > 1 else 0\n",
    "            ax.text(0.02, 0.98, f'Timeline Span: {timeline_span} years\\nTotal Events: {len(events)}',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✅ Discovery timeline chart created successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Discovery timeline chart creation failed: {str(e)}\")\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test function for discovery timeline\n",
    "def test_discovery_timeline():\n",
    "    \"\"\"Test the discovery timeline chart\"\"\"\n",
    "    try:\n",
    "        query_engine = ScientificDataQuery()\n",
    "\n",
    "        # Test with specific discovery\n",
    "        print(\"🔍 Testing with CRISPR discovery...\")\n",
    "        create_discovery_timeline_chart(query_engine, \"CRISPR\")\n",
    "\n",
    "        # Test with all discoveries\n",
    "        print(\"\\n🔍 Testing with all discoveries...\")\n",
    "        create_discovery_timeline_chart(query_engine)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"✅ Discovery timeline chart function defined\")\n",
    "print(\"💡 Run test_discovery_timeline() to test the timeline chart\")\n",
    "print(\"💡 Or use create_discovery_timeline_chart(query_engine, 'Discovery Name') for specific discovery\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1756842304528,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "ivyqLP2zyRoa"
   },
   "outputs": [],
   "source": [
    "# determine your researching topic\n",
    "search_terms = [\n",
    "        \"CRISPR\",\n",
    "        # \"RNA vaccine\",\n",
    "        # \"Gravitational wave\",\n",
    "        # \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        # \"Ancient DNA\",\n",
    "        # \"Water on Mars\",\n",
    "        # \"Penicillin\"\n",
    "        ]\n",
    "\n",
    "# determine your queries\n",
    "example_queries = [\n",
    "    \"Please compare the scientific discoveries of CRISPR and quantum computing.\",\n",
    "    # \"What are the major research discoveries of Jennifer Doudna?\",\n",
    "    # \"Search for scientific discoveries applied in the field of medicine.\",\n",
    "    # \"Give me a detailed introduction to CRISPR technology.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5415,
     "status": "aborted",
     "timestamp": 1756842221421,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "mlivBX99yRoa"
   },
   "outputs": [],
   "source": [
    "class ComprehensiveDemo:\n",
    "    \"\"\"Comprehensive demonstration class - shows a complete end-to-end workflow\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the demo system\"\"\"\n",
    "        self.scraper_result = None\n",
    "        self.extractor_result = None\n",
    "        self.assistant = None\n",
    "        self.query = None\n",
    "\n",
    "        print(\"🚀 Comprehensive demo system initialized\")\n",
    "\n",
    "    async def run_complete_pipeline(self):\n",
    "        \"\"\"Run the full end-to-end pipeline\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"🔬 Scientific Discovery Research System - Full Demo\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        try:\n",
    "            # Part 1: Perform data scraping (if search_terms is empty, use default terms)\n",
    "            self.scraper_result = await scrape_scientific_discoveries(search_terms)\n",
    "\n",
    "            # with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "            #     data = json.load(f)\n",
    "            # self.scraper_result = data\n",
    "\n",
    "            # # Part 2: Perform structured data extraction (based on scraping results or existing file)\n",
    "            self.extractor_result = perform_structured_extraction(self.scraper_result)\n",
    "\n",
    "            # Initialize query object for visualization and assistant usage\n",
    "            self.query = ScientificDataQuery(data=self.extractor_result)\n",
    "\n",
    "            # Part 3: Initialize interactive assistant and run sample queries\n",
    "            self.assistant = assistant_chat(example_queries)\n",
    "\n",
    "            # Visualization demo\n",
    "            self._demo_visualization()\n",
    "\n",
    "            print(\"\\n✅ Full demo completed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error occurred during demo: {str(e)}\")\n",
    "\n",
    "    def _demo_visualization(self):\n",
    "        \"\"\"Demonstrate data visualization\"\"\"\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if not self.query or not self.query.discoveries:\n",
    "            print(\"❌ No data available for visualization\")\n",
    "            return\n",
    "\n",
    "        # Create data visualizations\n",
    "        create_discovery_timeline_enhanced(self.query)\n",
    "        create_discoverer_network_enhanced(self.query)\n",
    "        create_discovery_timeline_chart(self.query)\n",
    "\n",
    "\n",
    "print(\"✅ ComprehensiveDemo class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 81338,
     "status": "ok",
     "timestamp": 1756842304509,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "jZ836xTmyRob",
    "outputId": "ae2814e1-5ecd-4f68-f9c6-d219f96d4dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Starting Part 4: Integration and Demonstration\n",
      "============================================================\n",
      "🚀 Comprehensive demo system initialized\n",
      "================================================================================\n",
      "🔬 Scientific Discovery Research System - Full Demo\n",
      "================================================================================\n",
      "🚀 Starting to scrape scientific discovery related Wikipedia articles...\n",
      "============================================================\n",
      "✅ Generated 8 Wikipedia URLs\n",
      "['https://en.wikipedia.org/wiki/CRISPR', 'https://en.wikipedia.org/wiki/RNA_vaccine', 'https://en.wikipedia.org/wiki/Gravitational_wave', 'https://en.wikipedia.org/wiki/Higgs_boson', 'https://en.wikipedia.org/wiki/Quantum_computing', 'https://en.wikipedia.org/wiki/Ancient_DNA', 'https://en.wikipedia.org/wiki/Water_on_Mars', 'https://en.wikipedia.org/wiki/Penicillin']\n",
      "Validation passed, 8 valid URLs found\n",
      "Starting batch scrape for 8 articles\n",
      "Progress: 1/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/CRISPR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">28s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m28s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">53s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m53s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">81s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m81s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: CRISPR\n",
      "Waiting 2 seconds...\n",
      "Progress: 2/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/RNA_vaccine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">18s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m18s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">74s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m74s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">93s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m93s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: mRNA vaccine\n",
      "Waiting 2 seconds...\n",
      "Progress: 3/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Gravitational_wave\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">27s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m27s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">46s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m46s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">74s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m74s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Gravitational wave\n",
      "Waiting 2 seconds...\n",
      "Progress: 4/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Higgs_boson\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">99s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m99s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">62s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m62s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">63s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m63s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Higgs boson\n",
      "Waiting 2 seconds...\n",
      "Progress: 5/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Quantum_computing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">36s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m36s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">85s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m85s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">22s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m22s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Quantum computing\n",
      "Waiting 2 seconds...\n",
      "Progress: 6/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Ancient_DNA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">10s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m10s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">28s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m28s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">39s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m39s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Ancient DNA\n",
      "Waiting 2 seconds...\n",
      "Progress: 7/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Water_on_Mars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">73s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m73s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">57s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m57s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">31s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m31s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Water on Mars\n",
      "Waiting 2 seconds...\n",
      "Progress: 8/8\n",
      "Starting to scrape article: https://en.wikipedia.org/wiki/Penicillin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">76s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m76s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">48s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m48s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">25s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m25s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped article: Penicillin\n",
      "Batch scrape completed, successfully scraped 8 articles\n",
      "Full data saved to: ./scientific_discoveries.json\n",
      "Main content saved to: ./content.md\n",
      "Section information saved to: ./section.md\n",
      "\n",
      "📊 Scraping Results Summary:\n",
      "========================================\n",
      "total_articles: 8\n",
      "successful_scrapes: 8\n",
      "failed_scrapes: 0\n",
      "total_content_length: 367048\n",
      "average_content_length: 45881\n",
      "success_rate: 100.0\n",
      "\n",
      "🎉 Scraping completed! Retrieved data for 8 articles\n",
      "🚀 Starting structured data extraction...\n",
      "============================================================\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "🔬 Interactive Scientific Discovery Research Assistant\n",
      "==================================================\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ Scientific research assistant initialized successfully\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "❌ No available data\n",
      "✅ Using provided example queries\n",
      "Question: Please compare the scientific discoveries of CRISPR and quantum computing.\n",
      "🔧 Calling function: compare_discoveries\n",
      "📝 Parameters: {'discovery1': 'CRISPR', 'discovery2': 'quantum computing'}\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "🔍 Exact matching failed, trying OpenAI intelligent matching: 'CRISPR'\n",
      "❌ OpenAI did not find high confidence match (confidence: 0.00)\n",
      "🔍 Exact matching failed, trying OpenAI intelligent matching: 'quantum computing'\n",
      "❌ OpenAI did not find high confidence match (confidence: 0.00)\n",
      "Answer: ❌ Discovery not found: CRISPR\n",
      "Question: What are the major research discoveries of Jennifer Doudna?\n",
      "🔧 Calling function: get_research_timeline\n",
      "📝 Parameters: {'scientist': 'Jennifer Doudna'}\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "🔍 Exact matching failed, trying OpenAI intelligent matching for scientist: 'Jennifer Doudna'\n",
      "❌ OpenAI did not find high confidence matching scientist (confidence: 0.00)\n",
      "Answer: ❌ No discoveries found for scientist 'Jennifer Doudna'\n",
      "Question: Search for scientific discoveries applied in the field of medicine.\n",
      "🔧 Calling function: search_by_application\n",
      "📝 Parameters: {'application': 'medicine'}\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "Answer: ❌ No discoveries found related to 'medicine'\n",
      "Question: Give me a detailed introduction to CRISPR technology.\n",
      "🔧 Calling function: get_discovery_details\n",
      "📝 Parameters: {'discovery_name': 'CRISPR'}\n",
      "Please enter OPENAI_API_KEY (input will be hidden): ··········\n",
      "✅ OpenAI client initialized successfully\n",
      "✅ OpenAI analyzer initialized successfully\n",
      "❌ Error: Data file not found at ./structured_extractions.json\n",
      "🔍 Exact matching failed, trying OpenAI intelligent matching: 'CRISPR'\n",
      "❌ OpenAI did not find high confidence match (confidence: 0.00)\n",
      "Answer: ❌ Discovery not found: CRISPR\n",
      "--------------------------------------------------\n",
      "❌ No data available for visualization\n",
      "\n",
      "✅ Full demo completed!\n",
      "\n",
      "🎉 Part 4 demo completed!\n",
      "============================================================\n",
      "📋 Demo Summary:\n",
      "✅ 1. Full end-to-end workflow\n",
      "✅ 2. Demonstration of all main features\n",
      "✅ 3. Error handling example\n",
      "✅ 4. Data visualization\n",
      "✅ 5. Interactive assistant demo\n"
     ]
    }
   ],
   "source": [
    "# Run the complete Part 4 demo\n",
    "def run_part4_demo():\n",
    "    \"\"\"Run the complete Part 4 demo\"\"\"\n",
    "    print(\"🎬 Starting Part 4: Integration and Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create demo instance\n",
    "    demo = ComprehensiveDemo()\n",
    "\n",
    "    # Run the complete pipeline (this function should be called with await in Jupyter)\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.get_event_loop().run_until_complete(demo.run_complete_pipeline())\n",
    "\n",
    "    print(\"\\n🎉 Part 4 demo completed!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📋 Demo Summary:\")\n",
    "    print(\"✅ 1. Full end-to-end workflow\")\n",
    "    print(\"✅ 2. Demonstration of all main features\")\n",
    "    print(\"✅ 3. Error handling example\")\n",
    "    print(\"✅ 4. Data visualization\")\n",
    "    print(\"✅ 5. Interactive assistant demo\")\n",
    "\n",
    "# Execute the demo\n",
    "run_part4_demo()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
