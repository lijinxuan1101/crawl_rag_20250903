{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG4aG6PBi5JK"
   },
   "source": [
    "# IS5126 Individual Assignment 1 - HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aw-dl5LPMa3"
   },
   "outputs": [],
   "source": [
    "%pip install crawl4ai\n",
    "%pip install beautifulsoup4 lxml requests\n",
    "!playwright install\n",
    "print(\"âœ… All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3GgZE_LPMa4"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import openai\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set font for visualization\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Enable nest_asyncio to support async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# Enable nest_asyncio to support async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkYZwNMmi5JM"
   },
   "source": [
    "## Part 1: Wikipedia Scraper Implementation:\n",
    "â€“ WikipediaScraper class with all required methods\n",
    "â€“ Demonstration of scraping your chosen 5+ Wikipedia articles\n",
    "â€“ Explanation of how your articles relate to your research domain\n",
    "â€“ Error handling examples with sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGJG0bMPi5JO"
   },
   "outputs": [],
   "source": [
    "class WikipediaScraper:\n",
    "    def __init__(self, base_urls: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize Wikipedia scraper\n",
    "\n",
    "        Args:\n",
    "            base_urls: List of Wikipedia article URLs to scrape\n",
    "        \"\"\"\n",
    "        self.base_urls = base_urls\n",
    "        self.scraped_data = []\n",
    "        self.rate_limit_delay = 2  # Request interval (seconds) to avoid overloading Wikipedia servers\n",
    "\n",
    "        # Validate URL format\n",
    "        self._validate_urls()\n",
    "\n",
    "    def _validate_urls(self):\n",
    "        \"\"\"Validate if URLs are valid Wikipedia links\"\"\"\n",
    "        valid_urls = []\n",
    "        for url in self.base_urls:\n",
    "            if self._is_valid_wikipedia_url(url):\n",
    "                valid_urls.append(url)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid Wikipedia URL: {url}\")\n",
    "\n",
    "        if not valid_urls:\n",
    "            raise ValueError(\"No valid Wikipedia URLs provided\")\n",
    "\n",
    "        self.base_urls = valid_urls\n",
    "        print(f\"Validation passed, {len(valid_urls)} valid URLs found\")\n",
    "\n",
    "    def _is_valid_wikipedia_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is a valid Wikipedia URL\"\"\"\n",
    "        try:\n",
    "            return (\n",
    "                'wikipedia.org' in url and\n",
    "                '/wiki/' in url\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _looks_like_html(self, content: str) -> bool:\n",
    "        \"\"\"Determine whether a string looks like HTML\"\"\"\n",
    "        try:\n",
    "            return bool(content and content.lstrip().startswith(\"<\"))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _html_to_text(self, html: str) -> str:\n",
    "        \"\"\"Convert Wikipedia HTML to cleaner plain text (keep title and paragraphs)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "            # Title\n",
    "            title_node = soup.select_one(\"#firstHeading\")\n",
    "            title_text = title_node.get_text(\" \", strip=True) if title_node else \"\"\n",
    "\n",
    "            # Main content container\n",
    "            content = soup.select_one(\"#mw-content-text\") or soup\n",
    "\n",
    "            # Noise selectors to remove\n",
    "            noise_selectors = [\n",
    "                \"#toc\", \".mw-references-wrap\", \"table.infobox\",\n",
    "                \"div.navbox\", \"table.metadata\", \"div.hatnote\",\n",
    "                \"script\", \"style\", \"noscript\",\n",
    "            ]\n",
    "            for sel in noise_selectors:\n",
    "                for node in content.select(sel):\n",
    "                    node.decompose()\n",
    "\n",
    "            # Paragraph text\n",
    "            paragraphs = []\n",
    "            for p in content.select(\"p\"):\n",
    "                text = p.get_text(\" \", strip=True)\n",
    "                text = re.sub(r\"\\s*\\[\\d+\\]\", \"\", text)  # remove footnote like [1]\n",
    "                if text:\n",
    "                    paragraphs.append(text)\n",
    "\n",
    "            combined = ((title_text + \"\\n\\n\") if title_text else \"\") + \"\\n\\n\".join(paragraphs)\n",
    "            combined = re.sub(r\"\\n{3,}\", \"\\n\\n\", combined).strip()\n",
    "            return combined\n",
    "        except Exception:\n",
    "            return html\n",
    "\n",
    "    async def scrape_article(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Scrape a single Wikipedia article\n",
    "\n",
    "        Args:\n",
    "            url: Article URL\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing article information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Starting to scrape article: {url}\")\n",
    "\n",
    "            async with AsyncWebCrawler(verbose=False) as crawler:\n",
    "                # Scrape article content\n",
    "                result = await crawler.arun(url)\n",
    "\n",
    "                if result.success:\n",
    "                    # Extract article information\n",
    "                    article_data = self._extract_article_info(result, url)\n",
    "                    print(f\"Successfully scraped article: {article_data.get('title', 'Unknown')}\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    error_msg = result.error_message if hasattr(result, 'error_message') else 'Unknown error'\n",
    "                    print(f\"Scraping failed: {error_msg}\")\n",
    "                    return self._create_error_response(url, error_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping article {url}: {str(e)}\")\n",
    "            return self._create_error_response(url, str(e))\n",
    "\n",
    "    def _extract_article_info(self, result, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract article info from scraping result\"\"\"\n",
    "        try:\n",
    "            content = result.html\n",
    "\n",
    "            # Extract title\n",
    "            title = self._extract_title(content, url)\n",
    "\n",
    "            # Extract main content\n",
    "            main_content = self._extract_main_content(content)\n",
    "\n",
    "            # Extract key sections\n",
    "            sections = self._extract_sections(content)\n",
    "\n",
    "            # Clean content\n",
    "            cleaned_content = self.clean_content(main_content)\n",
    "\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'main_content': cleaned_content,\n",
    "                'sections': sections,\n",
    "                'raw_content_length': len(content),\n",
    "                'cleaned_content_length': len(cleaned_content),\n",
    "                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'status': 'success'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article info: {str(e)}\")\n",
    "            return self._create_error_response(url, f\"Extraction failed: {str(e)}\")\n",
    "\n",
    "    def _extract_title(self, content: str, url: str) -> str:\n",
    "        \"\"\"Extract article title from HTML content\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Prefer title from h1\n",
    "            h1_title = soup.find('h1')\n",
    "            if h1_title:\n",
    "                title_text = h1_title.get_text(strip=True)\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            # Fallback: page <title>\n",
    "            title_tag = soup.find('title')\n",
    "            if title_tag:\n",
    "                title_text = title_tag.get_text(strip=True)\n",
    "                # Remove suffix like \" - Wikipedia\"\n",
    "                if ' - Wikipedia' in title_text:\n",
    "                    title_text = title_text.split(' - Wikipedia')[0]\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting title: {str(e)}\")\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "    def _extract_main_content(self, content: str) -> str:\n",
    "        \"\"\"Extract main article content from HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            # Remove noisy elements\n",
    "            noise_selectors = [\n",
    "                '#toc', '.mw-references-wrap', 'table.infobox',\n",
    "                'div.navbox', 'table.metadata', 'div.hatnote',\n",
    "                'script', 'style', 'noscript', 'table', 'nav',\n",
    "                '.mw-editsection', '.mw-cite-backlink', '.thumb',\n",
    "                '.image', 'sup.reference', 'span.mw-ref'\n",
    "            ]\n",
    "\n",
    "            for selector in noise_selectors:\n",
    "                for element in main_content.select(selector):\n",
    "                    element.decompose()\n",
    "\n",
    "            # Collect all paragraph text\n",
    "            paragraphs = main_content.find_all('p')\n",
    "            content_list = []\n",
    "\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text) > 10:  # filter out very short paragraphs\n",
    "                    # remove footnote numbers and artifacts\n",
    "                    text = re.sub(r'\\s*\\[\\d+\\]', '', text)\n",
    "                    text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "                    content_list.append(text)\n",
    "\n",
    "            if content_list:\n",
    "                return '\\n\\n'.join(content_list)\n",
    "            else:\n",
    "                return \"Unable to extract valid content\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract content from HTML: {str(e)}\")\n",
    "            return content[:1000]  # return first 1000 chars as fallback\n",
    "\n",
    "    def _extract_sections(self, content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract heading structure from HTML (without content)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            sections = []\n",
    "\n",
    "            # Find all heading tags\n",
    "            headings = main_content.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "            for heading in headings:\n",
    "                level = int(heading.name[1])  # h1->1, h2->2, h3->3\n",
    "                heading_text = heading.get_text(strip=True)\n",
    "\n",
    "                # Skip some headings\n",
    "                if self._should_skip_heading(heading_text):\n",
    "                    continue\n",
    "\n",
    "                sections.append({\n",
    "                    'level': level,\n",
    "                    'heading': heading_text\n",
    "                })\n",
    "\n",
    "            return sections\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract sections from HTML: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _should_skip_heading(self, heading_text: str) -> bool:\n",
    "        \"\"\"Decide whether a heading should be skipped\"\"\"\n",
    "        skip_patterns = [\n",
    "            'Contents', 'References', 'External links', 'Further reading',\n",
    "            'See also', 'Notes', 'Bibliography', 'Sources', 'Citations', 'Footnotes'\n",
    "        ]\n",
    "\n",
    "        for pattern in skip_patterns:\n",
    "            if pattern.lower() in heading_text.lower():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def clean_content(self, raw_content: str) -> str:\n",
    "        \"\"\"Clean and pre-process scraped content\"\"\"\n",
    "        if not raw_content:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            # Remove excessive whitespace and newlines\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', raw_content)\n",
    "            cleaned = re.sub(r'[ \\t]+', ' ', cleaned)\n",
    "\n",
    "            # Remove footnote references\n",
    "            cleaned = re.sub(r'\\s*\\[\\d+\\]', '', cleaned)\n",
    "\n",
    "            # Ensure proper blank lines between paragraphs\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "\n",
    "            return cleaned.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to clean content: {e}\")\n",
    "            return raw_content\n",
    "\n",
    "    async def scrape_all_articles(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Scrape all articles\"\"\"\n",
    "        print(f\"Starting batch scrape for {len(urls)} articles\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                print(f\"Progress: {i+1}/{len(urls)}\")\n",
    "\n",
    "                # Scrape single article\n",
    "                article_data = await self.scrape_article(url)\n",
    "                all_results.append(article_data)\n",
    "\n",
    "                # Rate limit: add delay between requests\n",
    "                if i < len(urls) - 1:  # not the last one\n",
    "                    print(f\"Waiting {self.rate_limit_delay} seconds...\")\n",
    "                    await asyncio.sleep(self.rate_limit_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping article {url}: {str(e)}\")\n",
    "                all_results.append(self._create_error_response(url, str(e)))\n",
    "\n",
    "        self.scraped_data = all_results\n",
    "        print(f\"Batch scrape completed, successfully scraped {len([r for r in all_results if r.get('status') == 'success'])} articles\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def _create_error_response(self, url: str, error_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create an error response\"\"\"\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': 'Error',\n",
    "            'main_content': '',\n",
    "            'sections': [],\n",
    "            'error': error_message,\n",
    "            'status': 'error',\n",
    "            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "    def save_to_json(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save scraped data to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Full data saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "    def save_content_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save main contents of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                main = item.get('main_content', '').strip()\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if main:\n",
    "                    lines.append(main)\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Main content saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving main content Markdown file: {str(e)}\")\n",
    "\n",
    "    def save_sections_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save section information of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                sections = item.get('sections', [])\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if isinstance(sections, list) and sections:\n",
    "                    for s in sections:\n",
    "                        if isinstance(s, dict):\n",
    "                            heading = s.get('heading') or s.get('title') or 'Section'\n",
    "                            content = s.get('content') or s.get('text') or ''\n",
    "                            lines.append(f\"## {heading}\")\n",
    "                            if content:\n",
    "                                lines.append(content)\n",
    "                            lines.append(\"\")\n",
    "                        else:\n",
    "                            lines.append(str(s))\n",
    "                            lines.append(\"\")\n",
    "                else:\n",
    "                    lines.append(\"No sections available\")\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Section information saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving section information Markdown file: {str(e)}\")\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get scraping summary\"\"\"\n",
    "        if not self.scraped_data:\n",
    "            return {\"message\": \"No scraped data\"}\n",
    "\n",
    "        successful = [r for r in self.scraped_data if r.get('status') == 'success']\n",
    "        failed = [r for r in self.scraped_data if r.get('status') == 'error']\n",
    "\n",
    "        total_content_length = sum(r.get('cleaned_content_length', 0) for r in successful)\n",
    "\n",
    "        return {\n",
    "            'total_articles': len(self.scraped_data),\n",
    "            'successful_scrapes': len(successful),\n",
    "            'failed_scrapes': len(failed),\n",
    "            'total_content_length': total_content_length,\n",
    "            'average_content_length': total_content_length // len(successful) if successful else 0,\n",
    "            'success_rate': len(successful) / len(self.scraped_data) * 100\n",
    "        }\n",
    "\n",
    "print(\"âœ… WikipediaScraper class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1Gt1AeCi5JP"
   },
   "outputs": [],
   "source": [
    "# Utility function to convert search keywords to Wikipedia URLs\n",
    "def build_wikipedia_urls(terms):\n",
    "    \"\"\"Generate Wikipedia article URL list from keywords (English articles).\n",
    "    - Rules: Replace spaces with underscores; trim whitespace; deduplicate.\n",
    "    - Note: If keywords are not English or non-standard titles, additional processing/search may be needed.\n",
    "    \"\"\"\n",
    "    if not terms:\n",
    "        return []\n",
    "    base = \"https://en.wikipedia.org/wiki/\"\n",
    "    urls = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        title = t.strip().replace(\" \", \"_\")\n",
    "        if not title:\n",
    "            continue\n",
    "        url = base + title\n",
    "        if url not in seen:\n",
    "            urls.append(url)\n",
    "            seen.add(url)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756824554004,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "Gff1GRT3i5JP",
    "outputId": "f5484163-b39c-4f7f-88ed-73c5d9adb80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®æŠ“å–å‡½æ•°å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "async def scrape_scientific_discoveries(search_terms_list: Optional[List[str]] = None):\n",
    "    \"\"\"Scrape scientific discovery related Wikipedia articles\"\"\"\n",
    "    print(\"ğŸš€ Starting to scrape scientific discovery related Wikipedia articles...\")\n",
    "    print(\"=\" * 60)\n",
    "    if search_terms_list != None:\n",
    "        scientific_urls = build_wikipedia_urls(search_terms_list)\n",
    "        print(f\"âœ… Generated {len(scientific_urls)} Wikipedia URLs\")\n",
    "    else:\n",
    "        search_terms = [\n",
    "        \"CRISPR\",\n",
    "        \"RNA vaccine\",\n",
    "        \"Gravitational wave\",\n",
    "        \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        \"Ancient DNA\",\n",
    "        \"Water on Mars\",\n",
    "        \"Penicillin\"\n",
    "        ]\n",
    "        scientific_urls = build_wikipedia_urls(search_terms)\n",
    "\n",
    "    print(scientific_urls)\n",
    "    scraper = WikipediaScraper(scientific_urls)\n",
    "    results = await scraper.scrape_all_articles(scientific_urls)\n",
    "\n",
    "    # Save data\n",
    "    scraper.save_to_json(results,\"./scientific_discoveries.json\")\n",
    "    scraper.save_content_to_markdown(results,\"./content.md\")\n",
    "    scraper.save_sections_to_markdown(results,\"./section.md\")\n",
    "\n",
    "    # Display summary\n",
    "    summary = scraper.get_summary()\n",
    "    print(\"\\nğŸ“Š Scraping Results Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Scraping completed! Retrieved data for {len(results)} articles\")\n",
    "    return results\n",
    "\n",
    "print(\"âœ… Data scraping function definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 65053,
     "status": "ok",
     "timestamp": 1756827058859,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "QA1LO9gQi5JQ",
    "outputId": "749bc907-3a2a-49cf-a91e-3b9bd038b87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æŠ“å–ç§‘å­¦å‘ç°ç›¸å…³çš„ç»´åŸºç™¾ç§‘æ–‡ç« ...\n",
      "============================================================\n",
      "['https://en.wikipedia.org/wiki/CRISPR', 'https://en.wikipedia.org/wiki/RNA_vaccine', 'https://en.wikipedia.org/wiki/Gravitational_wave', 'https://en.wikipedia.org/wiki/Higgs_boson', 'https://en.wikipedia.org/wiki/Quantum_computing', 'https://en.wikipedia.org/wiki/Ancient_DNA', 'https://en.wikipedia.org/wiki/Water_on_Mars', 'https://en.wikipedia.org/wiki/Penicillin']\n",
      "éªŒè¯é€šè¿‡ï¼Œå…±8ä¸ªæœ‰æ•ˆURL\n",
      "å¼€å§‹æ‰¹é‡çˆ¬å– 8 ç¯‡æ–‡ç« \n",
      "è¿›åº¦: 1/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/CRISPR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">05s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m05s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.</span><span style=\"color: #808000; text-decoration-color: #808000\">7</span><span style=\"color: #008000; text-decoration-color: #008000\">0s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;33m0.\u001b[0m\u001b[33m7\u001b[0m\u001b[32m0s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">76s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m76s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : CRISPR\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 2/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/RNA_vaccine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">63s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m63s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">21s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m21s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">85s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m85s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : mRNA vaccine\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 3/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Gravitational_wave\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">38s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m38s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">14s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m14s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">54s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m54s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Gravitational wave\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 4/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Higgs_boson\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">36s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m36s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">44s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m44s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">80s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m80s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Higgs boson\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 5/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Quantum_computing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">21s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m21s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">71s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m71s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">93s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m93s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Quantum computing\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 6/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Ancient_DNA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">27s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m27s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">62s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m62s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">91s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m91s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Ancient DNA\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 7/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Water_on_Mars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">34s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m34s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">96s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m96s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4.</span><span style=\"color: #008000; text-decoration-color: #008000\">31s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m31s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Water on Mars\n",
      "ç­‰å¾… 2 ç§’...\n",
      "è¿›åº¦: 8/8\n",
      "å¼€å§‹çˆ¬å–æ–‡ç« : https://en.wikipedia.org/wiki/Penicillin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">52s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m52s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">11s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m11s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">66s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m66s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸçˆ¬å–æ–‡ç« : Penicillin\n",
      "æ‰¹é‡çˆ¬å–å®Œæˆï¼ŒæˆåŠŸçˆ¬å– 8 ç¯‡æ–‡ç« \n",
      "å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: ./scientific_discoveries.json\n",
      "ä¸»å†…å®¹å·²ä¿å­˜åˆ°: ./content.md\n",
      "åˆ†èŠ‚ä¿¡æ¯å·²ä¿å­˜åˆ°: ./section.md\n",
      "\n",
      "ğŸ“Š æŠ“å–ç»“æœæ‘˜è¦:\n",
      "========================================\n",
      "total_articles: 8\n",
      "successful_scrapes: 8\n",
      "failed_scrapes: 0\n",
      "total_content_length: 367048\n",
      "average_content_length: 45881\n",
      "success_rate: 100.0\n",
      "\n",
      "ğŸ‰ æŠ“å–å®Œæˆï¼å…±è·å– 8 ç¯‡æ–‡ç« çš„æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "# Execute data scraping - can directly use await in Jupyter\n",
    "scraped_info = await scrape_scientific_discoveries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56kuX5znmOfO"
   },
   "source": [
    "## Part2 Structured Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ErSfBUyuPMa8"
   },
   "outputs": [],
   "source": [
    "# # Simple JSON file reader\n",
    "# import json\n",
    "\n",
    "# # Read the saved JSON data\n",
    "# with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# print(f\"âœ… Successfully read data of {len(data)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "xaiOwWS0PMa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scientific discovery domain Pydantic model definition completed\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model designed for scientific discovery domain\n",
    "class ScientificDiscoveryExtraction(BaseModel):\n",
    "    \"\"\"Structured data model for scientific discovery information\"\"\"\n",
    "\n",
    "    # Basic information\n",
    "    primary_name: str = Field(description=\"Primary name of the scientific discovery or technology\")\n",
    "\n",
    "    # Discoverers and time\n",
    "    discoverers: List[str] = Field(description=\"List of names of main discoverers or researchers\")\n",
    "    discovery_years: List[str] = Field(description=\"Most important discovery year, strongly recommend returning only one year. Must select the single year that best represents the core breakthrough of the technology/discovery from the article. Priority: key technical implementation > important paper publication > initial concept proposal. For example, for CRISPR, choose 2012 (Doudna and Charpentier's key paper) rather than 1987 (first sequence discovery). Avoid returning multiple years.\")\n",
    "    discovery_timeline: List[str] = Field(description=\"Timeline from initial to final discovery\")\n",
    "\n",
    "    # Technical details\n",
    "    mechanism: str = Field(description=\"Basic working principle or mechanism of the technology\")\n",
    "    key_features: List[str] = Field(description=\"Main features or advantages of the technology\")\n",
    "\n",
    "    # Applications and impact\n",
    "    applications: List[str] = Field(description=\"Main application fields or uses\")\n",
    "    significance: str = Field(description=\"Scientific or social significance of the discovery\")\n",
    "\n",
    "    # Optional additional information\n",
    "    institutions: Optional[List[str]] = Field(default=None, description=\"Related research institutions or universities\")\n",
    "    awards: Optional[List[str]] = Field(default=None, description=\"Important awards or honors received\")\n",
    "\n",
    "    # Metadata\n",
    "    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat(), description=\"Data extraction time\")\n",
    "\n",
    "print(\"âœ… Scientific discovery domain Pydantic model definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "9FdWkCKFPMa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Structured data extractor class definition completed\n"
     ]
    }
   ],
   "source": [
    "class StructuredDataExtractor:\n",
    "    \"\"\"A class for structured data extraction using the OpenAI API\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None, model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the extractor\n",
    "\n",
    "        Args:\n",
    "            api_key: OpenAI API key (optional, will use environment variable OPENAI_API_KEY first)\n",
    "            model: Model name to use\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        self.model = model\n",
    "        self.client = None\n",
    "\n",
    "        # Prefer the provided API key, then the environment variable; if not found, ask interactively\n",
    "        final_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not final_api_key:\n",
    "            try:\n",
    "                from getpass import getpass\n",
    "                final_api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "            except Exception:\n",
    "                final_api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "\n",
    "        if final_api_key:\n",
    "            self.client = openai.OpenAI(api_key=final_api_key)\n",
    "            print(\"âœ… OpenAI client initialized successfully\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No OpenAI API key found, related features will be limited\")\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Retry configuration\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2  # seconds\n",
    "\n",
    "    def extract_structured_data(self, content: str, model: str = None) -> Optional[ScientificDiscoveryExtraction]:\n",
    "        \"\"\"\n",
    "        Extract structured data using OpenAI structured output\n",
    "\n",
    "        Args:\n",
    "            content: Text content to extract\n",
    "            model: Model to use (optional)\n",
    "\n",
    "        Returns:\n",
    "            ScientificDiscoveryExtraction: Extracted structured data\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            self.logger.error(\"OpenAI client not initialized, please provide an API key\")\n",
    "            return None\n",
    "\n",
    "        model_to_use = model or self.model\n",
    "\n",
    "        # Build system prompt\n",
    "        system_prompt = \"\"\"\n",
    "        You are a professional scientific literature analyst. Please extract structured information \n",
    "        from the given scientific article content.\n",
    "\n",
    "        Extract the following information:\n",
    "        1. Main name of the scientific discovery or technology\n",
    "        2. Main discoverer(s) or researcher(s)\n",
    "        3. Important discovery time points\n",
    "        4. Timeline of the discovery\n",
    "        5. Basic working principle of the technology\n",
    "        6. Main features and advantages\n",
    "        7. Application areas\n",
    "        8. Scientific significance\n",
    "        9. Related institutions (if any)\n",
    "        10. Awards received (if any)\n",
    "\n",
    "        **Important extraction rules**:\n",
    "        - For discovery_years, follow these rules:\n",
    "          * If a range of years is mentioned (e.g., \"2010-2012\", \"from 2008 to 2011\"), \n",
    "            determine which year is most relevant to the key discovery in the title\n",
    "          * Prefer the year associated with the key breakthrough, first publication, or major experiment success\n",
    "          * If multiple years are mentioned and it's unclear which is more important, return the earliest one\n",
    "          * Example: If the title is \"CRISPR gene-editing technology\" and the text says \n",
    "            \"Research started in 2008, breakthrough in 2012\", choose 2012\n",
    "          * If completely uncertain, return the earliest year in the range\n",
    "\n",
    "        If some information is not explicitly mentioned, try to infer from the context, \n",
    "        or state \"unclear\" in the corresponding field.\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Please extract structured information from the following scientific article content:\n",
    "\n",
    "        {content[:8000]}  # Limit length to avoid token issues\n",
    "\n",
    "        Please extract information according to the ScientificDiscoveryExtraction model.\n",
    "\n",
    "        **Special note on discovery_years**:\n",
    "        - Carefully read the article title and content to understand the core of the scientific discovery\n",
    "        - If a year range is present, decide which year best represents the key breakthrough\n",
    "        - Priority: first major publication, critical technical success, or key experimental result\n",
    "        - If unclear, choose the earliest year in the range\n",
    "        \"\"\"\n",
    "\n",
    "        # Try extraction with retries\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self.logger.info(f\"Starting extraction attempt {attempt + 1}...\")\n",
    "\n",
    "                response = self.client.beta.chat.completions.parse(\n",
    "                    model=model_to_use,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    response_format=ScientificDiscoveryExtraction,\n",
    "                    temperature=0.1  # Low temperature for consistency\n",
    "                )\n",
    "\n",
    "                # Extract structured data\n",
    "                extracted_data = response.choices[0].message.parsed\n",
    "\n",
    "                if extracted_data:\n",
    "                    self.logger.info(\"âœ… Structured data extraction succeeded\")\n",
    "                    return extracted_data\n",
    "                else:\n",
    "                    self.logger.warning(\"âš ï¸ API returned empty data\")\n",
    "\n",
    "            except openai.RateLimitError as e:\n",
    "                self.logger.warning(f\"Rate limit error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (2 ** attempt))  # exponential backoff\n",
    "\n",
    "            except openai.APIError as e:\n",
    "                self.logger.error(f\"OpenAI API error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unknown error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "        self.logger.error(\"âŒ All extraction attempts failed\")\n",
    "        return None\n",
    "\n",
    "    def batch_extract(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process multiple articles in batch\n",
    "\n",
    "        Args:\n",
    "            articles: List of article data\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of extraction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        self.logger.info(f\"Starting batch processing of {len(articles)} articles...\")\n",
    "\n",
    "        for i, article in enumerate(articles):\n",
    "            try:\n",
    "                self.logger.info(f\"Processing article {i+1}/{len(articles)}: {article.get('title', 'Unknown')}\")\n",
    "\n",
    "                # Check article status\n",
    "                if article.get('status') != 'success':\n",
    "                    self.logger.warning(f\"Skipping failed article: {article.get('error', 'Unknown error')}\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'skipped',\n",
    "                        'extraction_error': article.get('error', 'Article scraping failed'),\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Extract main content\n",
    "                content = article.get('main_content', '')\n",
    "                if not content:\n",
    "                    self.logger.warning(\"Article content is empty, skipping\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Empty content',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Run structured extraction\n",
    "                extracted_data = self.extract_structured_data(content)\n",
    "\n",
    "                if extracted_data:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'success',\n",
    "                        'extraction_error': None,\n",
    "                        'structured_data': extracted_data.dict()\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Extraction failed after retries',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "\n",
    "                # Delay to avoid rate limit\n",
    "                if i < len(articles) - 1:\n",
    "                    time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing article: {e}\")\n",
    "                results.append({\n",
    "                    'article_info': article,\n",
    "                    'extraction_status': 'error',\n",
    "                    'extraction_error': str(e),\n",
    "                    'structured_data': None\n",
    "                })\n",
    "\n",
    "        # Summary statistics\n",
    "        successful = len([r for r in results if r['extraction_status'] == 'success'])\n",
    "        failed = len([r for r in results if r['extraction_status'] in ['failed', 'error']])\n",
    "        skipped = len([r for r in results if r['extraction_status'] == 'skipped'])\n",
    "\n",
    "        self.logger.info(f\"Batch processing finished: Success {successful}, Failed {failed}, Skipped {skipped}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_extraction_results(self, results: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"\n",
    "        Save extraction results to a file\n",
    "\n",
    "        Args:\n",
    "            results: List of extraction results\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n",
    "            self.logger.info(f\"âœ… Extraction results saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"âŒ Error saving file: {e}\")\n",
    "\n",
    "print(\"âœ… Structured data extractor class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "S3QYtLVgPMa-"
   },
   "outputs": [],
   "source": [
    "def perform_structured_extraction(scraper_result):\n",
    "    # Perform structured data extraction - this was the missing call code!\n",
    "    print(\"ğŸš€ Starting structured data extraction...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Check if API key is set\n",
    "    if not os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"âš ï¸ Please set the OpenAI API key first:\")\n",
    "        print(\"   os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\")\n",
    "        print(\"   Then re-run this cell\")\n",
    "    else:\n",
    "        try:\n",
    "            # 1. Initialize the extractor (using the previously defined class)\n",
    "            extractor = StructuredDataExtractor()\n",
    "\n",
    "            if scraper_result is not None:\n",
    "                scraped_articles = scraper_result\n",
    "                print(f\"ğŸ“– Using the provided scraping results\")\n",
    "            else:\n",
    "                print(f\"ğŸ“– No scraping results provided, trying to load from file\")\n",
    "                # 2. Read previously scraped data\n",
    "                with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "                    scraped_articles = json.load(f)\n",
    "\n",
    "            print(f\"ğŸ“– Successfully loaded {len(scraped_articles)} articles\")\n",
    "\n",
    "            # 3. Perform batch structured extraction (call the core method!)\n",
    "            print(\"ğŸ”„ Starting batch structured extraction...\")\n",
    "            extraction_results = extractor.batch_extract(scraped_articles)\n",
    "\n",
    "            # 4. Save extraction results (call the save method!)\n",
    "            extractor.save_extraction_results(extraction_results, './structured_extractions.json')\n",
    "\n",
    "            # 5. Show extraction statistics\n",
    "            successful = [r for r in extraction_results if r['extraction_status'] == 'success']\n",
    "            failed = [r for r in extraction_results if r['extraction_status'] != 'success']\n",
    "\n",
    "            print(f\"\\nğŸ“Š Extraction statistics:\")\n",
    "            print(f\"âœ… Successfully extracted: {len(successful)} articles\")\n",
    "            print(f\"âŒ Failed to extract: {len(failed)} articles\")\n",
    "\n",
    "            # 6. Display the first successful extraction result\n",
    "            if successful:\n",
    "                first_result = successful[0]\n",
    "                print(f\"\\nğŸ“„ Sample extraction result - {first_result['article_info']['title']}:\")\n",
    "                print(\"-\" * 50)\n",
    "                structured = first_result['structured_data']\n",
    "                print(f\"ğŸ“ Primary name: {structured['primary_name']}\")\n",
    "                print(f\"ğŸ‘¥ Discoverers: {', '.join(structured['discoverers'])}\")\n",
    "                print(f\"ğŸ“… Discovery years: {', '.join(structured['discovery_years'])}\")\n",
    "                print(f\"ğŸ”¬ Applications: {len(structured['applications'])} items\")\n",
    "                print(f\"â­ Key features: {len(structured['key_features'])} items\")\n",
    "\n",
    "            print(f\"\\nâœ… Structured extraction completed! Results saved to './structured_extractions.json'\")\n",
    "\n",
    "            return extraction_results\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Execution error: {str(e)}\")\n",
    "            print(\"Please check the API key settings and ensure the data file exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUgMw9L6PMa-"
   },
   "source": [
    "## Part 3: Function Calling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIAnalyzer:\n",
    "    \"\"\"OpenAIåˆ†æå™¨ç±»ï¼Œç”¨äºæ™ºèƒ½åŒ¹é…å’Œæ•°æ®åˆ†æ\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–\n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                raise ValueError(\"è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        print(\"âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    \n",
    "    def analyze_query_match(self, user_query: str, available_names: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æç”¨æˆ·æŸ¥è¯¢ä¸å¯ç”¨åç§°çš„åŒ¹é…åº¦\n",
    "        \n",
    "        Args:\n",
    "            user_query: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢\n",
    "            available_names: å¯ç”¨çš„åç§°åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«æœ€ä½³åŒ¹é…å’Œç½®ä¿¡åº¦çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ„å»ºæç¤ºè¯\n",
    "            prompt = f\"\"\"\n",
    "            ç”¨æˆ·æŸ¥è¯¢: \"{user_query}\"\n",
    "            \n",
    "            å¯ç”¨çš„ç§‘å­¦å‘ç°åç§°åˆ—è¡¨:\n",
    "            {json.dumps(available_names, ensure_ascii=False, indent=2)}\n",
    "            \n",
    "            è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ä¸å¯ç”¨åç§°çš„åŒ¹é…åº¦ï¼Œè¿”å›JSONæ ¼å¼ç»“æœ:\n",
    "            {{\n",
    "                \"best_match\": \"æœ€åŒ¹é…çš„åç§°\",\n",
    "                \"confidence\": 0.0-1.0çš„ç½®ä¿¡åº¦,\n",
    "                \"reasoning\": \"åŒ¹é…åŸå› \",\n",
    "                \"alternative_matches\": [\"å…¶ä»–å¯èƒ½çš„åŒ¹é…é¡¹\"]\n",
    "            }}\n",
    "            \n",
    "            æ³¨æ„ï¼š\n",
    "            1. è€ƒè™‘åŒä¹‰è¯ã€ç¼©å†™ã€ä¸åŒè¡¨è¾¾æ–¹å¼\n",
    "            2. å¦‚æœæ‰¾ä¸åˆ°åˆé€‚åŒ¹é…ï¼Œconfidenceè®¾ä¸º0.0\n",
    "            3. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦æ•°æ®åŒ¹é…åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†ææŸ¥è¯¢ä¸æ•°æ®çš„åŒ¹é…åº¦ã€‚\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ OpenAIåˆ†æå‡ºé”™: {e}\")\n",
    "            return {\n",
    "                \"best_match\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"reasoning\": f\"åˆ†æå¤±è´¥: {e}\",\n",
    "                \"alternative_matches\": []\n",
    "            }\n",
    "    \n",
    "    def suggest_search_strategy(self, user_query: str, data_sample: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"å»ºè®®æœç´¢ç­–ç•¥\n",
    "        \n",
    "        Args:\n",
    "            user_query: ç”¨æˆ·æŸ¥è¯¢\n",
    "            data_sample: æ•°æ®æ ·æœ¬\n",
    "            \n",
    "        Returns:\n",
    "            æœç´¢ç­–ç•¥å»ºè®®\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            ç”¨æˆ·æŸ¥è¯¢: \"{user_query}\"\n",
    "            \n",
    "            æ•°æ®æ ·æœ¬ç»“æ„:\n",
    "            {json.dumps(data_sample[:3], ensure_ascii=False, indent=2)}\n",
    "            \n",
    "            è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼Œå»ºè®®æœ€ä½³æœç´¢ç­–ç•¥ï¼Œè¿”å›JSONæ ¼å¼:\n",
    "            {{\n",
    "                \"search_type\": \"name|scientist|application|general\",\n",
    "                \"search_keywords\": [\"å…³é”®è¯1\", \"å…³é”®è¯2\"],\n",
    "                \"explanation\": \"æœç´¢ç­–ç•¥è¯´æ˜\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç§‘å­¦æ•°æ®æœç´¢ç­–ç•¥ä¸“å®¶ã€‚\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            return json.loads(response.choices[0].message.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æœç´¢ç­–ç•¥åˆ†æå‡ºé”™: {e}\")\n",
    "            return {\n",
    "                \"search_type\": \"general\",\n",
    "                \"search_keywords\": [user_query],\n",
    "                \"explanation\": f\"åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æœç´¢: {e}\"\n",
    "            }\n",
    "\n",
    "print(\"âœ… OpenAIAnalyzerç±»å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F64QtW3jNh4"
   },
   "source": [
    "***3.1 Scientific Data Query***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "c_SeoN47jNh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ScientificDataQuery class definition completed\n"
     ]
    }
   ],
   "source": [
    "class ScientificDataQuery:\n",
    "    \"\"\"Scientific discovery data query class - simple data access functions\n",
    "    \n",
    "    This class provides basic data access functions. Fuzzy matching is handled by OpenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file: str = \"./structured_extractions.json\", data: Optional[List[Dict[str, Any]]] = None):\n",
    "        \"\"\"Initialize the query class with data loading\n",
    "        \n",
    "        Args:\n",
    "            data_file: Path to the structured data file (default: \"./structured_extractions.json\")\n",
    "            data: Optional in-memory data to use instead of loading from file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if data is not None:\n",
    "                # Use in-memory data directly\n",
    "                self.raw_data = data\n",
    "                print(f\"âœ… Using provided in-memory data with {len(data)} items\")\n",
    "            else:\n",
    "                # Load from file (default behavior for function calls)\n",
    "                with open(data_file, 'r', encoding='utf-8') as f:\n",
    "                    self.raw_data = json.load(f)\n",
    "                print(f\"âœ… Loaded data from {data_file}\")\n",
    "\n",
    "            # Extract successful structured data\n",
    "            self.discoveries = []\n",
    "            for item in self.raw_data:\n",
    "                if isinstance(item, dict):\n",
    "                    if item.get('extraction_status') == 'success' and item.get('extracted_data'):\n",
    "                        # Merge extracted data with metadata\n",
    "                        extracted_data = item['extracted_data']\n",
    "                        if isinstance(extracted_data, dict):\n",
    "                            self.discoveries.append({**item, **extracted_data})\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ Skipping item with malformed 'extracted_data': {item.get('title', 'Unknown Title')}\")\n",
    "                    elif isinstance(item, dict) and 'primary_name' in item:\n",
    "                        # Already structured data\n",
    "                        self.discoveries.append(item)\n",
    "\n",
    "            print(f\"âœ… Successfully loaded {len(self.discoveries)} scientific discovery data\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ Error: Data file not found at {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ Error: Could not decode JSON from {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ An unexpected error occurred during data loading: {e}\")\n",
    "            self.discoveries = []\n",
    "\n",
    "    def find_discovery_by_name(self, discovery_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Find a scientific discovery by exact name match\"\"\"\n",
    "        discovery_name_lower = discovery_name.lower().strip()\n",
    "        for discovery in self.discoveries:\n",
    "            if discovery.get('primary_name', '').lower() == discovery_name_lower:\n",
    "                return discovery\n",
    "        return None\n",
    "\n",
    "    def find_discoveries_by_scientist(self, scientist_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find all discoveries by a scientist (exact name match)\"\"\"\n",
    "        scientist_name_lower = scientist_name.lower().strip()\n",
    "        matched_discoveries = []\n",
    "        \n",
    "        for discovery in self.discoveries:\n",
    "            for discoverer in discovery.get('discoverers', []):\n",
    "                if discoverer.lower() == scientist_name_lower:\n",
    "                    matched_discoveries.append(discovery)\n",
    "                    break\n",
    "        \n",
    "        return matched_discoveries\n",
    "\n",
    "    def get_all_discoveries(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all available scientific discoveries\"\"\"\n",
    "        return self.discoveries\n",
    "\n",
    "    def get_discovery_details(self, discovery_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get comprehensive details about a specific scientific discovery\"\"\"\n",
    "        discovery = self.find_discovery_by_name(discovery_name)\n",
    "        if discovery:\n",
    "            return {\n",
    "                \"primary_name\": discovery.get('primary_name'),\n",
    "                \"discoverers\": discovery.get('discoverers', []),\n",
    "                \"discovery_years\": discovery.get('discovery_years', []),\n",
    "                \"mechanism\": discovery.get('mechanism', ''),\n",
    "                \"key_features\": discovery.get('key_features', []),\n",
    "                \"applications\": discovery.get('applications', []),\n",
    "                \"significance\": discovery.get('significance', ''),\n",
    "                \"institutions\": discovery.get('institutions', []),\n",
    "                \"awards\": discovery.get('awards', []),\n",
    "                \"url\": discovery.get('url', '')\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def search_by_application(self, application_keyword: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for scientific discoveries related to a specific application keyword\"\"\"\n",
    "        results = []\n",
    "        keyword_lower = application_keyword.lower()\n",
    "        for discovery in self.discoveries:\n",
    "            applications = discovery.get('applications', [])\n",
    "            if any(keyword_lower in app.lower() for app in applications):\n",
    "                results.append(discovery)\n",
    "        return results\n",
    "\n",
    "print(\"âœ… ScientificDataQuery class definition completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8auVGAjNh4"
   },
   "source": [
    "***3.2 Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "D5EpLGIBjNh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Part 3 function definitions completed\n"
     ]
    }
   ],
   "source": [
    "def compare_discoveries(discovery1: str, discovery2: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two scientific discoveries\n",
    "\n",
    "    Args:\n",
    "        discovery1: Name of the first discovery\n",
    "        discovery2: Name of the second discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed comparison analysis\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "\n",
    "    # Find the two discoveries\n",
    "    disc1 = query.find_discovery_by_name(discovery1)\n",
    "    disc2 = query.find_discovery_by_name(discovery2)\n",
    "\n",
    "    if not disc1:\n",
    "        return f\"âŒ Discovery not found: {discovery1}\"\n",
    "    if not disc2:\n",
    "        return f\"âŒ Discovery not found: {discovery2}\"\n",
    "\n",
    "    # Build comparison analysis\n",
    "    comparison = f\"\"\"\n",
    "    ğŸ”¬ Scientific Discovery Comparison\n",
    "    {'='*50}\n",
    "\n",
    "    ğŸ“ Discovery 1: {disc1['primary_name']}\n",
    "    ğŸ“ Discovery 2: {disc2['primary_name']}\n",
    "\n",
    "    ğŸ‘¥ Discoverers:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1['discoverers'])}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2['discoverers'])}\n",
    "\n",
    "    ğŸ“… Discovery Years:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1['discovery_years'])}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2['discovery_years'])}\n",
    "\n",
    "    ğŸ”§ Mechanisms:\n",
    "    â€¢ {disc1['primary_name']}: {disc1['mechanism']}\n",
    "    â€¢ {disc2['primary_name']}: {disc2['mechanism']}\n",
    "\n",
    "    â­ Key Features:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1['key_features'])}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2['key_features'])}\n",
    "\n",
    "    ğŸ¯ Applications:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1['applications'])}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2['applications'])}\n",
    "\n",
    "    ğŸ† Awards:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1.get('awards', ['No award information']))}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2.get('awards', ['No award information']))}\n",
    "\n",
    "    ğŸ’¡ Significance:\n",
    "    â€¢ {disc1['primary_name']}: {disc1['significance']}\n",
    "    â€¢ {disc2['primary_name']}: {disc2['significance']}\n",
    "\n",
    "    ğŸ›ï¸ Institutions:\n",
    "    â€¢ {disc1['primary_name']}: {', '.join(disc1.get('institutions', ['No institution info']))}\n",
    "    â€¢ {disc2['primary_name']}: {', '.join(disc2.get('institutions', ['No institution info']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "def get_research_timeline(scientist: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the research timeline of a scientist\n",
    "\n",
    "    Args:\n",
    "        scientist: Scientist name\n",
    "\n",
    "    Returns:\n",
    "        Timeline of the scientist's discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discoveries = query.find_discoveries_by_scientist(scientist)\n",
    "\n",
    "    if not discoveries:\n",
    "        return f\"âŒ No discoveries found for scientist '{scientist}'\"\n",
    "\n",
    "    # Sort by discovery year\n",
    "    sorted_discoveries = sorted(discoveries,\n",
    "                                key=lambda x: int(x['discovery_years'][0]) if x['discovery_years'] else 0)\n",
    "\n",
    "    timeline = f\"\"\"\n",
    "    ğŸ‘¨â€ğŸ”¬ Research Timeline of {scientist}\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(sorted_discoveries, 1):\n",
    "        timeline += f\"\"\"\n",
    "    ğŸ”¬ Discovery {i}: {discovery['primary_name']}\n",
    "    ğŸ“… Year: {', '.join(discovery['discovery_years'])}\n",
    "    ğŸ¯ Significance: {discovery['significance']}\n",
    "    ğŸ›ï¸ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "    ğŸ† Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def search_by_application(application: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for discoveries by application area\n",
    "\n",
    "    Args:\n",
    "        application: Application keyword\n",
    "\n",
    "    Returns:\n",
    "        List of relevant discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    all_discoveries = query.get_all_discoveries()\n",
    "\n",
    "    application_lower = application.lower()\n",
    "    relevant_discoveries = []\n",
    "\n",
    "    for discovery in all_discoveries:\n",
    "        # Search within applications\n",
    "        for app in discovery.get('applications', []):\n",
    "            if application_lower in app.lower():\n",
    "                relevant_discoveries.append(discovery)\n",
    "                break\n",
    "\n",
    "    if not relevant_discoveries:\n",
    "        return f\"âŒ No discoveries found related to '{application}'\"\n",
    "\n",
    "    result = f\"\"\"\n",
    "    ğŸ¯ Discoveries related to '{application}'\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(relevant_discoveries, 1):\n",
    "        result += f\"\"\"\n",
    "        ğŸ”¬ Discovery {i}: {discovery['primary_name']}\n",
    "        ğŸ‘¥ Discoverers: {', '.join(discovery['discoverers'])}\n",
    "        ğŸ“… Year: {', '.join(discovery['discovery_years'])}\n",
    "        ğŸ¯ Applications: {', '.join(discovery['applications'])}\n",
    "        ğŸ’¡ Significance: {discovery['significance']}\n",
    "        \"\"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_discovery_details(discovery_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information of a specific discovery\n",
    "\n",
    "    Args:\n",
    "        discovery_name: Name of the scientific discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discovery = query.find_discovery_by_name(discovery_name)\n",
    "\n",
    "    if not discovery:\n",
    "        return f\"âŒ Discovery not found: {discovery_name}\"\n",
    "\n",
    "    details = f\"\"\"\n",
    "    ğŸ”¬ {discovery['primary_name']} - Details\n",
    "    {'='*60}\n",
    "\n",
    "    ğŸ‘¥ Discoverers: {', '.join(discovery['discoverers'])}\n",
    "\n",
    "    ğŸ“… Discovery Years: {', '.join(discovery['discovery_years'])}\n",
    "\n",
    "    ğŸ”§ Mechanism:\n",
    "    {discovery['mechanism']}\n",
    "\n",
    "    â­ Key Features:\n",
    "    {chr(10).join([f\"  â€¢ {feature}\" for feature in discovery['key_features']])}\n",
    "\n",
    "    ğŸ¯ Applications:\n",
    "    {chr(10).join([f\"  â€¢ {app}\" for app in discovery['applications']])}\n",
    "\n",
    "    ğŸ’¡ Significance:\n",
    "    {discovery['significance']}\n",
    "\n",
    "    ğŸ›ï¸ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "\n",
    "    ğŸ† Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "\n",
    "    ğŸ”— URL: {discovery.get('url', 'No URL available')}\n",
    "    \"\"\"\n",
    "\n",
    "    return details\n",
    "\n",
    "print(\"âœ… Part 3 function definitions completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2IBCB_ojNh5"
   },
   "source": [
    "***3.3 Function Scheme***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "LysKzxaVjNh5"
   },
   "outputs": [],
   "source": [
    "FUNCTION_SCHEMAS = [\n",
    "    {\n",
    "        \"name\": \"compare_discoveries\",\n",
    "        \"description\": \"Compare two scientific breakthroughs in detail. Accepts discovery names, keywords, or partial names (e.g., 'CRISPR', 'åŸºå› ç¼–è¾‘', 'mRNA', 'ç–«è‹—', 'å¼•åŠ›æ³¢'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery1\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"First scientific discovery name or keyword (supports full name, abbreviation, partial match, or Chinese/English keyword)\"\n",
    "                },\n",
    "                \"discovery2\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Second scientific discovery name or keyword (supports full name, abbreviation, partial match, or Chinese/English keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery1\", \"discovery2\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_research_timeline\",\n",
    "        \"description\": \"Get chronological timeline of a scientist's major discoveries. Accepts names in full, partial, or last name only (e.g., 'Jennifer Doudna', 'Doudna', 'çˆ±å› æ–¯å¦', 'Einstein'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"scientist\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientist's name or keyword (supports full name, partial name, or last name in English/Chinese)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"scientist\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_by_application\",\n",
    "        \"description\": \"Search for scientific discoveries by their application domain or usage field. Accepts keywords in English or Chinese (e.g., 'medicine', 'agriculture', 'biotechnology', 'åŒ»å­¦', 'å†œä¸š').\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"application\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Application domain keyword (e.g., 'medicine', 'agriculture', 'biotechnology', 'ææ–™ç§‘å­¦')\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"application\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_discovery_details\",\n",
    "        \"description\": \"Get comprehensive details about a specific scientific discovery. Accepts discovery names, abbreviations, or keywords (e.g., 'CRISPR', 'åŸºå› ç¼–è¾‘', 'mRNA', 'ç–«è‹—'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientific discovery name or keyword (supports partial name, abbreviation, or Chinese/English keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery_name\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJWvrdXdjNh5"
   },
   "source": [
    "***3.4 Assistant***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "W4I_kVHOjNh5"
   },
   "outputs": [],
   "source": [
    "class ScientificResearchAssistant:\n",
    "    \"\"\"ç§‘å­¦å‘ç°äº¤äº’å¼ç ”ç©¶åŠ©æ‰‹\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–åŠ©æ‰‹\"\"\"\n",
    "        self.client = None\n",
    "\n",
    "        # æ£€æŸ¥APIå¯†é’¥ï¼ˆæ”¯æŒäº¤äº’å¼è¾“å…¥ï¼‰\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            try:\n",
    "                from getpass import getpass\n",
    "                api_key = getpass('è¯·è¾“å…¥ OPENAI_API_KEYï¼ˆè¾“å…¥å†…å®¹ä¸å¯è§ï¼‰ï¼š')\n",
    "            except Exception:\n",
    "                api_key = input('è¯·è¾“å…¥ OPENAI_API_KEYï¼š')\n",
    "\n",
    "        if api_key:\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "            print(\"âœ… ç§‘å­¦ç ”ç©¶åŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æœªè®¾ç½®OpenAI APIå¯†é’¥ï¼ŒåŠŸèƒ½å°†å—é™\")\n",
    "\n",
    "        # å¯ç”¨å‡½æ•°æ˜ å°„\n",
    "        self.available_functions = {\n",
    "            \"compare_discoveries\": compare_discoveries,\n",
    "            \"get_research_timeline\": get_research_timeline,\n",
    "            \"search_by_application\": search_by_application,\n",
    "            \"get_discovery_details\": get_discovery_details\n",
    "        }\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"ä¸åŠ©æ‰‹å¯¹è¯\"\"\"\n",
    "\n",
    "        if not self.client:\n",
    "            return \"âŒ OpenAI APIæœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½å¯¹è¯\"\n",
    "\n",
    "        try:\n",
    "            # è°ƒç”¨OpenAI API with Function Calling\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\":\"\"\"\n",
    "                        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦å‘ç°ç ”ç©¶åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·ï¼š\n",
    "                        1. æ¯”è¾ƒä¸åŒçš„ç§‘å­¦å‘ç°\n",
    "                        2. æŸ¥çœ‹ç§‘å­¦å®¶çš„ç ”ç©¶æ—¶é—´çº¿\n",
    "                        3. æ ¹æ®åº”ç”¨é¢†åŸŸæœç´¢å‘ç°\n",
    "                        4. è·å–ç‰¹å®šå‘ç°çš„è¯¦ç»†ä¿¡æ¯\n",
    "                        è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œé€‰æ‹©åˆé€‚çš„å‡½æ•°æ¥å›ç­”ã€‚\"\"\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                functions=FUNCTION_SCHEMAS,\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "\n",
    "            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡½æ•°è°ƒç”¨\n",
    "            if message.function_call:\n",
    "                function_name = message.function_call.name\n",
    "                function_args = json.loads(message.function_call.arguments)\n",
    "\n",
    "                print(f\"ğŸ”§ è°ƒç”¨å‡½æ•°: {function_name}\")\n",
    "                print(f\"ğŸ“ å‚æ•°: {function_args}\")\n",
    "\n",
    "                # æ‰§è¡Œå‡½æ•°\n",
    "                if function_name in self.available_functions:\n",
    "                    result = self.available_functions[function_name](**function_args)\n",
    "                    return result\n",
    "                else:\n",
    "                    return f\"âŒ æœªçŸ¥å‡½æ•°: {function_name}\"\n",
    "\n",
    "            else:\n",
    "                return message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\"\n",
    "\n",
    "    def show_available_data(self) -> str:\n",
    "        \"\"\"æ˜¾ç¤ºå¯ç”¨çš„æ•°æ®\"\"\"\n",
    "        query = ScientificDataQuery()\n",
    "        discoveries = query.get_all_discoveries()\n",
    "\n",
    "        if not discoveries:\n",
    "            return \"âŒ æš‚æ— å¯ç”¨æ•°æ®\"\n",
    "\n",
    "        result = \"ğŸ“š å¯ç”¨çš„ç§‘å­¦å‘ç°æ•°æ®:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "\n",
    "        for i, discovery in enumerate(discoveries, 1):\n",
    "            result += f\"{i}. {discovery['primary_name']}\\n\"\n",
    "            result += f\"   å‘ç°è€…: {', '.join(discovery['discoverers'])}\\n\"\n",
    "            result += f\"   å¹´ä»½: {', '.join(discovery['discovery_years'])}\\n\\n\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T6ETPfTjNh5"
   },
   "source": [
    "***Instance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "y_-Fs_Y4jNh5"
   },
   "outputs": [],
   "source": [
    "def assistant_chat(example_queries: List[str]):\n",
    "    \"\"\"ä¸»å‡½æ•°æ¼”ç¤º\"\"\"\n",
    "    print(\"ğŸ”¬ ç§‘å­¦å‘ç°äº¤äº’å¼ç ”ç©¶åŠ©æ‰‹\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # åˆå§‹åŒ–åŠ©æ‰‹\n",
    "    assistant = ScientificResearchAssistant()\n",
    "\n",
    "    # æ˜¾ç¤ºå¯ç”¨æ•°æ®\n",
    "    print(assistant.show_available_data())\n",
    "\n",
    "    if example_queries !=[]:\n",
    "        print(\"âœ… ä½¿ç”¨ä¼ å…¥çš„ç¤ºä¾‹å¯¹è¯\")\n",
    "    else:\n",
    "        print(\"âœ… æœªä¼ å…¥ç¤ºä¾‹å¯¹è¯ï¼Œä½¿ç”¨é»˜è®¤ç¤ºä¾‹\")\n",
    "        example_queries = [\n",
    "            \"è¯·æ¯”è¾ƒCRISPRå’Œé‡å­è®¡ç®—ä¸¤ä¸ªç§‘å­¦å‘ç°\",\n",
    "            \"Jennifer Doudnaæœ‰å“ªäº›é‡è¦ç ”ç©¶å‘ç°ï¼Ÿ\",\n",
    "            \"æœç´¢åœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨çš„ç§‘å­¦å‘ç°\",\n",
    "            \"ç»™æˆ‘è¯¦ç»†ä»‹ç»ä¸€ä¸‹CRISPRæŠ€æœ¯\"\n",
    "        ]\n",
    "        for q in example_queries:\n",
    "            print(\"ç¤ºä¾‹é—®é¢˜:\", q)\n",
    "\n",
    "    # è¿›è¡Œå¯¹è¯\n",
    "    for q in example_queries:\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer:\", assistant.chat(q))\n",
    "\n",
    "    return assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y63DDOU4yRoa"
   },
   "source": [
    "# Part 4: Integration and Demonstration\n",
    "\n",
    "## ğŸ¯ ç›®æ ‡\n",
    "åˆ›å»ºä¸€ä¸ªå…¨é¢çš„æ¼”ç¤ºï¼Œå±•ç¤ºæ‰€æœ‰ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œï¼ŒåŒ…æ‹¬ï¼š\n",
    "- å®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹\n",
    "- æ‰€æœ‰ä¸»è¦åŠŸèƒ½æ¼”ç¤º\n",
    "- é”™è¯¯å¤„ç†ç¤ºä¾‹\n",
    "- æ•°æ®å¯è§†åŒ–\n",
    "- å¯è¿è¡Œçš„å®Œæ•´ç®¡é“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "ivyqLP2zyRoa"
   },
   "outputs": [],
   "source": [
    "search_terms = [\n",
    "        \"CRISPR\",\n",
    "        \"RNA vaccine\",\n",
    "        \"Gravitational wave\",\n",
    "        \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        \"Ancient DNA\",\n",
    "        \"Water on Mars\",\n",
    "        \"Penicillin\"\n",
    "        ]\n",
    "\n",
    "example_queries = [\n",
    "    \"Please compare the scientific discoveries of CRISPR and quantum computing.\",\n",
    "    \"What are the major research discoveries of Jennifer Doudna?\",\n",
    "    \"Search for scientific discoveries applied in the field of medicine.\",\n",
    "    \"Give me a detailed introduction to CRISPR technology.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "mlivBX99yRoa",
    "outputId": "f760f220-24c8-4cb2-d53e-8b71d141c387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç»¼åˆæ¼”ç¤ºç±»å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveDemo:\n",
    "    \"\"\"ç»¼åˆæ¼”ç¤ºç±» - å±•ç¤ºå®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ\"\"\"\n",
    "        self.scraper_result = None\n",
    "        self.extractor_result = None\n",
    "        self.assistant = None\n",
    "        self.query = None\n",
    "\n",
    "        print(\"ğŸš€ ç»¼åˆæ¼”ç¤ºç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    async def run_complete_pipeline(self):\n",
    "        \"\"\"è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯ç®¡é“\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ”¬ ç§‘å­¦å‘ç°ç ”ç©¶ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        try:\n",
    "            # Part 1: æ‰§è¡Œæ•°æ®æŠ“å–ï¼ˆè‹¥ä¼ ç©ºåˆ—è¡¨åˆ™ä½¿ç”¨é»˜è®¤æœç´¢è¯ï¼‰\n",
    "            # self.scraper_result = await scrape_scientific_discoveries(search_terms)\n",
    "\n",
    "            with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            self.scraper_result = data\n",
    "\n",
    "            # Part 2: æ‰§è¡Œç»“æ„åŒ–æ•°æ®æå–ï¼ˆåŸºäºæŠ“å–ç»“æœæˆ–å·²å­˜åœ¨æ–‡ä»¶ï¼‰\n",
    "            self.extractor_result = perform_structured_extraction(self.scraper_result)\n",
    "\n",
    "            # åˆå§‹åŒ–æŸ¥è¯¢å™¨ï¼Œä¾›å¯è§†åŒ–ä¸åŠ©æ‰‹ä½¿ç”¨\n",
    "            self.query = ScientificDataQuery()\n",
    "\n",
    "            # Part 3: åˆå§‹åŒ–äº¤äº’å¼åŠ©æ‰‹å¹¶è¿›è¡Œç¤ºä¾‹å¯¹è¯\n",
    "            self.assistant = assistant_chat(example_queries)\n",
    "\n",
    "            # å¯è§†åŒ–æ¼”ç¤º\n",
    "            self._demo_visualization()\n",
    "\n",
    "            print(\"\\nâœ… å®Œæ•´æ¼”ç¤ºå®Œæˆï¼\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "\n",
    "\n",
    "    def _demo_visualization(self):\n",
    "        \"\"\"æ¼”ç¤ºæ•°æ®å¯è§†åŒ–\"\"\"\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if not self.query or not self.query.discoveries:\n",
    "            print(\"âŒ æ²¡æœ‰æ•°æ®å¯ä¾›å¯è§†åŒ–\")\n",
    "            return\n",
    "\n",
    "        # åˆ›å»ºæ•°æ®å¯è§†åŒ–\n",
    "        # self._create_discovery_timeline()\n",
    "        # self._create_discoverer_network()\n",
    "        # self._create_application_distribution()\n",
    "\n",
    "    def _create_discovery_timeline(self):\n",
    "        \"\"\"åˆ›å»ºå‘ç°æ—¶é—´çº¿å›¾\"\"\"\n",
    "        print(\"ğŸ“… åˆ›å»ºç§‘å­¦å‘ç°æ—¶é—´çº¿å›¾...\")\n",
    "\n",
    "        try:\n",
    "            # å‡†å¤‡æ•°æ®ï¼ˆæ¥è‡ªç»“æ„åŒ–ç»“æœï¼‰\n",
    "            discoveries = self.query.discoveries\n",
    "            years = []\n",
    "            names = []\n",
    "\n",
    "            import re\n",
    "            def parse_year(value: str):\n",
    "                if not value:\n",
    "                    return None\n",
    "                m = re.search(r\"\\b(1|2)\\d{3}\\b\", str(value))\n",
    "                return int(m.group(0)) if m else None\n",
    "\n",
    "            for discovery in discoveries:\n",
    "                years_raw = discovery.get('discovery_years') or []\n",
    "                y = parse_year(years_raw[0] if years_raw else \"\")\n",
    "                if y is None:\n",
    "                    continue\n",
    "                years.append(y)\n",
    "                names.append(discovery['primary_name'])\n",
    "\n",
    "            # åˆ›å»ºæ—¶é—´çº¿å›¾\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.scatter(years, range(len(years)), s=100, alpha=0.7, c='skyblue', edgecolors='navy')\n",
    "\n",
    "            for i, (year, name) in enumerate(zip(years, names)):\n",
    "                plt.annotate(f\"{year}: {name[:20]}...\",\n",
    "                        (year, i),\n",
    "                        xytext=(10, 0),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=8,\n",
    "                        ha='left')\n",
    "\n",
    "            plt.xlabel('å‘ç°å¹´ä»½')\n",
    "            plt.ylabel('ç§‘å­¦å‘ç°')\n",
    "            plt.title('ç§‘å­¦å‘ç°æ—¶é—´çº¿')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"âœ… æ—¶é—´çº¿å›¾åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ—¶é—´çº¿å›¾åˆ›å»ºå¤±è´¥: {str(e)}\")\n",
    "\n",
    "    def _create_discoverer_network(self):\n",
    "        \"\"\"åˆ›å»ºå‘ç°è€…ç½‘ç»œå›¾\"\"\"\n",
    "        print(\"ğŸ‘¥ åˆ›å»ºå‘ç°è€…ç½‘ç»œå›¾...\")\n",
    "\n",
    "        try:\n",
    "            # ç»Ÿè®¡å‘ç°è€…\n",
    "            discoverer_count = {}\n",
    "            for discovery in self.query.discoveries:\n",
    "                for discoverer in discovery.get('discoverers', []):\n",
    "                    discoverer_count[discoverer] = discoverer_count.get(discoverer, 0) + 1\n",
    "\n",
    "            # åˆ›å»ºæŸ±çŠ¶å›¾\n",
    "            if discoverer_count:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                names = list(discoverer_count.keys())\n",
    "                counts = list(discoverer_count.values())\n",
    "\n",
    "                bars = plt.bar(range(len(names)), counts, color='lightcoral', alpha=0.7)\n",
    "                plt.xlabel('å‘ç°è€…')\n",
    "                plt.ylabel('å‘ç°æ•°é‡')\n",
    "                plt.title('å‘ç°è€…è´¡çŒ®ç»Ÿè®¡')\n",
    "                plt.xticks(range(len(names)), [name.split()[-1] for name in names], rotation=45)\n",
    "\n",
    "                # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "                for bar, count in zip(bars, counts):\n",
    "                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                            str(count), ha='center', va='bottom')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                print(\"âœ… å‘ç°è€…ç½‘ç»œå›¾åˆ›å»ºæˆåŠŸ\")\n",
    "            else:\n",
    "                print(\"âš ï¸ æ²¡æœ‰å‘ç°è€…æ•°æ®\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘ç°è€…ç½‘ç»œå›¾åˆ›å»ºå¤±è´¥: {str(e)}\")\n",
    "\n",
    "    def _create_application_distribution(self):\n",
    "        \"\"\"åˆ›å»ºåº”ç”¨é¢†åŸŸåˆ†å¸ƒå›¾\"\"\"\n",
    "        print(\"ğŸ¯ åˆ›å»ºåº”ç”¨é¢†åŸŸåˆ†å¸ƒå›¾...\")\n",
    "\n",
    "        try:\n",
    "            # ç»Ÿè®¡åº”ç”¨é¢†åŸŸ\n",
    "            application_count = {}\n",
    "            for discovery in self.query.discoveries:\n",
    "                for app in discovery.get('applications', []):\n",
    "                    # ç®€åŒ–åº”ç”¨é¢†åŸŸåç§°\n",
    "                    app_key = app.lower().strip()\n",
    "                    if 'research' in app_key:\n",
    "                        app_key = 'Research'\n",
    "                    elif 'medicine' in app_key or 'medical' in app_key:\n",
    "                        app_key = 'Medicine'\n",
    "                    elif 'biotechnology' in app_key or 'biotech' in app_key:\n",
    "                        app_key = 'Biotechnology'\n",
    "                    elif 'treatment' in app_key or 'therapy' in app_key:\n",
    "                        app_key = 'Treatment'\n",
    "                    else:\n",
    "                        app_key = app_key.title()\n",
    "\n",
    "                    application_count[app_key] = application_count.get(app_key, 0) + 1\n",
    "\n",
    "            # åˆ›å»ºé¥¼å›¾\n",
    "            if application_count:\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                labels = list(application_count.keys())\n",
    "                sizes = list(application_count.values())\n",
    "                colors = plt.cm.Set3(range(len(labels)))\n",
    "\n",
    "                plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "                plt.title('ç§‘å­¦å‘ç°åº”ç”¨é¢†åŸŸåˆ†å¸ƒ')\n",
    "                plt.axis('equal')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"âœ… åº”ç”¨é¢†åŸŸåˆ†å¸ƒå›¾åˆ›å»ºæˆåŠŸ\")\n",
    "            else:\n",
    "                print(\"âš ï¸ æ²¡æœ‰åº”ç”¨é¢†åŸŸæ•°æ®\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åº”ç”¨é¢†åŸŸåˆ†å¸ƒå›¾åˆ›å»ºå¤±è´¥: {str(e)}\")\n",
    "\n",
    "print(\"âœ… ç»¼åˆæ¼”ç¤ºç±»å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ836xTmyRob",
    "outputId": "cf54b291-5af0-4bcc-e422-40819a9f39a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ å¼€å§‹Part 4: Integration and Demonstration\n",
      "============================================================\n",
      "ğŸš€ ç»¼åˆæ¼”ç¤ºç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "================================================================================\n",
      "ğŸ”¬ ç§‘å­¦å‘ç°ç ”ç©¶ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º\n",
      "================================================================================\n",
      "ğŸš€ Starting structured data extraction...\n",
      "============================================================\n",
      "âœ… OpenAI client initialized successfully\n",
      "ğŸ“– Using the provided scraping results\n",
      "ğŸ“– Successfully loaded 8 articles\n",
      "ğŸ”„ Starting batch structured extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting batch processing of 8 articles...\n",
      "INFO:__main__:Processing article 1/8: CRISPR\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 2/8: mRNA vaccine\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 3/8: Gravitational wave\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 4/8: Higgs boson\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 5/8: Quantum computing\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 6/8: Ancient DNA\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 7/8: Water on Mars\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 8/8: Penicillin\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:âœ… Structured data extraction succeeded\n",
      "INFO:__main__:Batch processing finished: Success 8, Failed 0, Skipped 0\n",
      "INFO:__main__:âœ… Extraction results saved to: ./structured_extractions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Extraction statistics:\n",
      "âœ… Successfully extracted: 8 articles\n",
      "âŒ Failed to extract: 0 articles\n",
      "\n",
      "ğŸ“„ Sample extraction result - CRISPR:\n",
      "--------------------------------------------------\n",
      "ğŸ“ Primary name: CRISPR-Cas9 genome editing technology\n",
      "ğŸ‘¥ Discoverers: Jennifer Doudna, Emmanuelle Charpentier, Francisco Mojica, Yoshizumi Ishino, Rodolphe Barrangou, Ruud Jansen\n",
      "ğŸ“… Discovery years: 2012\n",
      "ğŸ”¬ Applications: 4 items\n",
      "â­ Key features: 4 items\n",
      "\n",
      "âœ… Structured extraction completed! Results saved to './structured_extractions.json'\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "ğŸ”¬ ç§‘å­¦å‘ç°äº¤äº’å¼ç ”ç©¶åŠ©æ‰‹\n",
      "==================================================\n",
      "âœ… ç§‘å­¦ç ”ç©¶åŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "âŒ æš‚æ— å¯ç”¨æ•°æ®\n",
      "âœ… ä½¿ç”¨ä¼ å…¥çš„ç¤ºä¾‹å¯¹è¯\n",
      "Question: Please compare the scientific discoveries of CRISPR and quantum computing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è°ƒç”¨å‡½æ•°: compare_discoveries\n",
      "ğŸ“ å‚æ•°: {'discovery1': 'CRISPR', 'discovery2': 'quantum computing'}\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "Answer: âŒ Discovery not found: CRISPR\n",
      "Question: What are the major research discoveries of Jennifer Doudna?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è°ƒç”¨å‡½æ•°: get_research_timeline\n",
      "ğŸ“ å‚æ•°: {'scientist': 'Jennifer Doudna'}\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "Answer: âŒ No discoveries found for scientist 'Jennifer Doudna'\n",
      "Question: Search for scientific discoveries applied in the field of medicine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è°ƒç”¨å‡½æ•°: search_by_application\n",
      "ğŸ“ å‚æ•°: {'application': 'medicine'}\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "Answer: âŒ No discoveries found related to 'medicine'\n",
      "Question: Give me a detailed introduction to CRISPR technology.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è°ƒç”¨å‡½æ•°: get_discovery_details\n",
      "ğŸ“ å‚æ•°: {'discovery_name': 'CRISPR'}\n",
      "âœ… Loaded data from ./structured_extractions.json\n",
      "âœ… Successfully loaded 0 scientific discovery data\n",
      "Answer: âŒ Discovery not found: CRISPR\n",
      "--------------------------------------------------\n",
      "âŒ æ²¡æœ‰æ•°æ®å¯ä¾›å¯è§†åŒ–\n",
      "\n",
      "âœ… å®Œæ•´æ¼”ç¤ºå®Œæˆï¼\n",
      "\n",
      "ğŸ‰ Part 4æ¼”ç¤ºå®Œæˆï¼\n",
      "============================================================\n",
      "ğŸ“‹ æ¼”ç¤ºå†…å®¹æ€»ç»“:\n",
      "âœ… 1. å®Œæ•´ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹\n",
      "âœ… 2. æ‰€æœ‰ä¸»è¦åŠŸèƒ½æ¼”ç¤º\n",
      "âœ… 3. é”™è¯¯å¤„ç†ç¤ºä¾‹\n",
      "âœ… 4. æ•°æ®å¯è§†åŒ–\n",
      "âœ… 5. äº¤äº’å¼åŠ©æ‰‹æ¼”ç¤º\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå®Œæ•´çš„Part 4æ¼”ç¤º\n",
    "def run_part4_demo():\n",
    "    \"\"\"è¿è¡ŒPart 4å®Œæ•´æ¼”ç¤º\"\"\"\n",
    "    print(\"ğŸ¬ å¼€å§‹Part 4: Integration and Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹\n",
    "    demo = ComprehensiveDemo()\n",
    "\n",
    "    # è¿è¡Œå®Œæ•´ç®¡é“ï¼ˆæœ¬å‡½æ•°éœ€åœ¨ Jupyter ä¸­ç”¨ await è°ƒç”¨ï¼‰\n",
    "    import nest_asyncio, asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.get_event_loop().run_until_complete(demo.run_complete_pipeline())\n",
    "\n",
    "    print(\"\\nğŸ‰ Part 4æ¼”ç¤ºå®Œæˆï¼\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“‹ æ¼”ç¤ºå†…å®¹æ€»ç»“:\")\n",
    "    print(\"âœ… 1. å®Œæ•´ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹\")\n",
    "    print(\"âœ… 2. æ‰€æœ‰ä¸»è¦åŠŸèƒ½æ¼”ç¤º\")\n",
    "    print(\"âœ… 3. é”™è¯¯å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"âœ… 4. æ•°æ®å¯è§†åŒ–\")\n",
    "    print(\"âœ… 5. äº¤äº’å¼åŠ©æ‰‹æ¼”ç¤º\")\n",
    "\n",
    "# æ‰§è¡Œæ¼”ç¤º\n",
    "run_part4_demo()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
