{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG4aG6PBi5JK"
   },
   "source": [
    "# IS5126 Individual Assignment 1 - HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aw-dl5LPMa3"
   },
   "outputs": [],
   "source": [
    "%pip install crawl4ai\n",
    "%pip install beautifulsoup4 lxml requests\n",
    "!playwright install\n",
    "print(\"✅ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3GgZE_LPMa4"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import openai\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set font for visualization\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Enable nest_asyncio to support async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "# Enable nest_asyncio to support async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkYZwNMmi5JM"
   },
   "source": [
    "## Part 1: Wikipedia Scraper Implementation:\n",
    "– WikipediaScraper class with all required methods\n",
    "– Demonstration of scraping your chosen 5+ Wikipedia articles\n",
    "– Explanation of how your articles relate to your research domain\n",
    "– Error handling examples with sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGJG0bMPi5JO"
   },
   "outputs": [],
   "source": [
    "class WikipediaScraper:\n",
    "    def __init__(self, base_urls: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize Wikipedia scraper\n",
    "\n",
    "        Args:\n",
    "            base_urls: List of Wikipedia article URLs to scrape\n",
    "        \"\"\"\n",
    "        self.base_urls = base_urls\n",
    "        self.scraped_data = []\n",
    "        self.rate_limit_delay = 2  # Request interval (seconds) to avoid overloading Wikipedia servers\n",
    "\n",
    "        # Validate URL format\n",
    "        self._validate_urls()\n",
    "\n",
    "    def _validate_urls(self):\n",
    "        \"\"\"Validate if URLs are valid Wikipedia links\"\"\"\n",
    "        valid_urls = []\n",
    "        for url in self.base_urls:\n",
    "            if self._is_valid_wikipedia_url(url):\n",
    "                valid_urls.append(url)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid Wikipedia URL: {url}\")\n",
    "\n",
    "        if not valid_urls:\n",
    "            raise ValueError(\"No valid Wikipedia URLs provided\")\n",
    "\n",
    "        self.base_urls = valid_urls\n",
    "        print(f\"Validation passed, {len(valid_urls)} valid URLs found\")\n",
    "\n",
    "    def _is_valid_wikipedia_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is a valid Wikipedia URL\"\"\"\n",
    "        try:\n",
    "            return (\n",
    "                'wikipedia.org' in url and\n",
    "                '/wiki/' in url\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _looks_like_html(self, content: str) -> bool:\n",
    "        \"\"\"Determine whether a string looks like HTML\"\"\"\n",
    "        try:\n",
    "            return bool(content and content.lstrip().startswith(\"<\"))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _html_to_text(self, html: str) -> str:\n",
    "        \"\"\"Convert Wikipedia HTML to cleaner plain text (keep title and paragraphs)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "            # Title\n",
    "            title_node = soup.select_one(\"#firstHeading\")\n",
    "            title_text = title_node.get_text(\" \", strip=True) if title_node else \"\"\n",
    "\n",
    "            # Main content container\n",
    "            content = soup.select_one(\"#mw-content-text\") or soup\n",
    "\n",
    "            # Noise selectors to remove\n",
    "            noise_selectors = [\n",
    "                \"#toc\", \".mw-references-wrap\", \"table.infobox\",\n",
    "                \"div.navbox\", \"table.metadata\", \"div.hatnote\",\n",
    "                \"script\", \"style\", \"noscript\",\n",
    "            ]\n",
    "            for sel in noise_selectors:\n",
    "                for node in content.select(sel):\n",
    "                    node.decompose()\n",
    "\n",
    "            # Paragraph text\n",
    "            paragraphs = []\n",
    "            for p in content.select(\"p\"):\n",
    "                text = p.get_text(\" \", strip=True)\n",
    "                text = re.sub(r\"\\s*\\[\\d+\\]\", \"\", text)  # remove footnote like [1]\n",
    "                if text:\n",
    "                    paragraphs.append(text)\n",
    "\n",
    "            combined = ((title_text + \"\\n\\n\") if title_text else \"\") + \"\\n\\n\".join(paragraphs)\n",
    "            combined = re.sub(r\"\\n{3,}\", \"\\n\\n\", combined).strip()\n",
    "            return combined\n",
    "        except Exception:\n",
    "            return html\n",
    "\n",
    "    async def scrape_article(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Scrape a single Wikipedia article\n",
    "\n",
    "        Args:\n",
    "            url: Article URL\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing article information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Starting to scrape article: {url}\")\n",
    "\n",
    "            async with AsyncWebCrawler(verbose=False) as crawler:\n",
    "                # Scrape article content\n",
    "                result = await crawler.arun(url)\n",
    "\n",
    "                if result.success:\n",
    "                    # Extract article information\n",
    "                    article_data = self._extract_article_info(result, url)\n",
    "                    print(f\"Successfully scraped article: {article_data.get('title', 'Unknown')}\")\n",
    "                    return article_data\n",
    "                else:\n",
    "                    error_msg = result.error_message if hasattr(result, 'error_message') else 'Unknown error'\n",
    "                    print(f\"Scraping failed: {error_msg}\")\n",
    "                    return self._create_error_response(url, error_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping article {url}: {str(e)}\")\n",
    "            return self._create_error_response(url, str(e))\n",
    "\n",
    "    def _extract_article_info(self, result, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract article info from scraping result\"\"\"\n",
    "        try:\n",
    "            content = result.html\n",
    "\n",
    "            # Extract title\n",
    "            title = self._extract_title(content, url)\n",
    "\n",
    "            # Extract main content\n",
    "            main_content = self._extract_main_content(content)\n",
    "\n",
    "            # Extract key sections\n",
    "            sections = self._extract_sections(content)\n",
    "\n",
    "            # Clean content\n",
    "            cleaned_content = self.clean_content(main_content)\n",
    "\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'main_content': cleaned_content,\n",
    "                'sections': sections,\n",
    "                'raw_content_length': len(content),\n",
    "                'cleaned_content_length': len(cleaned_content),\n",
    "                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'status': 'success'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article info: {str(e)}\")\n",
    "            return self._create_error_response(url, f\"Extraction failed: {str(e)}\")\n",
    "\n",
    "    def _extract_title(self, content: str, url: str) -> str:\n",
    "        \"\"\"Extract article title from HTML content\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Prefer title from h1\n",
    "            h1_title = soup.find('h1')\n",
    "            if h1_title:\n",
    "                title_text = h1_title.get_text(strip=True)\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            # Fallback: page <title>\n",
    "            title_tag = soup.find('title')\n",
    "            if title_tag:\n",
    "                title_text = title_tag.get_text(strip=True)\n",
    "                # Remove suffix like \" - Wikipedia\"\n",
    "                if ' - Wikipedia' in title_text:\n",
    "                    title_text = title_text.split(' - Wikipedia')[0]\n",
    "                if title_text and len(title_text) > 0:\n",
    "                    return title_text\n",
    "\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting title: {str(e)}\")\n",
    "            return \"Unknown Title\"\n",
    "\n",
    "    def _extract_main_content(self, content: str) -> str:\n",
    "        \"\"\"Extract main article content from HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            # Remove noisy elements\n",
    "            noise_selectors = [\n",
    "                '#toc', '.mw-references-wrap', 'table.infobox',\n",
    "                'div.navbox', 'table.metadata', 'div.hatnote',\n",
    "                'script', 'style', 'noscript', 'table', 'nav',\n",
    "                '.mw-editsection', '.mw-cite-backlink', '.thumb',\n",
    "                '.image', 'sup.reference', 'span.mw-ref'\n",
    "            ]\n",
    "\n",
    "            for selector in noise_selectors:\n",
    "                for element in main_content.select(selector):\n",
    "                    element.decompose()\n",
    "\n",
    "            # Collect all paragraph text\n",
    "            paragraphs = main_content.find_all('p')\n",
    "            content_list = []\n",
    "\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and len(text) > 10:  # filter out very short paragraphs\n",
    "                    # remove footnote numbers and artifacts\n",
    "                    text = re.sub(r'\\s*\\[\\d+\\]', '', text)\n",
    "                    text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "                    content_list.append(text)\n",
    "\n",
    "            if content_list:\n",
    "                return '\\n\\n'.join(content_list)\n",
    "            else:\n",
    "                return \"Unable to extract valid content\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract content from HTML: {str(e)}\")\n",
    "            return content[:1000]  # return first 1000 chars as fallback\n",
    "\n",
    "    def _extract_sections(self, content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract heading structure from HTML (without content)\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Locate main content area\n",
    "            main_content = soup.select_one('#mw-content-text')\n",
    "            if not main_content:\n",
    "                main_content = soup\n",
    "\n",
    "            sections = []\n",
    "\n",
    "            # Find all heading tags\n",
    "            headings = main_content.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "            for heading in headings:\n",
    "                level = int(heading.name[1])  # h1->1, h2->2, h3->3\n",
    "                heading_text = heading.get_text(strip=True)\n",
    "\n",
    "                # Skip some headings\n",
    "                if self._should_skip_heading(heading_text):\n",
    "                    continue\n",
    "\n",
    "                sections.append({\n",
    "                    'level': level,\n",
    "                    'heading': heading_text\n",
    "                })\n",
    "\n",
    "            return sections\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract sections from HTML: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _should_skip_heading(self, heading_text: str) -> bool:\n",
    "        \"\"\"Decide whether a heading should be skipped\"\"\"\n",
    "        skip_patterns = [\n",
    "            'Contents', 'References', 'External links', 'Further reading',\n",
    "            'See also', 'Notes', 'Bibliography', 'Sources', 'Citations', 'Footnotes'\n",
    "        ]\n",
    "\n",
    "        for pattern in skip_patterns:\n",
    "            if pattern.lower() in heading_text.lower():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def clean_content(self, raw_content: str) -> str:\n",
    "        \"\"\"Clean and pre-process scraped content\"\"\"\n",
    "        if not raw_content:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            # Remove excessive whitespace and newlines\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', raw_content)\n",
    "            cleaned = re.sub(r'[ \\t]+', ' ', cleaned)\n",
    "\n",
    "            # Remove footnote references\n",
    "            cleaned = re.sub(r'\\s*\\[\\d+\\]', '', cleaned)\n",
    "\n",
    "            # Ensure proper blank lines between paragraphs\n",
    "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "\n",
    "            return cleaned.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to clean content: {e}\")\n",
    "            return raw_content\n",
    "\n",
    "    async def scrape_all_articles(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Scrape all articles\"\"\"\n",
    "        print(f\"Starting batch scrape for {len(urls)} articles\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                print(f\"Progress: {i+1}/{len(urls)}\")\n",
    "\n",
    "                # Scrape single article\n",
    "                article_data = await self.scrape_article(url)\n",
    "                all_results.append(article_data)\n",
    "\n",
    "                # Rate limit: add delay between requests\n",
    "                if i < len(urls) - 1:  # not the last one\n",
    "                    print(f\"Waiting {self.rate_limit_delay} seconds...\")\n",
    "                    await asyncio.sleep(self.rate_limit_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping article {url}: {str(e)}\")\n",
    "                all_results.append(self._create_error_response(url, str(e)))\n",
    "\n",
    "        self.scraped_data = all_results\n",
    "        print(f\"Batch scrape completed, successfully scraped {len([r for r in all_results if r.get('status') == 'success'])} articles\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def _create_error_response(self, url: str, error_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create an error response\"\"\"\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': 'Error',\n",
    "            'main_content': '',\n",
    "            'sections': [],\n",
    "            'error': error_message,\n",
    "            'status': 'error',\n",
    "            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "    def save_to_json(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save scraped data to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Full data saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "    def save_content_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save main contents of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                main = item.get('main_content', '').strip()\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if main:\n",
    "                    lines.append(main)\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Main content saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving main content Markdown file: {str(e)}\")\n",
    "\n",
    "    def save_sections_to_markdown(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save section information of the results to a Markdown file\"\"\"\n",
    "        try:\n",
    "            lines = []\n",
    "            for i, item in enumerate(data, start=1):\n",
    "                title = item.get('title', f'Article {i}')\n",
    "                url = item.get('url', '')\n",
    "                sections = item.get('sections', [])\n",
    "\n",
    "                lines.append(f\"# {title}\")\n",
    "                if url:\n",
    "                    lines.append(f\"Source: {url}\")\n",
    "                lines.append(\"\")\n",
    "\n",
    "                if isinstance(sections, list) and sections:\n",
    "                    for s in sections:\n",
    "                        if isinstance(s, dict):\n",
    "                            heading = s.get('heading') or s.get('title') or 'Section'\n",
    "                            content = s.get('content') or s.get('text') or ''\n",
    "                            lines.append(f\"## {heading}\")\n",
    "                            if content:\n",
    "                                lines.append(content)\n",
    "                            lines.append(\"\")\n",
    "                        else:\n",
    "                            lines.append(str(s))\n",
    "                            lines.append(\"\")\n",
    "                else:\n",
    "                    lines.append(\"No sections available\")\n",
    "                    lines.append(\"\")\n",
    "\n",
    "                lines.append(\"---\\n\")\n",
    "\n",
    "            output = \"\\n\".join(lines).strip() + \"\\n\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(output)\n",
    "            print(f\"Section information saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving section information Markdown file: {str(e)}\")\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get scraping summary\"\"\"\n",
    "        if not self.scraped_data:\n",
    "            return {\"message\": \"No scraped data\"}\n",
    "\n",
    "        successful = [r for r in self.scraped_data if r.get('status') == 'success']\n",
    "        failed = [r for r in self.scraped_data if r.get('status') == 'error']\n",
    "\n",
    "        total_content_length = sum(r.get('cleaned_content_length', 0) for r in successful)\n",
    "\n",
    "        return {\n",
    "            'total_articles': len(self.scraped_data),\n",
    "            'successful_scrapes': len(successful),\n",
    "            'failed_scrapes': len(failed),\n",
    "            'total_content_length': total_content_length,\n",
    "            'average_content_length': total_content_length // len(successful) if successful else 0,\n",
    "            'success_rate': len(successful) / len(self.scraped_data) * 100\n",
    "        }\n",
    "\n",
    "print(\"✅ WikipediaScraper class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1Gt1AeCi5JP"
   },
   "outputs": [],
   "source": [
    "# Utility function to convert search keywords to Wikipedia URLs\n",
    "def build_wikipedia_urls(terms):\n",
    "    \"\"\"Generate Wikipedia article URL list from keywords (English articles).\n",
    "    - Rules: Replace spaces with underscores; trim whitespace; deduplicate.\n",
    "    - Note: If keywords are not English or non-standard titles, additional processing/search may be needed.\n",
    "    \"\"\"\n",
    "    if not terms:\n",
    "        return []\n",
    "    base = \"https://en.wikipedia.org/wiki/\"\n",
    "    urls = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        if not isinstance(t, str):\n",
    "            continue\n",
    "        title = t.strip().replace(\" \", \"_\")\n",
    "        if not title:\n",
    "            continue\n",
    "        url = base + title\n",
    "        if url not in seen:\n",
    "            urls.append(url)\n",
    "            seen.add(url)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756824554004,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "Gff1GRT3i5JP",
    "outputId": "f5484163-b39c-4f7f-88ed-73c5d9adb80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据抓取函数定义完成\n"
     ]
    }
   ],
   "source": [
    "async def scrape_scientific_discoveries(search_terms_list: Optional[List[str]] = None):\n",
    "    \"\"\"Scrape scientific discovery related Wikipedia articles\"\"\"\n",
    "    print(\"🚀 Starting to scrape scientific discovery related Wikipedia articles...\")\n",
    "    print(\"=\" * 60)\n",
    "    if search_terms_list != None:\n",
    "        scientific_urls = build_wikipedia_urls(search_terms_list)\n",
    "        print(f\"✅ Generated {len(scientific_urls)} Wikipedia URLs\")\n",
    "    else:\n",
    "        search_terms = [\n",
    "        \"CRISPR\",\n",
    "        \"RNA vaccine\",\n",
    "        \"Gravitational wave\",\n",
    "        \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        \"Ancient DNA\",\n",
    "        \"Water on Mars\",\n",
    "        \"Penicillin\"\n",
    "        ]\n",
    "        scientific_urls = build_wikipedia_urls(search_terms)\n",
    "\n",
    "    print(scientific_urls)\n",
    "    scraper = WikipediaScraper(scientific_urls)\n",
    "    results = await scraper.scrape_all_articles(scientific_urls)\n",
    "\n",
    "    # Save data\n",
    "    scraper.save_to_json(results,\"./scientific_discoveries.json\")\n",
    "    scraper.save_content_to_markdown(results,\"./content.md\")\n",
    "    scraper.save_sections_to_markdown(results,\"./section.md\")\n",
    "\n",
    "    # Display summary\n",
    "    summary = scraper.get_summary()\n",
    "    print(\"\\n📊 Scraping Results Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\n🎉 Scraping completed! Retrieved data for {len(results)} articles\")\n",
    "    return results\n",
    "\n",
    "print(\"✅ Data scraping function definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 65053,
     "status": "ok",
     "timestamp": 1756827058859,
     "user": {
      "displayName": "Jinxuan Li",
      "userId": "11427871420895325867"
     },
     "user_tz": -480
    },
    "id": "QA1LO9gQi5JQ",
    "outputId": "749bc907-3a2a-49cf-a91e-3b9bd038b87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始抓取科学发现相关的维基百科文章...\n",
      "============================================================\n",
      "['https://en.wikipedia.org/wiki/CRISPR', 'https://en.wikipedia.org/wiki/RNA_vaccine', 'https://en.wikipedia.org/wiki/Gravitational_wave', 'https://en.wikipedia.org/wiki/Higgs_boson', 'https://en.wikipedia.org/wiki/Quantum_computing', 'https://en.wikipedia.org/wiki/Ancient_DNA', 'https://en.wikipedia.org/wiki/Water_on_Mars', 'https://en.wikipedia.org/wiki/Penicillin']\n",
      "验证通过，共8个有效URL\n",
      "开始批量爬取 8 篇文章\n",
      "进度: 1/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/CRISPR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">05s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m05s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.</span><span style=\"color: #808000; text-decoration-color: #808000\">7</span><span style=\"color: #008000; text-decoration-color: #008000\">0s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;33m0.\u001b[0m\u001b[33m7\u001b[0m\u001b[32m0s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/CRISPR</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                 |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">76s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/CRISPR\u001b[0m\u001b[32m                                                                 |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m76s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: CRISPR\n",
      "等待 2 秒...\n",
      "进度: 2/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/RNA_vaccine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">63s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m63s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">21s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m21s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/RNA_vaccine</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">85s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/RNA_vaccine\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m85s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: mRNA vaccine\n",
      "等待 2 秒...\n",
      "进度: 3/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Gravitational_wave\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">38s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m38s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">14s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m14s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Gravitational_wave</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                     |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">54s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Gravitational_wave\u001b[0m\u001b[32m                                                     |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m54s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Gravitational wave\n",
      "等待 2 秒...\n",
      "进度: 4/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Higgs_boson\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">36s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m36s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">44s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m44s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Higgs_boson</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">80s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Higgs_boson\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m80s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Higgs boson\n",
      "等待 2 秒...\n",
      "进度: 5/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Quantum_computing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">21s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m21s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">71s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m71s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Quantum_computing</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">93s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Quantum_computing\u001b[0m\u001b[32m                                                      |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m93s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Quantum computing\n",
      "等待 2 秒...\n",
      "进度: 6/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Ancient_DNA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">27s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m27s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">62s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m62s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Ancient_DNA</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                            |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">91s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Ancient_DNA\u001b[0m\u001b[32m                                                            |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m91s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Ancient DNA\n",
      "等待 2 秒...\n",
      "进度: 7/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Water_on_Mars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">34s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m34s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">96s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m96s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Water_on_Mars</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                          |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4.</span><span style=\"color: #008000; text-decoration-color: #008000\">31s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Water_on_Mars\u001b[0m\u001b[32m                                                          |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m31s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Water on Mars\n",
      "等待 2 秒...\n",
      "进度: 8/8\n",
      "开始爬取文章: https://en.wikipedia.org/wiki/Penicillin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.</span><span style=\"color: #008000; text-decoration-color: #008000\">52s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m2.\u001b[0m\u001b[32m52s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">11s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m11s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://en.wikipedia.org/wiki/Penicillin</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3.</span><span style=\"color: #008000; text-decoration-color: #008000\">66s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://en.wikipedia.org/wiki/Penicillin\u001b[0m\u001b[32m                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m3.\u001b[0m\u001b[32m66s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功爬取文章: Penicillin\n",
      "批量爬取完成，成功爬取 8 篇文章\n",
      "完整数据已保存到: ./scientific_discoveries.json\n",
      "主内容已保存到: ./content.md\n",
      "分节信息已保存到: ./section.md\n",
      "\n",
      "📊 抓取结果摘要:\n",
      "========================================\n",
      "total_articles: 8\n",
      "successful_scrapes: 8\n",
      "failed_scrapes: 0\n",
      "total_content_length: 367048\n",
      "average_content_length: 45881\n",
      "success_rate: 100.0\n",
      "\n",
      "🎉 抓取完成！共获取 8 篇文章的数据\n"
     ]
    }
   ],
   "source": [
    "# Execute data scraping - can directly use await in Jupyter\n",
    "scraped_info = await scrape_scientific_discoveries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56kuX5znmOfO"
   },
   "source": [
    "## Part2 Structured Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ErSfBUyuPMa8"
   },
   "outputs": [],
   "source": [
    "# # Simple JSON file reader\n",
    "# import json\n",
    "\n",
    "# # Read the saved JSON data\n",
    "# with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# print(f\"✅ Successfully read data of {len(data)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "xaiOwWS0PMa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scientific discovery domain Pydantic model definition completed\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model designed for scientific discovery domain\n",
    "class ScientificDiscoveryExtraction(BaseModel):\n",
    "    \"\"\"Structured data model for scientific discovery information\"\"\"\n",
    "\n",
    "    # Basic information\n",
    "    primary_name: str = Field(description=\"Primary name of the scientific discovery or technology\")\n",
    "\n",
    "    # Discoverers and time\n",
    "    discoverers: List[str] = Field(description=\"List of names of main discoverers or researchers\")\n",
    "    discovery_years: List[str] = Field(description=\"Most important discovery year, strongly recommend returning only one year. Must select the single year that best represents the core breakthrough of the technology/discovery from the article. Priority: key technical implementation > important paper publication > initial concept proposal. For example, for CRISPR, choose 2012 (Doudna and Charpentier's key paper) rather than 1987 (first sequence discovery). Avoid returning multiple years.\")\n",
    "    discovery_timeline: List[str] = Field(description=\"Timeline from initial to final discovery\")\n",
    "\n",
    "    # Technical details\n",
    "    mechanism: str = Field(description=\"Basic working principle or mechanism of the technology\")\n",
    "    key_features: List[str] = Field(description=\"Main features or advantages of the technology\")\n",
    "\n",
    "    # Applications and impact\n",
    "    applications: List[str] = Field(description=\"Main application fields or uses\")\n",
    "    significance: str = Field(description=\"Scientific or social significance of the discovery\")\n",
    "\n",
    "    # Optional additional information\n",
    "    institutions: Optional[List[str]] = Field(default=None, description=\"Related research institutions or universities\")\n",
    "    awards: Optional[List[str]] = Field(default=None, description=\"Important awards or honors received\")\n",
    "\n",
    "    # Metadata\n",
    "    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat(), description=\"Data extraction time\")\n",
    "\n",
    "print(\"✅ Scientific discovery domain Pydantic model definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "9FdWkCKFPMa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Structured data extractor class definition completed\n"
     ]
    }
   ],
   "source": [
    "class StructuredDataExtractor:\n",
    "    \"\"\"A class for structured data extraction using the OpenAI API\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None, model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the extractor\n",
    "\n",
    "        Args:\n",
    "            api_key: OpenAI API key (optional, will use environment variable OPENAI_API_KEY first)\n",
    "            model: Model name to use\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        self.model = model\n",
    "        self.client = None\n",
    "\n",
    "        # Prefer the provided API key, then the environment variable; if not found, ask interactively\n",
    "        final_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not final_api_key:\n",
    "            try:\n",
    "                from getpass import getpass\n",
    "                final_api_key = getpass('Please enter OPENAI_API_KEY (input will be hidden): ')\n",
    "            except Exception:\n",
    "                final_api_key = input('Please enter OPENAI_API_KEY: ')\n",
    "\n",
    "        if final_api_key:\n",
    "            self.client = openai.OpenAI(api_key=final_api_key)\n",
    "            print(\"✅ OpenAI client initialized successfully\")\n",
    "        else:\n",
    "            print(\"⚠️ No OpenAI API key found, related features will be limited\")\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Retry configuration\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2  # seconds\n",
    "\n",
    "    def extract_structured_data(self, content: str, model: str = None) -> Optional[ScientificDiscoveryExtraction]:\n",
    "        \"\"\"\n",
    "        Extract structured data using OpenAI structured output\n",
    "\n",
    "        Args:\n",
    "            content: Text content to extract\n",
    "            model: Model to use (optional)\n",
    "\n",
    "        Returns:\n",
    "            ScientificDiscoveryExtraction: Extracted structured data\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            self.logger.error(\"OpenAI client not initialized, please provide an API key\")\n",
    "            return None\n",
    "\n",
    "        model_to_use = model or self.model\n",
    "\n",
    "        # Build system prompt\n",
    "        system_prompt = \"\"\"\n",
    "        You are a professional scientific literature analyst. Please extract structured information \n",
    "        from the given scientific article content.\n",
    "\n",
    "        Extract the following information:\n",
    "        1. Main name of the scientific discovery or technology\n",
    "        2. Main discoverer(s) or researcher(s)\n",
    "        3. Important discovery time points\n",
    "        4. Timeline of the discovery\n",
    "        5. Basic working principle of the technology\n",
    "        6. Main features and advantages\n",
    "        7. Application areas\n",
    "        8. Scientific significance\n",
    "        9. Related institutions (if any)\n",
    "        10. Awards received (if any)\n",
    "\n",
    "        **Important extraction rules**:\n",
    "        - For discovery_years, follow these rules:\n",
    "          * If a range of years is mentioned (e.g., \"2010-2012\", \"from 2008 to 2011\"), \n",
    "            determine which year is most relevant to the key discovery in the title\n",
    "          * Prefer the year associated with the key breakthrough, first publication, or major experiment success\n",
    "          * If multiple years are mentioned and it's unclear which is more important, return the earliest one\n",
    "          * Example: If the title is \"CRISPR gene-editing technology\" and the text says \n",
    "            \"Research started in 2008, breakthrough in 2012\", choose 2012\n",
    "          * If completely uncertain, return the earliest year in the range\n",
    "\n",
    "        If some information is not explicitly mentioned, try to infer from the context, \n",
    "        or state \"unclear\" in the corresponding field.\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Please extract structured information from the following scientific article content:\n",
    "\n",
    "        {content[:8000]}  # Limit length to avoid token issues\n",
    "\n",
    "        Please extract information according to the ScientificDiscoveryExtraction model.\n",
    "\n",
    "        **Special note on discovery_years**:\n",
    "        - Carefully read the article title and content to understand the core of the scientific discovery\n",
    "        - If a year range is present, decide which year best represents the key breakthrough\n",
    "        - Priority: first major publication, critical technical success, or key experimental result\n",
    "        - If unclear, choose the earliest year in the range\n",
    "        \"\"\"\n",
    "\n",
    "        # Try extraction with retries\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                self.logger.info(f\"Starting extraction attempt {attempt + 1}...\")\n",
    "\n",
    "                response = self.client.beta.chat.completions.parse(\n",
    "                    model=model_to_use,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    response_format=ScientificDiscoveryExtraction,\n",
    "                    temperature=0.1  # Low temperature for consistency\n",
    "                )\n",
    "\n",
    "                # Extract structured data\n",
    "                extracted_data = response.choices[0].message.parsed\n",
    "\n",
    "                if extracted_data:\n",
    "                    self.logger.info(\"✅ Structured data extraction succeeded\")\n",
    "                    return extracted_data\n",
    "                else:\n",
    "                    self.logger.warning(\"⚠️ API returned empty data\")\n",
    "\n",
    "            except openai.RateLimitError as e:\n",
    "                self.logger.warning(f\"Rate limit error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (2 ** attempt))  # exponential backoff\n",
    "\n",
    "            except openai.APIError as e:\n",
    "                self.logger.error(f\"OpenAI API error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unknown error (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "\n",
    "        self.logger.error(\"❌ All extraction attempts failed\")\n",
    "        return None\n",
    "\n",
    "    def batch_extract(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process multiple articles in batch\n",
    "\n",
    "        Args:\n",
    "            articles: List of article data\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of extraction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        self.logger.info(f\"Starting batch processing of {len(articles)} articles...\")\n",
    "\n",
    "        for i, article in enumerate(articles):\n",
    "            try:\n",
    "                self.logger.info(f\"Processing article {i+1}/{len(articles)}: {article.get('title', 'Unknown')}\")\n",
    "\n",
    "                # Check article status\n",
    "                if article.get('status') != 'success':\n",
    "                    self.logger.warning(f\"Skipping failed article: {article.get('error', 'Unknown error')}\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'skipped',\n",
    "                        'extraction_error': article.get('error', 'Article scraping failed'),\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Extract main content\n",
    "                content = article.get('main_content', '')\n",
    "                if not content:\n",
    "                    self.logger.warning(\"Article content is empty, skipping\")\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Empty content',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Run structured extraction\n",
    "                extracted_data = self.extract_structured_data(content)\n",
    "\n",
    "                if extracted_data:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'success',\n",
    "                        'extraction_error': None,\n",
    "                        'structured_data': extracted_data.dict()\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'article_info': article,\n",
    "                        'extraction_status': 'failed',\n",
    "                        'extraction_error': 'Extraction failed after retries',\n",
    "                        'structured_data': None\n",
    "                    })\n",
    "\n",
    "                # Delay to avoid rate limit\n",
    "                if i < len(articles) - 1:\n",
    "                    time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing article: {e}\")\n",
    "                results.append({\n",
    "                    'article_info': article,\n",
    "                    'extraction_status': 'error',\n",
    "                    'extraction_error': str(e),\n",
    "                    'structured_data': None\n",
    "                })\n",
    "\n",
    "        # Summary statistics\n",
    "        successful = len([r for r in results if r['extraction_status'] == 'success'])\n",
    "        failed = len([r for r in results if r['extraction_status'] in ['failed', 'error']])\n",
    "        skipped = len([r for r in results if r['extraction_status'] == 'skipped'])\n",
    "\n",
    "        self.logger.info(f\"Batch processing finished: Success {successful}, Failed {failed}, Skipped {skipped}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_extraction_results(self, results: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"\n",
    "        Save extraction results to a file\n",
    "\n",
    "        Args:\n",
    "            results: List of extraction results\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n",
    "            self.logger.info(f\"✅ Extraction results saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Error saving file: {e}\")\n",
    "\n",
    "print(\"✅ Structured data extractor class definition completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "S3QYtLVgPMa-"
   },
   "outputs": [],
   "source": [
    "def perform_structured_extraction(scraper_result):\n",
    "    # Perform structured data extraction - this was the missing call code!\n",
    "    print(\"🚀 Starting structured data extraction...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Check if API key is set\n",
    "    if not os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"⚠️ Please set the OpenAI API key first:\")\n",
    "        print(\"   os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\")\n",
    "        print(\"   Then re-run this cell\")\n",
    "    else:\n",
    "        try:\n",
    "            # 1. Initialize the extractor (using the previously defined class)\n",
    "            extractor = StructuredDataExtractor()\n",
    "\n",
    "            if scraper_result is not None:\n",
    "                scraped_articles = scraper_result\n",
    "                print(f\"📖 Using the provided scraping results\")\n",
    "            else:\n",
    "                print(f\"📖 No scraping results provided, trying to load from file\")\n",
    "                # 2. Read previously scraped data\n",
    "                with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "                    scraped_articles = json.load(f)\n",
    "\n",
    "            print(f\"📖 Successfully loaded {len(scraped_articles)} articles\")\n",
    "\n",
    "            # 3. Perform batch structured extraction (call the core method!)\n",
    "            print(\"🔄 Starting batch structured extraction...\")\n",
    "            extraction_results = extractor.batch_extract(scraped_articles)\n",
    "\n",
    "            # 4. Save extraction results (call the save method!)\n",
    "            extractor.save_extraction_results(extraction_results, './structured_extractions.json')\n",
    "\n",
    "            # 5. Show extraction statistics\n",
    "            successful = [r for r in extraction_results if r['extraction_status'] == 'success']\n",
    "            failed = [r for r in extraction_results if r['extraction_status'] != 'success']\n",
    "\n",
    "            print(f\"\\n📊 Extraction statistics:\")\n",
    "            print(f\"✅ Successfully extracted: {len(successful)} articles\")\n",
    "            print(f\"❌ Failed to extract: {len(failed)} articles\")\n",
    "\n",
    "            # 6. Display the first successful extraction result\n",
    "            if successful:\n",
    "                first_result = successful[0]\n",
    "                print(f\"\\n📄 Sample extraction result - {first_result['article_info']['title']}:\")\n",
    "                print(\"-\" * 50)\n",
    "                structured = first_result['structured_data']\n",
    "                print(f\"📝 Primary name: {structured['primary_name']}\")\n",
    "                print(f\"👥 Discoverers: {', '.join(structured['discoverers'])}\")\n",
    "                print(f\"📅 Discovery years: {', '.join(structured['discovery_years'])}\")\n",
    "                print(f\"🔬 Applications: {len(structured['applications'])} items\")\n",
    "                print(f\"⭐ Key features: {len(structured['key_features'])} items\")\n",
    "\n",
    "            print(f\"\\n✅ Structured extraction completed! Results saved to './structured_extractions.json'\")\n",
    "\n",
    "            return extraction_results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Execution error: {str(e)}\")\n",
    "            print(\"Please check the API key settings and ensure the data file exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUgMw9L6PMa-"
   },
   "source": [
    "## Part 3: Function Calling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIAnalyzer:\n",
    "    \"\"\"OpenAI分析器类，用于智能匹配和数据分析\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"初始化OpenAI客户端\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API密钥，如果为None则从环境变量获取\n",
    "        \"\"\"\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv('OPENAI_API_KEY')\n",
    "            if not api_key:\n",
    "                raise ValueError(\"请设置OPENAI_API_KEY环境变量或传入api_key参数\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        print(\"✅ OpenAI客户端初始化成功\")\n",
    "    \n",
    "    def analyze_query_match(self, user_query: str, available_names: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"分析用户查询与可用名称的匹配度\n",
    "        \n",
    "        Args:\n",
    "            user_query: 用户输入的查询\n",
    "            available_names: 可用的名称列表\n",
    "            \n",
    "        Returns:\n",
    "            包含最佳匹配和置信度的字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 构建提示词\n",
    "            prompt = f\"\"\"\n",
    "            用户查询: \"{user_query}\"\n",
    "            \n",
    "            可用的科学发现名称列表:\n",
    "            {json.dumps(available_names, ensure_ascii=False, indent=2)}\n",
    "            \n",
    "            请分析用户查询与可用名称的匹配度，返回JSON格式结果:\n",
    "            {{\n",
    "                \"best_match\": \"最匹配的名称\",\n",
    "                \"confidence\": 0.0-1.0的置信度,\n",
    "                \"reasoning\": \"匹配原因\",\n",
    "                \"alternative_matches\": [\"其他可能的匹配项\"]\n",
    "            }}\n",
    "            \n",
    "            注意：\n",
    "            1. 考虑同义词、缩写、不同表达方式\n",
    "            2. 如果找不到合适匹配，confidence设为0.0\n",
    "            3. 只返回JSON，不要其他文字\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的科学数据匹配助手，擅长分析查询与数据的匹配度。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ OpenAI分析出错: {e}\")\n",
    "            return {\n",
    "                \"best_match\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"reasoning\": f\"分析失败: {e}\",\n",
    "                \"alternative_matches\": []\n",
    "            }\n",
    "    \n",
    "    def suggest_search_strategy(self, user_query: str, data_sample: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"建议搜索策略\n",
    "        \n",
    "        Args:\n",
    "            user_query: 用户查询\n",
    "            data_sample: 数据样本\n",
    "            \n",
    "        Returns:\n",
    "            搜索策略建议\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            用户查询: \"{user_query}\"\n",
    "            \n",
    "            数据样本结构:\n",
    "            {json.dumps(data_sample[:3], ensure_ascii=False, indent=2)}\n",
    "            \n",
    "            请分析用户查询意图，建议最佳搜索策略，返回JSON格式:\n",
    "            {{\n",
    "                \"search_type\": \"name|scientist|application|general\",\n",
    "                \"search_keywords\": [\"关键词1\", \"关键词2\"],\n",
    "                \"explanation\": \"搜索策略说明\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个科学数据搜索策略专家。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            return json.loads(response.choices[0].message.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 搜索策略分析出错: {e}\")\n",
    "            return {\n",
    "                \"search_type\": \"general\",\n",
    "                \"search_keywords\": [user_query],\n",
    "                \"explanation\": f\"分析失败，使用通用搜索: {e}\"\n",
    "            }\n",
    "\n",
    "print(\"✅ OpenAIAnalyzer类定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F64QtW3jNh4"
   },
   "source": [
    "***3.1 Scientific Data Query***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "c_SeoN47jNh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ScientificDataQuery class definition completed\n"
     ]
    }
   ],
   "source": [
    "class ScientificDataQuery:\n",
    "    \"\"\"Scientific discovery data query class - simple data access functions\n",
    "    \n",
    "    This class provides basic data access functions. Fuzzy matching is handled by OpenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file: str = \"./structured_extractions.json\", data: Optional[List[Dict[str, Any]]] = None):\n",
    "        \"\"\"Initialize the query class with data loading\n",
    "        \n",
    "        Args:\n",
    "            data_file: Path to the structured data file (default: \"./structured_extractions.json\")\n",
    "            data: Optional in-memory data to use instead of loading from file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if data is not None:\n",
    "                # Use in-memory data directly\n",
    "                self.raw_data = data\n",
    "                print(f\"✅ Using provided in-memory data with {len(data)} items\")\n",
    "            else:\n",
    "                # Load from file (default behavior for function calls)\n",
    "                with open(data_file, 'r', encoding='utf-8') as f:\n",
    "                    self.raw_data = json.load(f)\n",
    "                print(f\"✅ Loaded data from {data_file}\")\n",
    "\n",
    "            # Extract successful structured data\n",
    "            self.discoveries = []\n",
    "            for item in self.raw_data:\n",
    "                if isinstance(item, dict):\n",
    "                    if item.get('extraction_status') == 'success' and item.get('extracted_data'):\n",
    "                        # Merge extracted data with metadata\n",
    "                        extracted_data = item['extracted_data']\n",
    "                        if isinstance(extracted_data, dict):\n",
    "                            self.discoveries.append({**item, **extracted_data})\n",
    "                        else:\n",
    "                            print(f\"⚠️ Skipping item with malformed 'extracted_data': {item.get('title', 'Unknown Title')}\")\n",
    "                    elif isinstance(item, dict) and 'primary_name' in item:\n",
    "                        # Already structured data\n",
    "                        self.discoveries.append(item)\n",
    "\n",
    "            print(f\"✅ Successfully loaded {len(self.discoveries)} scientific discovery data\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Error: Data file not found at {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"❌ Error: Could not decode JSON from {data_file}\")\n",
    "            self.discoveries = []\n",
    "        except Exception as e:\n",
    "            print(f\"❌ An unexpected error occurred during data loading: {e}\")\n",
    "            self.discoveries = []\n",
    "\n",
    "    def find_discovery_by_name(self, discovery_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Find a scientific discovery by exact name match\"\"\"\n",
    "        discovery_name_lower = discovery_name.lower().strip()\n",
    "        for discovery in self.discoveries:\n",
    "            if discovery.get('primary_name', '').lower() == discovery_name_lower:\n",
    "                return discovery\n",
    "        return None\n",
    "\n",
    "    def find_discoveries_by_scientist(self, scientist_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find all discoveries by a scientist (exact name match)\"\"\"\n",
    "        scientist_name_lower = scientist_name.lower().strip()\n",
    "        matched_discoveries = []\n",
    "        \n",
    "        for discovery in self.discoveries:\n",
    "            for discoverer in discovery.get('discoverers', []):\n",
    "                if discoverer.lower() == scientist_name_lower:\n",
    "                    matched_discoveries.append(discovery)\n",
    "                    break\n",
    "        \n",
    "        return matched_discoveries\n",
    "\n",
    "    def get_all_discoveries(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all available scientific discoveries\"\"\"\n",
    "        return self.discoveries\n",
    "\n",
    "    def get_discovery_details(self, discovery_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get comprehensive details about a specific scientific discovery\"\"\"\n",
    "        discovery = self.find_discovery_by_name(discovery_name)\n",
    "        if discovery:\n",
    "            return {\n",
    "                \"primary_name\": discovery.get('primary_name'),\n",
    "                \"discoverers\": discovery.get('discoverers', []),\n",
    "                \"discovery_years\": discovery.get('discovery_years', []),\n",
    "                \"mechanism\": discovery.get('mechanism', ''),\n",
    "                \"key_features\": discovery.get('key_features', []),\n",
    "                \"applications\": discovery.get('applications', []),\n",
    "                \"significance\": discovery.get('significance', ''),\n",
    "                \"institutions\": discovery.get('institutions', []),\n",
    "                \"awards\": discovery.get('awards', []),\n",
    "                \"url\": discovery.get('url', '')\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def search_by_application(self, application_keyword: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for scientific discoveries related to a specific application keyword\"\"\"\n",
    "        results = []\n",
    "        keyword_lower = application_keyword.lower()\n",
    "        for discovery in self.discoveries:\n",
    "            applications = discovery.get('applications', [])\n",
    "            if any(keyword_lower in app.lower() for app in applications):\n",
    "                results.append(discovery)\n",
    "        return results\n",
    "\n",
    "print(\"✅ ScientificDataQuery class definition completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8auVGAjNh4"
   },
   "source": [
    "***3.2 Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "D5EpLGIBjNh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Part 3 function definitions completed\n"
     ]
    }
   ],
   "source": [
    "def compare_discoveries(discovery1: str, discovery2: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two scientific discoveries\n",
    "\n",
    "    Args:\n",
    "        discovery1: Name of the first discovery\n",
    "        discovery2: Name of the second discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed comparison analysis\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "\n",
    "    # Find the two discoveries\n",
    "    disc1 = query.find_discovery_by_name(discovery1)\n",
    "    disc2 = query.find_discovery_by_name(discovery2)\n",
    "\n",
    "    if not disc1:\n",
    "        return f\"❌ Discovery not found: {discovery1}\"\n",
    "    if not disc2:\n",
    "        return f\"❌ Discovery not found: {discovery2}\"\n",
    "\n",
    "    # Build comparison analysis\n",
    "    comparison = f\"\"\"\n",
    "    🔬 Scientific Discovery Comparison\n",
    "    {'='*50}\n",
    "\n",
    "    📍 Discovery 1: {disc1['primary_name']}\n",
    "    📍 Discovery 2: {disc2['primary_name']}\n",
    "\n",
    "    👥 Discoverers:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['discoverers'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['discoverers'])}\n",
    "\n",
    "    📅 Discovery Years:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['discovery_years'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['discovery_years'])}\n",
    "\n",
    "    🔧 Mechanisms:\n",
    "    • {disc1['primary_name']}: {disc1['mechanism']}\n",
    "    • {disc2['primary_name']}: {disc2['mechanism']}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['key_features'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['key_features'])}\n",
    "\n",
    "    🎯 Applications:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1['applications'])}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2['applications'])}\n",
    "\n",
    "    🏆 Awards:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1.get('awards', ['No award information']))}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2.get('awards', ['No award information']))}\n",
    "\n",
    "    💡 Significance:\n",
    "    • {disc1['primary_name']}: {disc1['significance']}\n",
    "    • {disc2['primary_name']}: {disc2['significance']}\n",
    "\n",
    "    🏛️ Institutions:\n",
    "    • {disc1['primary_name']}: {', '.join(disc1.get('institutions', ['No institution info']))}\n",
    "    • {disc2['primary_name']}: {', '.join(disc2.get('institutions', ['No institution info']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "def get_research_timeline(scientist: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the research timeline of a scientist\n",
    "\n",
    "    Args:\n",
    "        scientist: Scientist name\n",
    "\n",
    "    Returns:\n",
    "        Timeline of the scientist's discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discoveries = query.find_discoveries_by_scientist(scientist)\n",
    "\n",
    "    if not discoveries:\n",
    "        return f\"❌ No discoveries found for scientist '{scientist}'\"\n",
    "\n",
    "    # Sort by discovery year\n",
    "    sorted_discoveries = sorted(discoveries,\n",
    "                                key=lambda x: int(x['discovery_years'][0]) if x['discovery_years'] else 0)\n",
    "\n",
    "    timeline = f\"\"\"\n",
    "    👨‍🔬 Research Timeline of {scientist}\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(sorted_discoveries, 1):\n",
    "        timeline += f\"\"\"\n",
    "    🔬 Discovery {i}: {discovery['primary_name']}\n",
    "    📅 Year: {', '.join(discovery['discovery_years'])}\n",
    "    🎯 Significance: {discovery['significance']}\n",
    "    🏛️ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "    🏆 Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "    \"\"\"\n",
    "\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def search_by_application(application: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for discoveries by application area\n",
    "\n",
    "    Args:\n",
    "        application: Application keyword\n",
    "\n",
    "    Returns:\n",
    "        List of relevant discoveries\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    all_discoveries = query.get_all_discoveries()\n",
    "\n",
    "    application_lower = application.lower()\n",
    "    relevant_discoveries = []\n",
    "\n",
    "    for discovery in all_discoveries:\n",
    "        # Search within applications\n",
    "        for app in discovery.get('applications', []):\n",
    "            if application_lower in app.lower():\n",
    "                relevant_discoveries.append(discovery)\n",
    "                break\n",
    "\n",
    "    if not relevant_discoveries:\n",
    "        return f\"❌ No discoveries found related to '{application}'\"\n",
    "\n",
    "    result = f\"\"\"\n",
    "    🎯 Discoveries related to '{application}'\n",
    "    {'='*50}\n",
    "    \"\"\"\n",
    "\n",
    "    for i, discovery in enumerate(relevant_discoveries, 1):\n",
    "        result += f\"\"\"\n",
    "        🔬 Discovery {i}: {discovery['primary_name']}\n",
    "        👥 Discoverers: {', '.join(discovery['discoverers'])}\n",
    "        📅 Year: {', '.join(discovery['discovery_years'])}\n",
    "        🎯 Applications: {', '.join(discovery['applications'])}\n",
    "        💡 Significance: {discovery['significance']}\n",
    "        \"\"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_discovery_details(discovery_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information of a specific discovery\n",
    "\n",
    "    Args:\n",
    "        discovery_name: Name of the scientific discovery\n",
    "\n",
    "    Returns:\n",
    "        Detailed information\n",
    "    \"\"\"\n",
    "\n",
    "    query = ScientificDataQuery()\n",
    "    discovery = query.find_discovery_by_name(discovery_name)\n",
    "\n",
    "    if not discovery:\n",
    "        return f\"❌ Discovery not found: {discovery_name}\"\n",
    "\n",
    "    details = f\"\"\"\n",
    "    🔬 {discovery['primary_name']} - Details\n",
    "    {'='*60}\n",
    "\n",
    "    👥 Discoverers: {', '.join(discovery['discoverers'])}\n",
    "\n",
    "    📅 Discovery Years: {', '.join(discovery['discovery_years'])}\n",
    "\n",
    "    🔧 Mechanism:\n",
    "    {discovery['mechanism']}\n",
    "\n",
    "    ⭐ Key Features:\n",
    "    {chr(10).join([f\"  • {feature}\" for feature in discovery['key_features']])}\n",
    "\n",
    "    🎯 Applications:\n",
    "    {chr(10).join([f\"  • {app}\" for app in discovery['applications']])}\n",
    "\n",
    "    💡 Significance:\n",
    "    {discovery['significance']}\n",
    "\n",
    "    🏛️ Institutions: {', '.join(discovery.get('institutions', ['None']))}\n",
    "\n",
    "    🏆 Awards: {', '.join(discovery.get('awards', ['None']))}\n",
    "\n",
    "    🔗 URL: {discovery.get('url', 'No URL available')}\n",
    "    \"\"\"\n",
    "\n",
    "    return details\n",
    "\n",
    "print(\"✅ Part 3 function definitions completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2IBCB_ojNh5"
   },
   "source": [
    "***3.3 Function Scheme***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "LysKzxaVjNh5"
   },
   "outputs": [],
   "source": [
    "FUNCTION_SCHEMAS = [\n",
    "    {\n",
    "        \"name\": \"compare_discoveries\",\n",
    "        \"description\": \"Compare two scientific breakthroughs in detail. Accepts discovery names, keywords, or partial names (e.g., 'CRISPR', '基因编辑', 'mRNA', '疫苗', '引力波'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery1\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"First scientific discovery name or keyword (supports full name, abbreviation, partial match, or Chinese/English keyword)\"\n",
    "                },\n",
    "                \"discovery2\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Second scientific discovery name or keyword (supports full name, abbreviation, partial match, or Chinese/English keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery1\", \"discovery2\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_research_timeline\",\n",
    "        \"description\": \"Get chronological timeline of a scientist's major discoveries. Accepts names in full, partial, or last name only (e.g., 'Jennifer Doudna', 'Doudna', '爱因斯坦', 'Einstein'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"scientist\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientist's name or keyword (supports full name, partial name, or last name in English/Chinese)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"scientist\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_by_application\",\n",
    "        \"description\": \"Search for scientific discoveries by their application domain or usage field. Accepts keywords in English or Chinese (e.g., 'medicine', 'agriculture', 'biotechnology', '医学', '农业').\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"application\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Application domain keyword (e.g., 'medicine', 'agriculture', 'biotechnology', '材料科学')\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"application\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_discovery_details\",\n",
    "        \"description\": \"Get comprehensive details about a specific scientific discovery. Accepts discovery names, abbreviations, or keywords (e.g., 'CRISPR', '基因编辑', 'mRNA', '疫苗'). Supports fuzzy matching in both English and Chinese.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"discovery_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Scientific discovery name or keyword (supports partial name, abbreviation, or Chinese/English keyword)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"discovery_name\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJWvrdXdjNh5"
   },
   "source": [
    "***3.4 Assistant***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "W4I_kVHOjNh5"
   },
   "outputs": [],
   "source": [
    "class ScientificResearchAssistant:\n",
    "    \"\"\"科学发现交互式研究助手\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化助手\"\"\"\n",
    "        self.client = None\n",
    "\n",
    "        # 检查API密钥（支持交互式输入）\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            try:\n",
    "                from getpass import getpass\n",
    "                api_key = getpass('请输入 OPENAI_API_KEY（输入内容不可见）：')\n",
    "            except Exception:\n",
    "                api_key = input('请输入 OPENAI_API_KEY：')\n",
    "\n",
    "        if api_key:\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "            print(\"✅ 科学研究助手初始化成功\")\n",
    "        else:\n",
    "            print(\"⚠️ 未设置OpenAI API密钥，功能将受限\")\n",
    "\n",
    "        # 可用函数映射\n",
    "        self.available_functions = {\n",
    "            \"compare_discoveries\": compare_discoveries,\n",
    "            \"get_research_timeline\": get_research_timeline,\n",
    "            \"search_by_application\": search_by_application,\n",
    "            \"get_discovery_details\": get_discovery_details\n",
    "        }\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"与助手对话\"\"\"\n",
    "\n",
    "        if not self.client:\n",
    "            return \"❌ OpenAI API未配置，无法进行智能对话\"\n",
    "\n",
    "        try:\n",
    "            # 调用OpenAI API with Function Calling\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\":\"\"\"\n",
    "                        你是一个专业的科学发现研究助手。你可以帮助用户：\n",
    "                        1. 比较不同的科学发现\n",
    "                        2. 查看科学家的研究时间线\n",
    "                        3. 根据应用领域搜索发现\n",
    "                        4. 获取特定发现的详细信息\n",
    "                        请根据用户的问题，选择合适的函数来回答。\"\"\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                functions=FUNCTION_SCHEMAS,\n",
    "                function_call=\"auto\"\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "\n",
    "            # 检查是否需要函数调用\n",
    "            if message.function_call:\n",
    "                function_name = message.function_call.name\n",
    "                function_args = json.loads(message.function_call.arguments)\n",
    "\n",
    "                print(f\"🔧 调用函数: {function_name}\")\n",
    "                print(f\"📝 参数: {function_args}\")\n",
    "\n",
    "                # 执行函数\n",
    "                if function_name in self.available_functions:\n",
    "                    result = self.available_functions[function_name](**function_args)\n",
    "                    return result\n",
    "                else:\n",
    "                    return f\"❌ 未知函数: {function_name}\"\n",
    "\n",
    "            else:\n",
    "                return message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ 处理请求时发生错误: {str(e)}\"\n",
    "\n",
    "    def show_available_data(self) -> str:\n",
    "        \"\"\"显示可用的数据\"\"\"\n",
    "        query = ScientificDataQuery()\n",
    "        discoveries = query.get_all_discoveries()\n",
    "\n",
    "        if not discoveries:\n",
    "            return \"❌ 暂无可用数据\"\n",
    "\n",
    "        result = \"📚 可用的科学发现数据:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "\n",
    "        for i, discovery in enumerate(discoveries, 1):\n",
    "            result += f\"{i}. {discovery['primary_name']}\\n\"\n",
    "            result += f\"   发现者: {', '.join(discovery['discoverers'])}\\n\"\n",
    "            result += f\"   年份: {', '.join(discovery['discovery_years'])}\\n\\n\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T6ETPfTjNh5"
   },
   "source": [
    "***Instance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "y_-Fs_Y4jNh5"
   },
   "outputs": [],
   "source": [
    "def assistant_chat(example_queries: List[str]):\n",
    "    \"\"\"主函数演示\"\"\"\n",
    "    print(\"🔬 科学发现交互式研究助手\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 初始化助手\n",
    "    assistant = ScientificResearchAssistant()\n",
    "\n",
    "    # 显示可用数据\n",
    "    print(assistant.show_available_data())\n",
    "\n",
    "    if example_queries !=[]:\n",
    "        print(\"✅ 使用传入的示例对话\")\n",
    "    else:\n",
    "        print(\"✅ 未传入示例对话，使用默认示例\")\n",
    "        example_queries = [\n",
    "            \"请比较CRISPR和量子计算两个科学发现\",\n",
    "            \"Jennifer Doudna有哪些重要研究发现？\",\n",
    "            \"搜索在医学领域应用的科学发现\",\n",
    "            \"给我详细介绍一下CRISPR技术\"\n",
    "        ]\n",
    "        for q in example_queries:\n",
    "            print(\"示例问题:\", q)\n",
    "\n",
    "    # 进行对话\n",
    "    for q in example_queries:\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer:\", assistant.chat(q))\n",
    "\n",
    "    return assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y63DDOU4yRoa"
   },
   "source": [
    "# Part 4: Integration and Demonstration\n",
    "\n",
    "## 🎯 目标\n",
    "创建一个全面的演示，展示所有组件如何协同工作，包括：\n",
    "- 完整的端到端工作流程\n",
    "- 所有主要功能演示\n",
    "- 错误处理示例\n",
    "- 数据可视化\n",
    "- 可运行的完整管道\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "ivyqLP2zyRoa"
   },
   "outputs": [],
   "source": [
    "search_terms = [\n",
    "        \"CRISPR\",\n",
    "        \"RNA vaccine\",\n",
    "        \"Gravitational wave\",\n",
    "        \"Higgs boson\",\n",
    "        \"Quantum computing\",\n",
    "        \"Ancient DNA\",\n",
    "        \"Water on Mars\",\n",
    "        \"Penicillin\"\n",
    "        ]\n",
    "\n",
    "example_queries = [\n",
    "    \"Please compare the scientific discoveries of CRISPR and quantum computing.\",\n",
    "    \"What are the major research discoveries of Jennifer Doudna?\",\n",
    "    \"Search for scientific discoveries applied in the field of medicine.\",\n",
    "    \"Give me a detailed introduction to CRISPR technology.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "mlivBX99yRoa",
    "outputId": "f760f220-24c8-4cb2-d53e-8b71d141c387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 综合演示类定义完成\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveDemo:\n",
    "    \"\"\"综合演示类 - 展示完整的端到端工作流程\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化演示系统\"\"\"\n",
    "        self.scraper_result = None\n",
    "        self.extractor_result = None\n",
    "        self.assistant = None\n",
    "        self.query = None\n",
    "\n",
    "        print(\"🚀 综合演示系统初始化完成\")\n",
    "\n",
    "    async def run_complete_pipeline(self):\n",
    "        \"\"\"运行完整的端到端管道\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"🔬 科学发现研究系统 - 完整演示\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        try:\n",
    "            # Part 1: 执行数据抓取（若传空列表则使用默认搜索词）\n",
    "            # self.scraper_result = await scrape_scientific_discoveries(search_terms)\n",
    "\n",
    "            with open('./scientific_discoveries.json', 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            self.scraper_result = data\n",
    "\n",
    "            # Part 2: 执行结构化数据提取（基于抓取结果或已存在文件）\n",
    "            self.extractor_result = perform_structured_extraction(self.scraper_result)\n",
    "\n",
    "            # 初始化查询器，供可视化与助手使用\n",
    "            self.query = ScientificDataQuery()\n",
    "\n",
    "            # Part 3: 初始化交互式助手并进行示例对话\n",
    "            self.assistant = assistant_chat(example_queries)\n",
    "\n",
    "            # 可视化演示\n",
    "            self._demo_visualization()\n",
    "\n",
    "            print(\"\\n✅ 完整演示完成！\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 演示过程中发生错误: {str(e)}\")\n",
    "\n",
    "\n",
    "    def _demo_visualization(self):\n",
    "        \"\"\"演示数据可视化\"\"\"\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if not self.query or not self.query.discoveries:\n",
    "            print(\"❌ 没有数据可供可视化\")\n",
    "            return\n",
    "\n",
    "        # 创建数据可视化\n",
    "        # self._create_discovery_timeline()\n",
    "        # self._create_discoverer_network()\n",
    "        # self._create_application_distribution()\n",
    "\n",
    "    def _create_discovery_timeline(self):\n",
    "        \"\"\"创建发现时间线图\"\"\"\n",
    "        print(\"📅 创建科学发现时间线图...\")\n",
    "\n",
    "        try:\n",
    "            # 准备数据（来自结构化结果）\n",
    "            discoveries = self.query.discoveries\n",
    "            years = []\n",
    "            names = []\n",
    "\n",
    "            import re\n",
    "            def parse_year(value: str):\n",
    "                if not value:\n",
    "                    return None\n",
    "                m = re.search(r\"\\b(1|2)\\d{3}\\b\", str(value))\n",
    "                return int(m.group(0)) if m else None\n",
    "\n",
    "            for discovery in discoveries:\n",
    "                years_raw = discovery.get('discovery_years') or []\n",
    "                y = parse_year(years_raw[0] if years_raw else \"\")\n",
    "                if y is None:\n",
    "                    continue\n",
    "                years.append(y)\n",
    "                names.append(discovery['primary_name'])\n",
    "\n",
    "            # 创建时间线图\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.scatter(years, range(len(years)), s=100, alpha=0.7, c='skyblue', edgecolors='navy')\n",
    "\n",
    "            for i, (year, name) in enumerate(zip(years, names)):\n",
    "                plt.annotate(f\"{year}: {name[:20]}...\",\n",
    "                        (year, i),\n",
    "                        xytext=(10, 0),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=8,\n",
    "                        ha='left')\n",
    "\n",
    "            plt.xlabel('发现年份')\n",
    "            plt.ylabel('科学发现')\n",
    "            plt.title('科学发现时间线')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"✅ 时间线图创建成功\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 时间线图创建失败: {str(e)}\")\n",
    "\n",
    "    def _create_discoverer_network(self):\n",
    "        \"\"\"创建发现者网络图\"\"\"\n",
    "        print(\"👥 创建发现者网络图...\")\n",
    "\n",
    "        try:\n",
    "            # 统计发现者\n",
    "            discoverer_count = {}\n",
    "            for discovery in self.query.discoveries:\n",
    "                for discoverer in discovery.get('discoverers', []):\n",
    "                    discoverer_count[discoverer] = discoverer_count.get(discoverer, 0) + 1\n",
    "\n",
    "            # 创建柱状图\n",
    "            if discoverer_count:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                names = list(discoverer_count.keys())\n",
    "                counts = list(discoverer_count.values())\n",
    "\n",
    "                bars = plt.bar(range(len(names)), counts, color='lightcoral', alpha=0.7)\n",
    "                plt.xlabel('发现者')\n",
    "                plt.ylabel('发现数量')\n",
    "                plt.title('发现者贡献统计')\n",
    "                plt.xticks(range(len(names)), [name.split()[-1] for name in names], rotation=45)\n",
    "\n",
    "                # 添加数值标签\n",
    "                for bar, count in zip(bars, counts):\n",
    "                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                            str(count), ha='center', va='bottom')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                print(\"✅ 发现者网络图创建成功\")\n",
    "            else:\n",
    "                print(\"⚠️ 没有发现者数据\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 发现者网络图创建失败: {str(e)}\")\n",
    "\n",
    "    def _create_application_distribution(self):\n",
    "        \"\"\"创建应用领域分布图\"\"\"\n",
    "        print(\"🎯 创建应用领域分布图...\")\n",
    "\n",
    "        try:\n",
    "            # 统计应用领域\n",
    "            application_count = {}\n",
    "            for discovery in self.query.discoveries:\n",
    "                for app in discovery.get('applications', []):\n",
    "                    # 简化应用领域名称\n",
    "                    app_key = app.lower().strip()\n",
    "                    if 'research' in app_key:\n",
    "                        app_key = 'Research'\n",
    "                    elif 'medicine' in app_key or 'medical' in app_key:\n",
    "                        app_key = 'Medicine'\n",
    "                    elif 'biotechnology' in app_key or 'biotech' in app_key:\n",
    "                        app_key = 'Biotechnology'\n",
    "                    elif 'treatment' in app_key or 'therapy' in app_key:\n",
    "                        app_key = 'Treatment'\n",
    "                    else:\n",
    "                        app_key = app_key.title()\n",
    "\n",
    "                    application_count[app_key] = application_count.get(app_key, 0) + 1\n",
    "\n",
    "            # 创建饼图\n",
    "            if application_count:\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                labels = list(application_count.keys())\n",
    "                sizes = list(application_count.values())\n",
    "                colors = plt.cm.Set3(range(len(labels)))\n",
    "\n",
    "                plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "                plt.title('科学发现应用领域分布')\n",
    "                plt.axis('equal')\n",
    "                plt.show()\n",
    "\n",
    "                print(\"✅ 应用领域分布图创建成功\")\n",
    "            else:\n",
    "                print(\"⚠️ 没有应用领域数据\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 应用领域分布图创建失败: {str(e)}\")\n",
    "\n",
    "print(\"✅ 综合演示类定义完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ836xTmyRob",
    "outputId": "cf54b291-5af0-4bcc-e422-40819a9f39a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 开始Part 4: Integration and Demonstration\n",
      "============================================================\n",
      "🚀 综合演示系统初始化完成\n",
      "================================================================================\n",
      "🔬 科学发现研究系统 - 完整演示\n",
      "================================================================================\n",
      "🚀 Starting structured data extraction...\n",
      "============================================================\n",
      "✅ OpenAI client initialized successfully\n",
      "📖 Using the provided scraping results\n",
      "📖 Successfully loaded 8 articles\n",
      "🔄 Starting batch structured extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting batch processing of 8 articles...\n",
      "INFO:__main__:Processing article 1/8: CRISPR\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 2/8: mRNA vaccine\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 3/8: Gravitational wave\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 4/8: Higgs boson\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 5/8: Quantum computing\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 6/8: Ancient DNA\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 7/8: Water on Mars\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Processing article 8/8: Penicillin\n",
      "INFO:__main__:Starting extraction attempt 1...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:✅ Structured data extraction succeeded\n",
      "INFO:__main__:Batch processing finished: Success 8, Failed 0, Skipped 0\n",
      "INFO:__main__:✅ Extraction results saved to: ./structured_extractions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Extraction statistics:\n",
      "✅ Successfully extracted: 8 articles\n",
      "❌ Failed to extract: 0 articles\n",
      "\n",
      "📄 Sample extraction result - CRISPR:\n",
      "--------------------------------------------------\n",
      "📝 Primary name: CRISPR-Cas9 genome editing technology\n",
      "👥 Discoverers: Jennifer Doudna, Emmanuelle Charpentier, Francisco Mojica, Yoshizumi Ishino, Rodolphe Barrangou, Ruud Jansen\n",
      "📅 Discovery years: 2012\n",
      "🔬 Applications: 4 items\n",
      "⭐ Key features: 4 items\n",
      "\n",
      "✅ Structured extraction completed! Results saved to './structured_extractions.json'\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "🔬 科学发现交互式研究助手\n",
      "==================================================\n",
      "✅ 科学研究助手初始化成功\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "❌ 暂无可用数据\n",
      "✅ 使用传入的示例对话\n",
      "Question: Please compare the scientific discoveries of CRISPR and quantum computing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 调用函数: compare_discoveries\n",
      "📝 参数: {'discovery1': 'CRISPR', 'discovery2': 'quantum computing'}\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "Answer: ❌ Discovery not found: CRISPR\n",
      "Question: What are the major research discoveries of Jennifer Doudna?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 调用函数: get_research_timeline\n",
      "📝 参数: {'scientist': 'Jennifer Doudna'}\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "Answer: ❌ No discoveries found for scientist 'Jennifer Doudna'\n",
      "Question: Search for scientific discoveries applied in the field of medicine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 调用函数: search_by_application\n",
      "📝 参数: {'application': 'medicine'}\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "Answer: ❌ No discoveries found related to 'medicine'\n",
      "Question: Give me a detailed introduction to CRISPR technology.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 调用函数: get_discovery_details\n",
      "📝 参数: {'discovery_name': 'CRISPR'}\n",
      "✅ Loaded data from ./structured_extractions.json\n",
      "✅ Successfully loaded 0 scientific discovery data\n",
      "Answer: ❌ Discovery not found: CRISPR\n",
      "--------------------------------------------------\n",
      "❌ 没有数据可供可视化\n",
      "\n",
      "✅ 完整演示完成！\n",
      "\n",
      "🎉 Part 4演示完成！\n",
      "============================================================\n",
      "📋 演示内容总结:\n",
      "✅ 1. 完整端到端工作流程\n",
      "✅ 2. 所有主要功能演示\n",
      "✅ 3. 错误处理示例\n",
      "✅ 4. 数据可视化\n",
      "✅ 5. 交互式助手演示\n"
     ]
    }
   ],
   "source": [
    "# 运行完整的Part 4演示\n",
    "def run_part4_demo():\n",
    "    \"\"\"运行Part 4完整演示\"\"\"\n",
    "    print(\"🎬 开始Part 4: Integration and Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建演示实例\n",
    "    demo = ComprehensiveDemo()\n",
    "\n",
    "    # 运行完整管道（本函数需在 Jupyter 中用 await 调用）\n",
    "    import nest_asyncio, asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.get_event_loop().run_until_complete(demo.run_complete_pipeline())\n",
    "\n",
    "    print(\"\\n🎉 Part 4演示完成！\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📋 演示内容总结:\")\n",
    "    print(\"✅ 1. 完整端到端工作流程\")\n",
    "    print(\"✅ 2. 所有主要功能演示\")\n",
    "    print(\"✅ 3. 错误处理示例\")\n",
    "    print(\"✅ 4. 数据可视化\")\n",
    "    print(\"✅ 5. 交互式助手演示\")\n",
    "\n",
    "# 执行演示\n",
    "run_part4_demo()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "is5126",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
